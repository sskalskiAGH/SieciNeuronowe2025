{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98495354",
   "metadata": {},
   "source": [
    "Sieć perceptronowa - nasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b07acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiary: Train: (3669, 5), Val: (786, 5), Test: (787, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Ustawienie ziarna dla powtarzalności\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---- WCZYTYWANIE DANYCH ----\n",
    "data = pd.read_csv('XAU_1d_data.csv', delimiter=\";\")\n",
    "data['Date'] = pd.to_datetime(data['Date']).dt.date\n",
    "\n",
    "# ---- FEATURE ENGINEERING (Zgodnie z wytycznymi dla szeregów czasowych) ----\n",
    "# Przewidujemy 'Close' na jutro na podstawie dzisiejszych danych\n",
    "data['Target'] = data['Close'].shift(-1)\n",
    "data = data.dropna()\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Volume', 'Close']\n",
    "target = 'Target'\n",
    "\n",
    "# Usuwanie outlierów \n",
    "for col in features + [target]:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3*std) & (data[col] <= mean + 3*std)]\n",
    "\n",
    "X_raw = data[features].values\n",
    "y_raw = data[target].values.reshape(-1, 1)\n",
    "\n",
    "# ---- PODZIAŁ CHRONOLOGICZNY (Wymóg: pierwsze x% train, potem y% val, z% test) ----\n",
    "def split_time_series(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    return (X[:train_end], y[:train_end], \n",
    "            X[train_end:val_end], y[train_end:val_end], \n",
    "            X[val_end:], y[val_end:])\n",
    "\n",
    "X_train_raw, y_train_raw, X_val_raw, y_val_raw, X_test_raw, y_test_raw = split_time_series(X_raw, y_raw)\n",
    "\n",
    "# ---- SKALOWANIE ----\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_x.fit_transform(X_train_raw)\n",
    "X_val = scaler_x.transform(X_val_raw)\n",
    "X_test = scaler_x.transform(X_test_raw)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train_raw)\n",
    "y_val = scaler_y.transform(y_val_raw)\n",
    "y_test = scaler_y.transform(y_test_raw)\n",
    "\n",
    "print(f\"Rozmiary: Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e61364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam trening MLP (Własna implementacja)...\n"
     ]
    }
   ],
   "source": [
    "# Funkcje aktywacji i pomocnicze \n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def initialize_weights_bias(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "        biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "    return weights, biases\n",
    "\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=1e-6):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.05\n",
    "    else:\n",
    "        learning_rate *= 0.7\n",
    "    if abs(mse - previous_mse) < threshold:\n",
    "        learning_rate *= learning_rate_adjust\n",
    "    return learning_rate\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'mse': mean_squared_error(y_true, y_pred),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'r2':  r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def train_mlp_custom(X, y, learning_rate, learning_rate_adjust, epochs, hidden_layers_sizes, optimizer='gd', momentum=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights, biases = initialize_weights_bias(input_size, hidden_layers_sizes, output_size)\n",
    "\n",
    "    velocities_w = [np.zeros_like(w) for w in weights]\n",
    "    velocities_b = [np.zeros_like(b) for b in biases]\n",
    "\n",
    "    m_w = [np.zeros_like(w) for w in weights]\n",
    "    v_w = [np.zeros_like(w) for w in weights]\n",
    "    m_b = [np.zeros_like(b) for b in biases]\n",
    "    v_b = [np.zeros_like(b) for b in biases]\n",
    "\n",
    "    prev_loss = np.inf\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ---- Forward pass ----\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i in range(len(weights)):\n",
    "            z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "            zs.append(z)\n",
    "            activations.append(z if i == len(weights)-1 else relu(z))\n",
    "\n",
    "        # ---- Backpropagation ----\n",
    "        error = activations[-1] - y\n",
    "        loss = np.mean(error ** 2)\n",
    "        learning_rate = adjust_learning_rate(learning_rate, loss, prev_loss, learning_rate_adjust)\n",
    "        prev_loss = loss\n",
    "\n",
    "        deltas = [error]\n",
    "        for i in range(len(weights)-1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "\n",
    "        # ---- Aktualizacja wag i biasów ----\n",
    "        for i in range(len(weights)):\n",
    "            grad_w = activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "            grad_b = np.mean(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "            if optimizer == 'gd':\n",
    "                weights[i] -= learning_rate * grad_w\n",
    "                biases[i]  -= learning_rate * grad_b\n",
    "            elif optimizer == 'momentum':\n",
    "                velocities_w[i] = momentum * velocities_w[i] - learning_rate * grad_w\n",
    "                velocities_b[i] = momentum * velocities_b[i] - learning_rate * grad_b\n",
    "                weights[i] += velocities_w[i]\n",
    "                biases[i]  += velocities_b[i]\n",
    "            elif optimizer == 'adam':\n",
    "                # dla wag\n",
    "                m_w[i] = beta1 * m_w[i] + (1 - beta1) * grad_w\n",
    "                v_w[i] = beta2 * v_w[i] + (1 - beta2) * (grad_w ** 2)\n",
    "                m_hat_w = m_w[i] / (1 - beta1 ** epoch)\n",
    "                v_hat_w = v_w[i] / (1 - beta2 ** epoch)\n",
    "                weights[i] -= learning_rate * m_hat_w / (np.sqrt(v_hat_w) + epsilon)\n",
    "\n",
    "                # dla biasów\n",
    "                m_b[i] = beta1 * m_b[i] + (1 - beta1) * grad_b\n",
    "                v_b[i] = beta2 * v_b[i] + (1 - beta2) * (grad_b ** 2)\n",
    "                m_hat_b = m_b[i] / (1 - beta1 ** epoch)\n",
    "                v_hat_b = v_b[i] / (1 - beta2 ** epoch)\n",
    "                biases[i]  -= learning_rate * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "def predict_mlp_custom(X, weights, biases):\n",
    "    output = X\n",
    "    for i in range(len(weights)):\n",
    "        output = np.dot(output, weights[i]) + biases[i]\n",
    "        if i < len(weights) - 1:\n",
    "            output = relu(output)\n",
    "    return output\n",
    "\n",
    "# ---- EKSPERYMENTY MLP (Zgodnie z wymogami: 4 wartości parametrów, 5 powtórzeń) ----\n",
    "results_mlp = []\n",
    "lrs = [0.001, 0.01, 0.05, 0.1] # 4 wartości\n",
    "epochs_options = [500, 1000, 1500, 2000] # 4 wartości\n",
    "hidden_configs = [[10], [10, 10], [20, 10], [32, 16, 8]] # 4 wartości\n",
    "\n",
    "print(\"Rozpoczynam trening MLP (Własna implementacja)...\")\n",
    "for lr in lrs: \n",
    "    for hid in hidden_configs:\n",
    "        for r in range(5): # Wymóg: 5 powtórzeń\n",
    "            w, b = train_mlp_custom(\n",
    "                X_train, y_train, lr, 0.0005, 1000, hid, 'momentum', 0.9)\n",
    "            # --- PREDYKCJE ---\n",
    "            pred_train = predict_mlp_custom(X_train, w, b)\n",
    "            pred_val   = predict_mlp_custom(X_val, w, b)\n",
    "            pred_test  = predict_mlp_custom(X_test, w, b)\n",
    "\n",
    "            # --- ODSKALOWANIE ---\n",
    "            y_train_real = scaler_y.inverse_transform(y_train)\n",
    "            y_val_real   = scaler_y.inverse_transform(y_val)\n",
    "            y_test_real  = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "            pred_train_real = scaler_y.inverse_transform(pred_train)\n",
    "            pred_val_real   = scaler_y.inverse_transform(pred_val)\n",
    "            pred_test_real  = scaler_y.inverse_transform(pred_test)\n",
    "\n",
    "            # --- METRYKI ---\n",
    "            train_metrics = regression_metrics(y_train_real, pred_train_real)\n",
    "            val_metrics   = regression_metrics(y_val_real,   pred_val_real)\n",
    "            test_metrics  = regression_metrics(y_test_real,  pred_test_real)\n",
    "\n",
    "            # --- ZAPIS WYNIKÓW ---\n",
    "            results_mlp.append({\n",
    "                'model': 'Custom_MLP',\n",
    "                'lr': lr,\n",
    "                'layers': str(hid),\n",
    "                'run': r,\n",
    "\n",
    "                # TRAIN\n",
    "                'train_mse': train_metrics['mse'],\n",
    "                'train_mae': train_metrics['mae'],\n",
    "                'train_r2':  train_metrics['r2'],\n",
    "\n",
    "                # VALIDATION\n",
    "                'val_mse': val_metrics['mse'],\n",
    "                'val_mae': val_metrics['mae'],\n",
    "                'val_r2':  val_metrics['r2'],\n",
    "\n",
    "                # TEST\n",
    "                'test_mse': test_metrics['mse'],\n",
    "                'test_mae': test_metrics['mae'],\n",
    "                'test_r2':  test_metrics['r2'],\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5383796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Tworzenie DataFrame dla szczegółowych wyników ----\n",
    "df_mlp = pd.DataFrame(results_mlp)\n",
    "\n",
    "# ---- ZAPIS SZCZEGÓŁOWY (każdy run osobno) ----\n",
    "df_mlp.to_excel('wyniki_regresja_szczegolowe.xlsx', sheet_name='MLP_szczegolowy', index=False)\n",
    "\n",
    "# ---- AGREGACJA (średnie, min, max) po wszystkich run i konfiguracjach ----\n",
    "summary_mlp = df_mlp.groupby(['lr', 'layers']).agg({\n",
    "    # TRAIN\n",
    "    'train_mse': ['mean', 'min', 'max'],\n",
    "    'train_mae': ['mean', 'min', 'max'],\n",
    "    'train_r2':  ['mean', 'min', 'max'],\n",
    "    # VALIDATION\n",
    "    'val_mse': ['mean', 'min', 'max'],\n",
    "    'val_mae': ['mean', 'min', 'max'],\n",
    "    'val_r2':  ['mean', 'min', 'max'],\n",
    "    # TEST\n",
    "    'test_mse': ['mean', 'min', 'max'],\n",
    "    'test_mae': ['mean', 'min', 'max'],\n",
    "    'test_r2':  ['mean', 'min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# ---- ZAPIS ZAGREGOWANY ----\n",
    "summary_mlp.to_excel('wyniki_regresja_zagregowane.xlsx', sheet_name='MLP_zagregowany', index=False)\n",
    "\n",
    "print(\"Pliki Excel zostały wygenerowane:\")\n",
    "print(\"1. wyniki_regresja_szczegolowe.xlsx  (wszystkie runy)\")\n",
    "print(\"2. wyniki_regresja_zagregowane.xlsx  (średnie, min, max)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93587b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=5):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS = 7 # Analiza ostatniego tygodnia (7 dni)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, TIME_STEPS)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, TIME_STEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0382be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 20:09:22.317457: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, Flatten, Input\n",
    "\n",
    "def run_keras_experiment(model_type, param_list, param_name):\n",
    "    results = []\n",
    "    for val in param_list:\n",
    "        for r in range(5):\n",
    "            model = Sequential()\n",
    "            model.add(Input(shape=(TIME_STEPS, X_train.shape[1])))\n",
    "            \n",
    "            if model_type == 'LSTM':\n",
    "                model.add(LSTM(val, activation='relu'))\n",
    "            elif model_type == 'CNN':\n",
    "                model.add(Conv1D(filters=val, kernel_size=3, activation='relu'))\n",
    "                model.add(Flatten())\n",
    "            \n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            \n",
    "            model.fit(X_train_seq, y_train_seq, epochs=20, verbose=0, batch_size=32)\n",
    "            \n",
    "            pred = model.predict(X_test_seq, verbose=0)\n",
    "            p_real = scaler_y.inverse_transform(pred)\n",
    "            t_real = scaler_y.inverse_transform(y_test_seq)\n",
    "            \n",
    "            mse = mean_squared_error(t_real, p_real)\n",
    "            r2 = r2_score(t_real, p_real)\n",
    "            results.append({param_name: val, 'run': r, 'mse': mse, 'r2': r2})\n",
    "    return results\n",
    "\n",
    "# Wykonanie badań\n",
    "lstm_results = run_keras_experiment('LSTM', [16, 32, 64, 128], 'units')\n",
    "cnn_results = run_keras_experiment('CNN', [16, 32, 64, 128], 'filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3e0a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PODSUMOWANIE LSTM:\n",
      "                mse                     r2\n",
      "               mean          min      mean\n",
      "units                                     \n",
      "16      9787.000649  3153.343998  0.899299\n",
      "32     10159.380027  5340.790149  0.895468\n",
      "64     11804.270649  3274.055325  0.878543\n",
      "128     9370.512709  2503.672161  0.903584\n",
      "\n",
      "Plik 'wyniki_regresja_final.xlsx' został wygenerowany.\n"
     ]
    }
   ],
   "source": [
    "# Łączenie wyników w tabele\n",
    "df_mlp = pd.DataFrame(results_mlp)\n",
    "df_lstm = pd.DataFrame(lstm_results)\n",
    "df_cnn = pd.DataFrame(cnn_results)\n",
    "\n",
    "# wyciągania średnich i najlepszych wyników \n",
    "summary_lstm = df_lstm.groupby('units').agg({'mse': ['mean', 'min'], 'r2': 'mean'})\n",
    "print(\"PODSUMOWANIE LSTM:\")\n",
    "print(summary_lstm)\n",
    "\n",
    "# Eksport do Excela dla sprawozdania\n",
    "with pd.ExcelWriter('wyniki_regresja_final.xlsx') as writer:\n",
    "    df_mlp.to_excel(writer, sheet_name='MLP_Wlasny')\n",
    "    df_lstm.to_excel(writer, sheet_name='LSTM')\n",
    "    df_cnn.to_excel(writer, sheet_name='CNN')\n",
    "\n",
    "print(\"\\nPlik 'wyniki_regresja_final.xlsx' został wygenerowany.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
