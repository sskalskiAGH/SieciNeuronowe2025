{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a11941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba całkowicie pustych wierszy: 114\n",
      "\n",
      "--- Informacje o wymiarach ---\n",
      "Liczba wierszy: 9357\n",
      "Liczba kolumn: 15\n",
      "Liczba wierszy z brakującymi danymi: 0\n",
      "\n",
      "--- Pierwsze 5 wierszy ---\n",
      "         Date      Time  CO(GT)  PT08.S1(CO)  NMHC(GT)  C6H6(GT)  \\\n",
      "0  10/03/2004  18.00.00     2.6       1360.0     150.0      11.9   \n",
      "1  10/03/2004  19.00.00     2.0       1292.0     112.0       9.4   \n",
      "2  10/03/2004  20.00.00     2.2       1402.0      88.0       9.0   \n",
      "3  10/03/2004  21.00.00     2.2       1376.0      80.0       9.2   \n",
      "4  10/03/2004  22.00.00     1.6       1272.0      51.0       6.5   \n",
      "\n",
      "   PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)  \\\n",
      "0         1046.0    166.0        1056.0    113.0        1692.0       1268.0   \n",
      "1          955.0    103.0        1174.0     92.0        1559.0        972.0   \n",
      "2          939.0    131.0        1140.0    114.0        1555.0       1074.0   \n",
      "3          948.0    172.0        1092.0    122.0        1584.0       1203.0   \n",
      "4          836.0    131.0        1205.0    116.0        1490.0       1110.0   \n",
      "\n",
      "      T    RH      AH  \n",
      "0  13.6  48.9  0.7578  \n",
      "1  13.3  47.7  0.7255  \n",
      "2  11.9  54.0  0.7502  \n",
      "3  11.0  60.0  0.7867  \n",
      "4  11.2  59.6  0.7888  \n",
      "\n",
      "--- Ciągłość czasu ---\n",
      "\n",
      "--- Brakujące odczyty (łącznie: 0) ---\n",
      "Brak brakujących dni/godzin w ciągłości odczytów.\n",
      "\n",
      "--- Ujemne wartości ---\n",
      "\n",
      "--- Wiersze z ujemnymi wartościami: 13 ---\n",
      "      CO(GT)  PT08.S1(CO)  NMHC(GT)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
      "8530    0.20        716.0     275.0       0.2          390.0     29.0   \n",
      "8531    0.20        731.0     275.0       0.3          408.0     32.0   \n",
      "8532    0.20        735.0     275.0       0.3          407.0     37.0   \n",
      "8533    0.50        779.0     275.0       1.0          500.0    134.0   \n",
      "8534    1.50        915.0     275.0       5.2          774.0    358.0   \n",
      "8552    0.60        761.0     275.0       1.3          525.0     79.0   \n",
      "8553    0.50        788.0     275.0       0.9          486.0     80.0   \n",
      "8554    0.45        790.0     275.0       0.7          471.0     81.0   \n",
      "8555    0.40        804.0     275.0       0.9          489.0     77.0   \n",
      "8556    0.50        831.0     275.0       1.4          537.0    124.0   \n",
      "8557    0.70        881.0     275.0       3.5          680.0    159.0   \n",
      "8558    2.40       1099.0     275.0      11.0         1013.0    396.0   \n",
      "8559    2.80       1100.0     275.0      12.4         1065.0    479.0   \n",
      "\n",
      "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)    T    RH      AH  \n",
      "8530        1804.0     27.0         551.0        221.0 -0.1  37.5  0.2326  \n",
      "8531        1727.0     30.0         561.0        225.0 -0.3  38.4  0.2347  \n",
      "8532        1677.0     33.0         579.0        232.0 -0.6  40.1  0.2404  \n",
      "8533        1361.0     84.0         652.0        313.0 -0.6  41.4  0.2478  \n",
      "8534         911.0    147.0         839.0        629.0 -0.2  40.2  0.2478  \n",
      "8552        1307.0     67.0         605.0        332.0 -0.1  31.9  0.1975  \n",
      "8553        1294.0     71.0         650.0        420.0 -1.1  41.4  0.2379  \n",
      "8554        1302.0     75.0         669.0        532.0 -1.4  44.9  0.2536  \n",
      "8555        1262.0     73.0         698.0        630.0 -1.3  46.3  0.2635  \n",
      "8556        1158.0     96.0         722.0        768.0 -1.2  47.2  0.2700  \n",
      "8557        1023.0    110.0         813.0        866.0 -1.3  47.5  0.2702  \n",
      "8558         690.0    150.0        1056.0       1254.0 -1.9  51.4  0.2801  \n",
      "8559         669.0    169.0        1072.0       1347.0 -0.5  45.9  0.2763  \n",
      "\n",
      "Jeśli ujemne wartości są tylko w kolumnie T (Temperatura w stopniach Celcjusza), to jest to poprawnie.\n",
      "Aktualna liczba kolumn: 15\n",
      "\n",
      "--- Podział na zbiory train, validation i test ---\n",
      "Set\n",
      "train         6549\n",
      "validation    1404\n",
      "test          1404\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--> Zapisano gotowy plik: AirQualityUCI_outcome.xlsx.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nazwa_pliku = 'AirQualityUCI.csv'\n",
    "\n",
    "try:\n",
    "\n",
    "    df = pd.read_csv(nazwa_pliku, sep=';', decimal=',')\n",
    "    \n",
    "    ile_pustych = df.isna().all(axis=1).sum()\n",
    "    print(f\"Liczba całkowicie pustych wierszy: {ile_pustych}\")\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df = df.iloc[:, :-2]\n",
    "    \n",
    "    print(\"\\n--- Informacje o wymiarach ---\")\n",
    "    wiersze, kolumny = df.shape\n",
    "    print(f\"Liczba wierszy: {wiersze}\")\n",
    "    print(f\"Liczba kolumn: {kolumny}\")\n",
    "    \n",
    "    # Odczyt \"-200\" to błąd czujnika\n",
    "    df.replace(-200, np.nan, inplace=True)\n",
    "    # Uzupełnianie braków metodą interpolacji liniowej\n",
    "    cols_numeric = df.select_dtypes(include=[np.number]).columns\n",
    "    df[cols_numeric] = df[cols_numeric].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    liczba_wierszy_z_nan = df.isna().any(axis=1).sum()\n",
    "    print(f\"Liczba wierszy z brakującymi danymi: {liczba_wierszy_z_nan}\")\n",
    "   \n",
    "    print(\"\\n--- Pierwsze 5 wierszy ---\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\n--- Ciągłość czasu ---\")\n",
    "    # Tworzę dodatkową kolumną Datetime\n",
    "    df['Time_Clean'] = df['Time'].astype(str).str.replace('.', ':', regex=False)\n",
    "    df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time_Clean'], dayfirst=True)\n",
    "    # Generuję pełny zakres DateTime\n",
    "    full_range = pd.date_range(start=df['Datetime'].min(), end=df['Datetime'].max(), freq='H')\n",
    "    # Porównuję faktyczne dane w zbiorze z tymi wygenerowanymi\n",
    "    missing_dates = set(full_range) - set(df['Datetime'])\n",
    "    \n",
    "    print(f\"\\n--- Brakujące odczyty (łącznie: {len(missing_dates)}) ---\")\n",
    "    if missing_dates:\n",
    "        print(f\"Liczba brakujących wpisów: {len(missing_dates)}\")\n",
    "        print(\"Wszystkie brakujące daty:\")\n",
    "        print(*sorted(missing_dates), sep='\\n')\n",
    "    else:\n",
    "        print(\"Brak brakujących dni/godzin w ciągłości odczytów.\")\n",
    "        \n",
    "    print(\"\\n--- Ujemne wartości ---\")\n",
    "    maska_ujemne = (df[cols_numeric] < 0).any(axis=1)\n",
    "    negative_rows = df[maska_ujemne]\n",
    "    \n",
    "    print(f\"\\n--- Wiersze z ujemnymi wartościami: {len(negative_rows)} ---\")\n",
    "    if not negative_rows.empty:\n",
    "        print(negative_rows[cols_numeric])\n",
    "        print(\"\\nJeśli ujemne wartości są tylko w kolumnie T (Temperatura w stopniach Celcjusza), to jest to poprawnie.\")\n",
    "    else:\n",
    "        print(\"Brak ujemnych wartości po interpolacji.\")\n",
    "        \n",
    "    #Usunięcie niepotrzebnych kolumn\n",
    "    df.drop(columns=['Datetime', 'Time_Clean'], inplace=True)\n",
    "    print(f\"Aktualna liczba kolumn: {df.shape[1]}\")\n",
    "    \n",
    "    print(\"\\n--- Podział na zbiory train, validation i test ---\")\n",
    "    temp_time = df['Time'].astype(str).str.replace('.', ':', regex=False)\n",
    "    df['Temp_Datetime'] = pd.to_datetime(df['Date'] + ' ' + temp_time, dayfirst=True)\n",
    "    df.sort_values(by='Temp_Datetime', inplace=True)\n",
    "    \n",
    "    n = len(df)\n",
    "    train_end = int(0.70 * n)\n",
    "    val_end = int(0.85 * n)\n",
    "    \n",
    "    # Tworzenie kolumny Set\n",
    "    df['Set'] = 'test' # Domyślnie wypełnia 'test' (dla ostatnich 15%)\n",
    "    df.iloc[:train_end, df.columns.get_loc('Set')] = 'train'         # Pierwsze 70%\n",
    "    df.iloc[train_end:val_end, df.columns.get_loc('Set')] = 'validation' # Kolejne 15%\n",
    "    df.drop(columns=['Temp_Datetime'], inplace=True)\n",
    "    print(df['Set'].value_counts(sort=False))\n",
    "    \n",
    "    df.to_excel(\"AirQualityUCI_outcome.xlsx\", index=False)\n",
    "    print(\"\\n--> Zapisano gotowy plik: AirQualityUCI_outcome.xlsx.xlsx\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Nie znaleziono pliku. Sprawdź czy nazwa i ścieżka są poprawne.\")\n",
    "except Exception as e:\n",
    "    print(f\"Wystąpił błąd: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02004d",
   "metadata": {},
   "source": [
    "## Nasza sieć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f270c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(6549, 12), y=(6549, 1)\n",
      "Validation: X=(1404, 12), y=(1404, 1)\n",
      "Test: X=(1404, 12), y=(1404, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_excel(\"AirQualityUCI_outcome.xlsx\")\n",
    "\n",
    "features = ['PT08.S1(CO)','NMHC(GT)','C6H6(GT)','PT08.S2(NMHC)','NOx(GT)',\n",
    "            'PT08.S3(NOx)','NO2(GT)','PT08.S4(NO2)','PT08.S5(O3)','T','RH','AH']\n",
    "target = 'CO(GT)'\n",
    "\n",
    "# Tworzymy zbiory według kolumny 'Set'\n",
    "X_train = data[data['Set']=='train'][features].values\n",
    "y_train = data[data['Set']=='train'][target].values.reshape(-1,1)\n",
    "\n",
    "X_validation = data[data['Set']=='validation'][features].values\n",
    "y_validation = data[data['Set']=='validation'][target].values.reshape(-1,1)\n",
    "\n",
    "X_test = data[data['Set']=='test'][features].values\n",
    "y_test = data[data['Set']=='test'][target].values.reshape(-1,1)\n",
    "\n",
    "# Standaryzacja\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_validation = scaler_X.transform(X_validation)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_validation = scaler_y.transform(y_validation)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation: X={X_validation.shape}, y={y_validation.shape}\")\n",
    "print(f\"Test: X={X_test.shape}, y={y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb73e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layers optimizer  momentum  learning_rate  learning_rate_adjust  \\\n",
      "0           [10]        gd       0.0           0.01                0.0005   \n",
      "1           [10]        gd       0.9           0.01                0.0005   \n",
      "2           [10]  momentum       0.0           0.01                0.0005   \n",
      "3           [10]  momentum       0.9           0.01                0.0005   \n",
      "4           [10]      adam       0.0           0.01                0.0005   \n",
      "5           [10]      adam       0.9           0.01                0.0005   \n",
      "6           [10]        gd       0.0           0.01                0.0005   \n",
      "7           [10]        gd       0.9           0.01                0.0005   \n",
      "8           [10]  momentum       0.0           0.01                0.0005   \n",
      "9           [10]  momentum       0.9           0.01                0.0005   \n",
      "10          [10]      adam       0.0           0.01                0.0005   \n",
      "11          [10]      adam       0.9           0.01                0.0005   \n",
      "12          [10]        gd       0.0           0.01                0.0005   \n",
      "13          [10]        gd       0.9           0.01                0.0005   \n",
      "14          [10]  momentum       0.0           0.01                0.0005   \n",
      "15          [10]  momentum       0.9           0.01                0.0005   \n",
      "16          [10]      adam       0.0           0.01                0.0005   \n",
      "17          [10]      adam       0.9           0.01                0.0005   \n",
      "18      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "19      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "20      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "21      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "22      [10, 10]      adam       0.0           0.01                0.0005   \n",
      "23      [10, 10]      adam       0.9           0.01                0.0005   \n",
      "24      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "25      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "26      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "27      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "28      [10, 10]      adam       0.0           0.01                0.0005   \n",
      "29      [10, 10]      adam       0.9           0.01                0.0005   \n",
      "30      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "31      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "32      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "33      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "34      [10, 10]      adam       0.0           0.01                0.0005   \n",
      "35      [10, 10]      adam       0.9           0.01                0.0005   \n",
      "\n",
      "    epochs  repeat       mse       mae        r2  \n",
      "0     1000       1  0.187111  0.321201  0.784497  \n",
      "1     1000       1  0.144203  0.263322  0.833916  \n",
      "2     1000       1  0.228525  0.369735  0.736800  \n",
      "3     1000       1  1.695980  1.090384 -0.953321  \n",
      "4     1000       1  0.193263  0.322098  0.777413  \n",
      "5     1000       1  0.169112  0.303208  0.805228  \n",
      "6     1000       2  0.185815  0.316531  0.785990  \n",
      "7     1000       2  0.196956  0.338791  0.773158  \n",
      "8     1000       2  0.239251  0.363316  0.724446  \n",
      "9     1000       2  0.208434  0.335383  0.759939  \n",
      "10    1000       2  0.202662  0.332298  0.766587  \n",
      "11    1000       2  0.268012  0.367576  0.691321  \n",
      "12    1000       3  0.148522  0.283753  0.828942  \n",
      "13    1000       3  0.181246  0.310808  0.791252  \n",
      "14    1000       3  0.153052  0.280229  0.823725  \n",
      "15    1000       3  0.437439  0.496462  0.496186  \n",
      "16    1000       3  0.176495  0.314420  0.796725  \n",
      "17    1000       3  0.171412  0.289259  0.802579  \n",
      "18    1000       1  0.167034  0.298544  0.807621  \n",
      "19    1000       1  0.194201  0.329434  0.776332  \n",
      "20    1000       1  0.139769  0.267904  0.839023  \n",
      "21    1000       1  0.240989  0.383758  0.722444  \n",
      "22    1000       1  0.191926  0.328403  0.778952  \n",
      "23    1000       1  0.277546  0.420906  0.680341  \n",
      "24    1000       2  0.136261  0.257853  0.843063  \n",
      "25    1000       2  0.212536  0.366715  0.755215  \n",
      "26    1000       2  0.186161  0.310140  0.785592  \n",
      "27    1000       2  0.199515  0.320802  0.770211  \n",
      "28    1000       2  0.240711  0.362472  0.722764  \n",
      "29    1000       2  0.221370  0.358905  0.745041  \n",
      "30    1000       3  0.146903  0.291403  0.830807  \n",
      "31    1000       3  0.183653  0.307645  0.788480  \n",
      "32    1000       3  0.160267  0.272519  0.815414  \n",
      "33    1000       3  0.194076  0.332658  0.776475  \n",
      "34    1000       3  0.189566  0.327896  0.781671  \n",
      "35    1000       3  0.222120  0.338589  0.744176  \n",
      "  hidden_layers optimizer  momentum  learning_rate  learning_rate_adjust  \\\n",
      "0          [10]        gd       0.0           0.01                0.0005   \n",
      "1          [10]        gd       0.9           0.01                0.0005   \n",
      "2          [10]  momentum       0.0           0.01                0.0005   \n",
      "3          [10]  momentum       0.9           0.01                0.0005   \n",
      "4          [10]      adam       0.0           0.01                0.0005   \n",
      "\n",
      "   epochs  repeat       mse       mae        r2  \n",
      "0    1000       1  0.187111  0.321201  0.784497  \n",
      "1    1000       1  0.144203  0.263322  0.833916  \n",
      "2    1000       1  0.228525  0.369735  0.736800  \n",
      "3    1000       1  1.695980  1.090384 -0.953321  \n",
      "4    1000       1  0.193263  0.322098  0.777413  \n",
      "   hidden_layers  learning_rate optimizer  momentum   avg_mse   avg_mae  \\\n",
      "0       [10, 10]           0.01      adam       0.0  0.207401  0.339590   \n",
      "1       [10, 10]           0.01      adam       0.9  0.240345  0.372800   \n",
      "2       [10, 10]           0.01        gd       0.0  0.150066  0.282600   \n",
      "3       [10, 10]           0.01        gd       0.9  0.196797  0.334598   \n",
      "4       [10, 10]           0.01  momentum       0.0  0.162066  0.283521   \n",
      "5       [10, 10]           0.01  momentum       0.9  0.211527  0.345740   \n",
      "6           [10]           0.01      adam       0.0  0.190806  0.322939   \n",
      "7           [10]           0.01      adam       0.9  0.202845  0.320014   \n",
      "8           [10]           0.01        gd       0.0  0.173816  0.307162   \n",
      "9           [10]           0.01        gd       0.9  0.174135  0.304307   \n",
      "10          [10]           0.01  momentum       0.0  0.206942  0.337760   \n",
      "11          [10]           0.01  momentum       0.9  0.780618  0.640743   \n",
      "\n",
      "      avg_r2  best_mse   best_r2  \n",
      "0   0.761129  0.189566  0.781671  \n",
      "1   0.723186  0.221370  0.745041  \n",
      "2   0.827163  0.136261  0.843063  \n",
      "3   0.773342  0.183653  0.788480  \n",
      "4   0.813343  0.139769  0.839023  \n",
      "5   0.756377  0.194076  0.776475  \n",
      "6   0.780241  0.176495  0.796725  \n",
      "7   0.766376  0.169112  0.805228  \n",
      "8   0.799810  0.148522  0.828942  \n",
      "9   0.799442  0.144203  0.833916  \n",
      "10  0.761657  0.153052  0.823725  \n",
      "11  0.100935  0.208434  0.759939  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def initialize_weights_bias(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "        biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "    return weights, biases\n",
    "\n",
    "# Podział danych na zbiory treningowe, generalizacyjne i walidacyjne\n",
    "def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "    np.random.shuffle(data) # tasowanie danych\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio) \n",
    "    validation_size = int(len(data) * validation_ratio)\n",
    "\n",
    "    train_data = data[:train_size] # wybiera obserwacje do liczby \"train_size\"\n",
    "    validation_data = data[train_size:train_size + validation_size] # wybiera obserwacje od \"train_size\" do sumy \"train_size\" i \"validation_size\"\n",
    "    test_data = data[train_size + validation_size:] # wybiera obserwacje od powyzszej sumy do końca\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Funkcja dostosowująca tempa nauki\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=1e-6):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.05\n",
    "    else:\n",
    "        learning_rate *= 0.7\n",
    "\n",
    "    if abs(mse - previous_mse) < threshold:\n",
    "        learning_rate *= learning_rate_adjust\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "\n",
    "def train(X, y, learning_rate, learning_rate_adjust, epochs,\n",
    "          hidden_layers_sizes, optimizer='gd', momentum=0.9,\n",
    "          beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights, biases = initialize_weights_bias(input_size, hidden_layers_sizes, output_size)\n",
    "\n",
    "    # struktury dla optimizerów\n",
    "    velocities_w = [np.zeros_like(w) for w in weights]\n",
    "    velocities_b = [np.zeros_like(b) for b in biases]\n",
    "    m_w = [np.zeros_like(w) for w in weights]\n",
    "    v_w = [np.zeros_like(w) for w in weights]\n",
    "    m_b = [np.zeros_like(b) for b in biases]\n",
    "    v_b = [np.zeros_like(b) for b in biases]\n",
    "\n",
    "    prev_loss = np.inf\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ---- Forward pass ----\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i in range(len(weights)):\n",
    "            z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "            zs.append(z)\n",
    "            activations.append(z if i == len(weights)-1 else relu(z))\n",
    "        predicted_output = activations[-1]\n",
    "\n",
    "        # ---- Backprop ----\n",
    "        error = predicted_output - y\n",
    "        loss = np.mean(error ** 2)\n",
    "        learning_rate = adjust_learning_rate(learning_rate, loss, prev_loss, learning_rate_adjust)\n",
    "        prev_loss = loss\n",
    "\n",
    "        deltas = [error]\n",
    "        for i in range(len(weights)-1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "\n",
    "        # ---- Aktualizacja wag i biasów ----\n",
    "        for i in range(len(weights)):\n",
    "            grad_w = activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "            grad_b = np.mean(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "            if optimizer == 'gd':\n",
    "                weights[i] -= learning_rate * grad_w\n",
    "                biases[i]  -= learning_rate * grad_b\n",
    "\n",
    "            elif optimizer == 'momentum':\n",
    "                velocities_w[i] = momentum * velocities_w[i] - learning_rate * grad_w\n",
    "                velocities_b[i] = momentum * velocities_b[i] - learning_rate * grad_b\n",
    "                weights[i] += velocities_w[i]\n",
    "                biases[i]  += velocities_b[i]\n",
    "\n",
    "            elif optimizer == 'adam':\n",
    "                # Wagi\n",
    "                m_w[i] = beta1 * m_w[i] + (1 - beta1) * grad_w\n",
    "                v_w[i] = beta2 * v_w[i] + (1 - beta2) * (grad_w ** 2)\n",
    "                m_hat_w = m_w[i] / (1 - beta1 ** epoch)\n",
    "                v_hat_w = v_w[i] / (1 - beta2 ** epoch)\n",
    "                weights[i] -= learning_rate * m_hat_w / (np.sqrt(v_hat_w) + epsilon)\n",
    "\n",
    "                # Biasy\n",
    "                m_b[i] = beta1 * m_b[i] + (1 - beta1) * grad_b\n",
    "                v_b[i] = beta2 * v_b[i] + (1 - beta2) * (grad_b ** 2)\n",
    "                m_hat_b = m_b[i] / (1 - beta1 ** epoch)\n",
    "                v_hat_b = v_b[i] / (1 - beta2 ** epoch)\n",
    "                biases[i]  -= learning_rate * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "def predict(X, weights, biases):\n",
    "    output = X\n",
    "    for i in range(len(weights)):\n",
    "        output = np.dot(output, weights[i]) + biases[i]\n",
    "        if i < len(weights) - 1:\n",
    "            output = relu(output)\n",
    "    return output\n",
    "\n",
    "def mae_np(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Parametry sieci\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.0005]\n",
    "epochses = [1000]\n",
    "repeat = 3\n",
    "optimizers = ['gd', 'momentum', 'adam']\n",
    "momentums = [0.0, 0.9]\n",
    "# gd → zwykły gradient prosty\n",
    "# momentum → gradient z momentem\n",
    "\n",
    "# Warstwy\n",
    "hidden_layers_sizes_list = [\n",
    "    [10],         \n",
    "    [10, 10]       \n",
    "]\n",
    "\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "# Przechowywanie wyników dla różnych konfiguracji warstw\n",
    "results = []\n",
    "\n",
    "# Testowanie\n",
    "for hidden_layers_sizes in hidden_layers_sizes_list:  \n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for lr_adj in learning_rate_adjusts:\n",
    "                for epochs in epochses:\n",
    "                    for optimizer in optimizers:\n",
    "                        for momentum in momentums:\n",
    "                            trained_weights, trained_biases = train(\n",
    "                                X_train, y_train, lr, lr_adj, epochs, hidden_layers_sizes, optimizer=optimizer, momentum=momentum\n",
    "                            )\n",
    "                            predictions_train = predict(X_train, trained_weights, trained_biases)\n",
    "                            predictions_validation = predict(X_validation, trained_weights, trained_biases)\n",
    "                            predictions_test = predict(X_test, trained_weights, trained_biases)\n",
    "\n",
    "\n",
    "                            # Odwracanie skalowania y\n",
    "                            y_pred_test_real = scaler_y.inverse_transform(predictions_test)\n",
    "                            y_test_real = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "                            # --- METRYKI TRAIN ---\n",
    "                            mse_train, mae_train, r2_train = calculate_regression_metrics(\n",
    "                                y_train, predictions_train\n",
    "                            )\n",
    "\n",
    "                            # --- METRYKI VALIDATION ---\n",
    "                            mse_val, mae_val, r2_val = calculate_regression_metrics(\n",
    "                                y_validation, predictions_validation\n",
    "                            )\n",
    "\n",
    "                            # --- METRYKI TEST ---\n",
    "                            mse_test, mae_test, r2_test = calculate_regression_metrics(\n",
    "                                y_test, predictions_test\n",
    "                            )\n",
    "\n",
    "                            # --- ZAPIS WYNIKÓW ---\n",
    "                            results.append({\n",
    "                                'hidden_layers': str(hidden_layers_sizes),\n",
    "                                'optimizer': optimizer,\n",
    "                                'momentum': momentum,\n",
    "                                'learning_rate': lr,\n",
    "                                'learning_rate_adjust': lr_adj,\n",
    "                                'epochs': epochs,\n",
    "                                'repeat': r,\n",
    "\n",
    "                                # TRAIN\n",
    "                                'mse_train': mse_train,\n",
    "                                'mae_train': mae_train,\n",
    "                                'r2_train': r2_train,\n",
    "\n",
    "                                # VALIDATION\n",
    "                                'mse_val': mse_val,\n",
    "                                'mae_val': mae_val,\n",
    "                                'r2_val': r2_val,\n",
    "\n",
    "                                # TEST\n",
    "                                'mse_test': mse_test,\n",
    "                                'mae_test': mae_test,\n",
    "                                'r2_test': r2_test\n",
    "                            })\n",
    "\n",
    "\n",
    "# ---- Tworzenie DataFrame dla własnej MLP ----\n",
    "df_mlp = pd.DataFrame(results)\n",
    "\n",
    "# ---- ZAPIS SZCZEGÓŁOWY ----\n",
    "df_mlp.to_excel('wyniki_regresja_szczegolowe.xlsx', sheet_name='MLP_szczegolowy', index=False)\n",
    "\n",
    "# grupujemy po wszystkich konfiguracjach (układ warstw, optymalizator, momentum, lr itp.)\n",
    "summary_mlp = df_mlp.groupby(['hidden_layers', 'optimizer', 'momentum', 'learning_rate', 'learning_rate_adjust', 'epochs']) \\\n",
    "    .agg({\n",
    "        # TRAIN\n",
    "        'mse_train': ['mean', 'min', 'max'],\n",
    "        'mae_train': ['mean', 'min', 'max'],\n",
    "        'r2_train': ['mean', 'min', 'max'],\n",
    "\n",
    "        # VALIDATION\n",
    "        'mse_val': ['mean', 'min', 'max'],\n",
    "        'mae_val': ['mean', 'min', 'max'],\n",
    "        'r2_val': ['mean', 'min', 'max'],\n",
    "\n",
    "        # TEST\n",
    "        'mse_test': ['mean', 'min', 'max'],\n",
    "        'mae_test': ['mean', 'min', 'max'],\n",
    "        'r2_test': ['mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "\n",
    "# ---- ZAPIS ZAGREGOWANY ----\n",
    "summary_mlp.to_excel('wyniki_regresja_zagregowane.xlsx', sheet_name='MLP_zagregowany', index=False)\n",
    "\n",
    "print(\"Pliki Excel zostały wygenerowane:\")\n",
    "print(\"1. wyniki_regresja_szczegolowe.xlsx\")\n",
    "print(\"2. wyniki_regresja_zagregowane.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
