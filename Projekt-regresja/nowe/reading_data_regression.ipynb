{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f02004d",
   "metadata": {},
   "source": [
    "## \n",
    " sieć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50344a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dane wspólne gotowe. Podział: Train=6549, Val=1404, Test=1404\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "data_common = pd.read_excel(\"AirQualityUCI_outcome.xlsx\")\n",
    "\n",
    "features = ['PT08.S1(CO)','NMHC(GT)','C6H6(GT)','PT08.S2(NMHC)','NOx(GT)',\n",
    "            'PT08.S3(NOx)','NO2(GT)','PT08.S4(NO2)','PT08.S5(O3)','T','RH','AH']\n",
    "target = 'CO(GT)'\n",
    "\n",
    "\n",
    "X_train_raw = data_common[data_common['Set']=='train'][features].values\n",
    "y_train_raw = data_common[data_common['Set']=='train'][target].values.reshape(-1,1)\n",
    "\n",
    "X_val_raw = data_common[data_common['Set']=='validation'][features].values\n",
    "y_val_raw = data_common[data_common['Set']=='validation'][target].values.reshape(-1,1)\n",
    "\n",
    "X_test_raw = data_common[data_common['Set']=='test'][features].values\n",
    "y_test_raw = data_common[data_common['Set']=='test'][target].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "scaler_X = StandardScaler().fit(X_train_raw)\n",
    "scaler_y = StandardScaler().fit(y_train_raw)\n",
    "\n",
    "X_train, y_train = scaler_X.transform(X_train_raw), scaler_y.transform(y_train_raw)\n",
    "X_val, y_val = scaler_X.transform(X_val_raw), scaler_y.transform(y_val_raw)\n",
    "X_test, y_test = scaler_X.transform(X_test_raw), scaler_y.transform(y_test_raw)\n",
    "\n",
    "print(f\"Dane wspólne gotowe. Podział: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def initialize_weights_bias(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        \n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "        biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "    return weights, biases\n",
    "\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=1e-6):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.05\n",
    "    else:\n",
    "        learning_rate *= 0.7\n",
    "    if abs(mse - previous_mse) < threshold:\n",
    "        learning_rate *= learning_rate_adjust\n",
    "    return learning_rate\n",
    "\n",
    "def predict(X, weights, biases):\n",
    "    output = X\n",
    "    for i in range(len(weights)):\n",
    "        output = np.dot(output, weights[i]) + biases[i]\n",
    "        if i < len(weights) - 1:\n",
    "            output = relu(output)\n",
    "    return output\n",
    "\n",
    "def train(X, y, learning_rate, learning_rate_adjust, epochs,\n",
    "          hidden_layers_sizes, optimizer='gd', momentum=0.9,\n",
    "          beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights, biases = initialize_weights_bias(input_size, hidden_layers_sizes, output_size)\n",
    "\n",
    "    \n",
    "    v_w = [np.zeros_like(w) for w in weights]\n",
    "    v_b = [np.zeros_like(b) for b in biases]\n",
    "    m_w, v_sq_w = [np.zeros_like(w) for w in weights], [np.zeros_like(w) for w in weights]\n",
    "    m_b, v_sq_b = [np.zeros_like(b) for b in biases], [np.zeros_like(b) for b in biases]\n",
    "\n",
    "    prev_loss = np.inf\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i in range(len(weights)):\n",
    "            z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "            zs.append(z)\n",
    "            \n",
    "            activations.append(z if i == len(weights)-1 else relu(z))\n",
    "        \n",
    "        \n",
    "        error = activations[-1] - y\n",
    "        loss = np.mean(error ** 2)\n",
    "        \n",
    "        \n",
    "        learning_rate = adjust_learning_rate(learning_rate, loss, prev_loss, learning_rate_adjust)\n",
    "        prev_loss = loss\n",
    "\n",
    "        deltas = [error]\n",
    "        for i in range(len(weights)-1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            grad_w = activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "            grad_b = np.mean(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "            if optimizer == 'gd':\n",
    "                weights[i] -= learning_rate * grad_w\n",
    "                biases[i]  -= learning_rate * grad_b\n",
    "            \n",
    "            elif optimizer == 'momentum':\n",
    "                v_w[i] = momentum * v_w[i] - learning_rate * grad_w\n",
    "                v_b[i] = momentum * v_b[i] - learning_rate * grad_b\n",
    "                weights[i] += v_w[i]\n",
    "                biases[i]  += v_b[i]\n",
    "            \n",
    "            elif optimizer == 'adam':\n",
    "                m_w[i] = beta1 * m_w[i] + (1 - beta1) * grad_w\n",
    "                v_sq_w[i] = beta2 * v_sq_w[i] + (1 - beta2) * (grad_w ** 2)\n",
    "                m_hat = m_w[i] / (1 - beta1**epoch)\n",
    "                v_hat = v_sq_w[i] / (1 - beta2**epoch)\n",
    "                weights[i] -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "                \n",
    "                m_b[i] = beta1 * m_b[i] + (1 - beta1) * grad_b\n",
    "                v_sq_b[i] = beta2 * v_sq_b[i] + (1 - beta2) * (grad_b ** 2)\n",
    "                m_hat_b = m_b[i] / (1 - beta1**epoch)\n",
    "                v_hat_b = v_sq_b[i] / (1 - beta2**epoch)\n",
    "                biases[i] -= learning_rate * m_hat_b / (np.sqrt(v_hat_b) + epsilon)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a19e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam pełną analizę MLP dla Air Quality (4 parametry x 4 wartości x 5 powtórzeń)...\n",
      "\n",
      "Badam: learning_rate\n",
      "  Wartość: 0.001... Done.\n",
      "  Wartość: 0.01... Done.\n",
      "  Wartość: 0.05... Done.\n",
      "  Wartość: 0.1... Done.\n",
      "\n",
      "Badam: epochs\n",
      "  Wartość: 500... Done.\n",
      "  Wartość: 1000... Done.\n",
      "  Wartość: 1500... Done.\n",
      "  Wartość: 2000... Done.\n",
      "\n",
      "Badam: layers\n",
      "  Wartość: [10]... Done.\n",
      "  Wartość: [10, 10]... Done.\n",
      "  Wartość: [20, 10]... Done.\n",
      "  Wartość: [32, 16, 8]... Done.\n",
      "\n",
      "Badam: momentum\n",
      "  Wartość: 0.0... Done.\n",
      "  Wartość: 0.5... Done.\n",
      "  Wartość: 0.9... Done.\n",
      "  Wartość: 0.98... Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "test_params = {\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'epochs': [500, 1000, 1500, 2000],\n",
    "    'layers': [[10], [10, 10], [20, 10], [32, 16, 8]],\n",
    "    'momentum': [0.0, 0.5, 0.9, 0.98]\n",
    "}\n",
    "\n",
    "\n",
    "base = {'lr': 0.01, 'epochs': 1000, 'layers': [10, 10], 'mom': 0.9}\n",
    "\n",
    "results_common_mlp = []\n",
    "\n",
    "print(\"Rozpoczynam pełną analizę MLP dla Air Quality (4 parametry x 4 wartości x 5 powtórzeń)...\")\n",
    "\n",
    "for param_name, values in test_params.items():\n",
    "    print(f\"\\nBadam: {param_name}\")\n",
    "    for val in values:\n",
    "        print(f\"  Wartość: {val}...\", end=\" \")\n",
    "        \n",
    "        \n",
    "        curr_lr = val if param_name == 'learning_rate' else base['lr']\n",
    "        curr_epochs = val if param_name == 'epochs' else base['epochs']\n",
    "        curr_layers = val if param_name == 'layers' else base['layers']\n",
    "        curr_mom = val if param_name == 'momentum' else base['mom']\n",
    "        \n",
    "        for r in range(1, 6): \n",
    "            w, b = train(X_train, y_train, learning_rate=curr_lr, learning_rate_adjust=0.0005, \n",
    "                         epochs=curr_epochs, hidden_layers_sizes=curr_layers, \n",
    "                         optimizer='momentum', momentum=curr_mom)\n",
    "            \n",
    "            \n",
    "            for set_name, Xs, ys in [('train', X_train, y_train), ('val', X_val, y_val), ('test', X_test, y_test)]:\n",
    "                preds = predict(Xs, w, b)\n",
    "                \n",
    "                y_real = scaler_y.inverse_transform(ys)\n",
    "                p_real = scaler_y.inverse_transform(preds)\n",
    "                \n",
    "                results_common_mlp.append({\n",
    "                    'tested_param': param_name,\n",
    "                    'val': str(val),\n",
    "                    'run': r,\n",
    "                    'set': set_name,\n",
    "                    'mse': mean_squared_error(y_real, p_real),\n",
    "                    'mae': mean_absolute_error(y_real, p_real),\n",
    "                    'r2': r2_score(y_real, p_real)\n",
    "                })\n",
    "        print(\"Done.\")\n",
    "\n",
    "\n",
    "df_common_mlp = pd.DataFrame(results_common_mlp)\n",
    "df_common_mlp.to_excel('common_MLP_full_analysis.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cc17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Badanie LSTM - Parametr: layers\n",
      "  Wartość 1... Done.\n",
      "  Wartość 2... Done.\n",
      "  Wartość 3... Done.\n",
      "  Wartość 4... Done.\n",
      "\n",
      "Badanie LSTM - Parametr: units\n",
      "  Wartość 16... Done.\n",
      "  Wartość 32... Done.\n",
      "  Wartość 64... Done.\n",
      "  Wartość 128... Done.\n",
      "\n",
      "Badanie LSTM - Parametr: lr\n",
      "  Wartość 0.0001... Done.\n",
      "  Wartość 0.001... Done.\n",
      "  Wartość 0.01... Done.\n",
      "  Wartość 0.05... Done.\n",
      "\n",
      "Badanie LSTM - Parametr: epochs\n",
      "  Wartość 10... Done.\n",
      "  Wartość 20... Done.\n",
      "  Wartość 50... Done.\n",
      "  Wartość 80... Done.\n",
      "\n",
      "Badanie CNN - Parametr: layers\n",
      "  Wartość 1... Done.\n",
      "  Wartość 2... Done.\n",
      "  Wartość 3... Done.\n",
      "  Wartość 4... Done.\n",
      "\n",
      "Badanie CNN - Parametr: units\n",
      "  Wartość 16... Done.\n",
      "  Wartość 32... Done.\n",
      "  Wartość 64... Done.\n",
      "  Wartość 128... Done.\n",
      "\n",
      "Badanie CNN - Parametr: lr\n",
      "  Wartość 0.0001... Done.\n",
      "  Wartość 0.001... Done.\n",
      "  Wartość 0.01... Done.\n",
      "  Wartość 0.05... Done.\n",
      "\n",
      "Badanie CNN - Parametr: epochs\n",
      "  Wartość 10... Done.\n",
      "  Wartość 20... Done.\n",
      "  Wartość 50... Done.\n",
      "  Wartość 80... Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_common_model(model_type, layers, units, lr):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(10, X_train.shape[1]))) \n",
    "    for i in range(layers):\n",
    "        if model_type == 'LSTM':\n",
    "            model.add(LSTM(units, return_sequences=(i < layers - 1), activation='relu'))\n",
    "        else: \n",
    "            model.add(Conv1D(filters=units, kernel_size=3, padding='same', activation='relu'))\n",
    "    if model_type == 'CNN': model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "    return model\n",
    "\n",
    "def run_comprehensive_dl_common(model_type):\n",
    "    results = []\n",
    "    tests = {\n",
    "        'layers': [1, 2, 3, 4],\n",
    "        'units': [16, 32, 64, 128],\n",
    "        'lr': [0.0001, 0.001, 0.01, 0.05],\n",
    "        'epochs': [10, 20, 50, 80]\n",
    "    }\n",
    "    base = {'layers': 1, 'units': 32, 'lr': 0.001, 'epochs': 20}\n",
    "\n",
    "    for p_name, p_values in tests.items():\n",
    "        print(f\"\\nBadanie {model_type} - Parametr: {p_name}\")\n",
    "        for val in p_values:\n",
    "            print(f\"  Wartość {val}...\", end=\" \")\n",
    "            for r in range(1, 6):\n",
    "                cfg = base.copy()\n",
    "                cfg[p_name] = val\n",
    "                \n",
    "                model = build_common_model(model_type, cfg['layers'], cfg['units'], cfg['lr'])\n",
    "                model.fit(X_train_seq, y_train_seq, epochs=cfg['epochs'], batch_size=32, verbose=0)\n",
    "                \n",
    "                \n",
    "                pred = model.predict(X_test_seq, verbose=0)\n",
    "                p_real = scaler_y.inverse_transform(pred)\n",
    "                y_real = scaler_y.inverse_transform(y_test_seq)\n",
    "                \n",
    "                results.append({\n",
    "                    'model': model_type, 'tested_param': p_name, 'val': str(val), 'run': r,\n",
    "                    'mse': mean_squared_error(y_real, p_real),\n",
    "                    'mae': mean_absolute_error(y_real, p_real),\n",
    "                    'r2': r2_score(y_real, p_real)\n",
    "                })\n",
    "            print(\"Done.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "res_lstm_common = run_comprehensive_dl_common('LSTM')\n",
    "res_cnn_common = run_comprehensive_dl_common('CNN')\n",
    "\n",
    "\n",
    "pd.concat([pd.DataFrame(res_lstm_common), pd.DataFrame(res_cnn_common)]).to_excel('common_Advanced_full_analysis.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
