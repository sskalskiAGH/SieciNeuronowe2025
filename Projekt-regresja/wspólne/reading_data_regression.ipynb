{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a11941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba całkowicie pustych wierszy: 114\n",
      "\n",
      "--- Informacje o wymiarach ---\n",
      "Liczba wierszy: 9357\n",
      "Liczba kolumn: 15\n",
      "Liczba wierszy z brakującymi danymi: 0\n",
      "\n",
      "--- Pierwsze 5 wierszy ---\n",
      "         Date      Time  CO(GT)  PT08.S1(CO)  NMHC(GT)  C6H6(GT)  \\\n",
      "0  10/03/2004  18.00.00     2.6       1360.0     150.0      11.9   \n",
      "1  10/03/2004  19.00.00     2.0       1292.0     112.0       9.4   \n",
      "2  10/03/2004  20.00.00     2.2       1402.0      88.0       9.0   \n",
      "3  10/03/2004  21.00.00     2.2       1376.0      80.0       9.2   \n",
      "4  10/03/2004  22.00.00     1.6       1272.0      51.0       6.5   \n",
      "\n",
      "   PT08.S2(NMHC)  NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)  \\\n",
      "0         1046.0    166.0        1056.0    113.0        1692.0       1268.0   \n",
      "1          955.0    103.0        1174.0     92.0        1559.0        972.0   \n",
      "2          939.0    131.0        1140.0    114.0        1555.0       1074.0   \n",
      "3          948.0    172.0        1092.0    122.0        1584.0       1203.0   \n",
      "4          836.0    131.0        1205.0    116.0        1490.0       1110.0   \n",
      "\n",
      "      T    RH      AH  \n",
      "0  13.6  48.9  0.7578  \n",
      "1  13.3  47.7  0.7255  \n",
      "2  11.9  54.0  0.7502  \n",
      "3  11.0  60.0  0.7867  \n",
      "4  11.2  59.6  0.7888  \n",
      "\n",
      "--- Ciągłość czasu ---\n",
      "\n",
      "--- Brakujące odczyty (łącznie: 0) ---\n",
      "Brak brakujących dni/godzin w ciągłości odczytów.\n",
      "\n",
      "--- Ujemne wartości ---\n",
      "\n",
      "--- Wiersze z ujemnymi wartościami: 13 ---\n",
      "      CO(GT)  PT08.S1(CO)  NMHC(GT)  C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
      "8530    0.20        716.0     275.0       0.2          390.0     29.0   \n",
      "8531    0.20        731.0     275.0       0.3          408.0     32.0   \n",
      "8532    0.20        735.0     275.0       0.3          407.0     37.0   \n",
      "8533    0.50        779.0     275.0       1.0          500.0    134.0   \n",
      "8534    1.50        915.0     275.0       5.2          774.0    358.0   \n",
      "8552    0.60        761.0     275.0       1.3          525.0     79.0   \n",
      "8553    0.50        788.0     275.0       0.9          486.0     80.0   \n",
      "8554    0.45        790.0     275.0       0.7          471.0     81.0   \n",
      "8555    0.40        804.0     275.0       0.9          489.0     77.0   \n",
      "8556    0.50        831.0     275.0       1.4          537.0    124.0   \n",
      "8557    0.70        881.0     275.0       3.5          680.0    159.0   \n",
      "8558    2.40       1099.0     275.0      11.0         1013.0    396.0   \n",
      "8559    2.80       1100.0     275.0      12.4         1065.0    479.0   \n",
      "\n",
      "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)    T    RH      AH  \n",
      "8530        1804.0     27.0         551.0        221.0 -0.1  37.5  0.2326  \n",
      "8531        1727.0     30.0         561.0        225.0 -0.3  38.4  0.2347  \n",
      "8532        1677.0     33.0         579.0        232.0 -0.6  40.1  0.2404  \n",
      "8533        1361.0     84.0         652.0        313.0 -0.6  41.4  0.2478  \n",
      "8534         911.0    147.0         839.0        629.0 -0.2  40.2  0.2478  \n",
      "8552        1307.0     67.0         605.0        332.0 -0.1  31.9  0.1975  \n",
      "8553        1294.0     71.0         650.0        420.0 -1.1  41.4  0.2379  \n",
      "8554        1302.0     75.0         669.0        532.0 -1.4  44.9  0.2536  \n",
      "8555        1262.0     73.0         698.0        630.0 -1.3  46.3  0.2635  \n",
      "8556        1158.0     96.0         722.0        768.0 -1.2  47.2  0.2700  \n",
      "8557        1023.0    110.0         813.0        866.0 -1.3  47.5  0.2702  \n",
      "8558         690.0    150.0        1056.0       1254.0 -1.9  51.4  0.2801  \n",
      "8559         669.0    169.0        1072.0       1347.0 -0.5  45.9  0.2763  \n",
      "\n",
      "Jeśli ujemne wartości są tylko w kolumnie T (Temperatura w stopniach Celcjusza), to jest to poprawnie.\n",
      "Aktualna liczba kolumn: 15\n",
      "\n",
      "--- Podział na zbiory train, validation i test ---\n",
      "Set\n",
      "train         6549\n",
      "validation    1404\n",
      "test          1404\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--> Zapisano gotowy plik: AirQualityUCI_outcome.xlsx.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nazwa_pliku = 'AirQualityUCI.csv'\n",
    "\n",
    "try:\n",
    "\n",
    "    df = pd.read_csv(nazwa_pliku, sep=';', decimal=',')\n",
    "    \n",
    "    ile_pustych = df.isna().all(axis=1).sum()\n",
    "    print(f\"Liczba całkowicie pustych wierszy: {ile_pustych}\")\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    df = df.iloc[:, :-2]\n",
    "    \n",
    "    print(\"\\n--- Informacje o wymiarach ---\")\n",
    "    wiersze, kolumny = df.shape\n",
    "    print(f\"Liczba wierszy: {wiersze}\")\n",
    "    print(f\"Liczba kolumn: {kolumny}\")\n",
    "    \n",
    "    # Odczyt \"-200\" to błąd czujnika\n",
    "    df.replace(-200, np.nan, inplace=True)\n",
    "    # Uzupełnianie braków metodą interpolacji liniowej\n",
    "    cols_numeric = df.select_dtypes(include=[np.number]).columns\n",
    "    df[cols_numeric] = df[cols_numeric].interpolate(method='linear', limit_direction='both')\n",
    "    \n",
    "    liczba_wierszy_z_nan = df.isna().any(axis=1).sum()\n",
    "    print(f\"Liczba wierszy z brakującymi danymi: {liczba_wierszy_z_nan}\")\n",
    "   \n",
    "    print(\"\\n--- Pierwsze 5 wierszy ---\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\n--- Ciągłość czasu ---\")\n",
    "    # Tworzę dodatkową kolumną Datetime\n",
    "    df['Time_Clean'] = df['Time'].astype(str).str.replace('.', ':', regex=False)\n",
    "    df['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time_Clean'], dayfirst=True)\n",
    "    # Generuję pełny zakres DateTime\n",
    "    full_range = pd.date_range(start=df['Datetime'].min(), end=df['Datetime'].max(), freq='H')\n",
    "    # Porównuję faktyczne dane w zbiorze z tymi wygenerowanymi\n",
    "    missing_dates = set(full_range) - set(df['Datetime'])\n",
    "    \n",
    "    print(f\"\\n--- Brakujące odczyty (łącznie: {len(missing_dates)}) ---\")\n",
    "    if missing_dates:\n",
    "        print(f\"Liczba brakujących wpisów: {len(missing_dates)}\")\n",
    "        print(\"Wszystkie brakujące daty:\")\n",
    "        print(*sorted(missing_dates), sep='\\n')\n",
    "    else:\n",
    "        print(\"Brak brakujących dni/godzin w ciągłości odczytów.\")\n",
    "        \n",
    "    print(\"\\n--- Ujemne wartości ---\")\n",
    "    maska_ujemne = (df[cols_numeric] < 0).any(axis=1)\n",
    "    negative_rows = df[maska_ujemne]\n",
    "    \n",
    "    print(f\"\\n--- Wiersze z ujemnymi wartościami: {len(negative_rows)} ---\")\n",
    "    if not negative_rows.empty:\n",
    "        print(negative_rows[cols_numeric])\n",
    "        print(\"\\nJeśli ujemne wartości są tylko w kolumnie T (Temperatura w stopniach Celcjusza), to jest to poprawnie.\")\n",
    "    else:\n",
    "        print(\"Brak ujemnych wartości po interpolacji.\")\n",
    "        \n",
    "    #Usunięcie niepotrzebnych kolumn\n",
    "    df.drop(columns=['Datetime', 'Time_Clean'], inplace=True)\n",
    "    print(f\"Aktualna liczba kolumn: {df.shape[1]}\")\n",
    "    \n",
    "    print(\"\\n--- Podział na zbiory train, validation i test ---\")\n",
    "    temp_time = df['Time'].astype(str).str.replace('.', ':', regex=False)\n",
    "    df['Temp_Datetime'] = pd.to_datetime(df['Date'] + ' ' + temp_time, dayfirst=True)\n",
    "    df.sort_values(by='Temp_Datetime', inplace=True)\n",
    "    \n",
    "    n = len(df)\n",
    "    train_end = int(0.70 * n)\n",
    "    val_end = int(0.85 * n)\n",
    "    \n",
    "    # Tworzenie kolumny Set\n",
    "    df['Set'] = 'test' # Domyślnie wypełnia 'test' (dla ostatnich 15%)\n",
    "    df.iloc[:train_end, df.columns.get_loc('Set')] = 'train'         # Pierwsze 70%\n",
    "    df.iloc[train_end:val_end, df.columns.get_loc('Set')] = 'validation' # Kolejne 15%\n",
    "    df.drop(columns=['Temp_Datetime'], inplace=True)\n",
    "    print(df['Set'].value_counts(sort=False))\n",
    "    \n",
    "    df.to_excel(\"AirQualityUCI_outcome.xlsx\", index=False)\n",
    "    print(\"\\n--> Zapisano gotowy plik: AirQualityUCI_outcome.xlsx.xlsx\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Nie znaleziono pliku. Sprawdź czy nazwa i ścieżka są poprawne.\")\n",
    "except Exception as e:\n",
    "    print(f\"Wystąpił błąd: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02004d",
   "metadata": {},
   "source": [
    "## Nasza sieć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f270c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(6549, 12), y=(6549, 1)\n",
      "Validation: X=(1404, 12), y=(1404, 1)\n",
      "Test: X=(1404, 12), y=(1404, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_excel(\"AirQualityUCI_outcome.xlsx\")\n",
    "\n",
    "features = ['PT08.S1(CO)','NMHC(GT)','C6H6(GT)','PT08.S2(NMHC)','NOx(GT)',\n",
    "            'PT08.S3(NOx)','NO2(GT)','PT08.S4(NO2)','PT08.S5(O3)','T','RH','AH']\n",
    "target = 'CO(GT)'\n",
    "\n",
    "# Tworzymy zbiory według kolumny 'Set'\n",
    "X_train = data[data['Set']=='train'][features].values\n",
    "y_train = data[data['Set']=='train'][target].values.reshape(-1,1)\n",
    "\n",
    "X_validation = data[data['Set']=='validation'][features].values\n",
    "y_validation = data[data['Set']=='validation'][target].values.reshape(-1,1)\n",
    "\n",
    "X_test = data[data['Set']=='test'][features].values\n",
    "y_test = data[data['Set']=='test'][target].values.reshape(-1,1)\n",
    "\n",
    "# Standaryzacja\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_validation = scaler_X.transform(X_validation)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_validation = scaler_y.transform(y_validation)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation: X={X_validation.shape}, y={y_validation.shape}\")\n",
    "print(f\"Test: X={X_test.shape}, y={y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb73e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layers optimizer  momentum  learning_rate  learning_rate_adjust  \\\n",
      "0           [10]        gd       0.0           0.01                0.0005   \n",
      "1           [10]        gd       0.9           0.01                0.0005   \n",
      "2           [10]  momentum       0.0           0.01                0.0005   \n",
      "3           [10]  momentum       0.9           0.01                0.0005   \n",
      "4           [10]        gd       0.0           0.01                0.0005   \n",
      "5           [10]        gd       0.9           0.01                0.0005   \n",
      "6           [10]  momentum       0.0           0.01                0.0005   \n",
      "7           [10]  momentum       0.9           0.01                0.0005   \n",
      "8           [10]        gd       0.0           0.01                0.0005   \n",
      "9           [10]        gd       0.9           0.01                0.0005   \n",
      "10          [10]  momentum       0.0           0.01                0.0005   \n",
      "11          [10]  momentum       0.9           0.01                0.0005   \n",
      "12      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "13      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "14      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "15      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "16      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "17      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "18      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "19      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "20      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "21      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "22      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "23      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "\n",
      "    epochs  repeat       mse       mae        r2  \n",
      "0     1000       1  0.167228  0.291325  0.807398  \n",
      "1     1000       1  0.196579  0.342025  0.773593  \n",
      "2     1000       1  0.148805  0.283710  0.828616  \n",
      "3     1000       1  0.175968  0.321114  0.797332  \n",
      "4     1000       2  0.248759  0.385579  0.713495  \n",
      "5     1000       2  0.151605  0.279641  0.825391  \n",
      "6     1000       2  0.214563  0.359657  0.752881  \n",
      "7     1000       2  0.201922  0.333988  0.767439  \n",
      "8     1000       3  0.179123  0.326492  0.793698  \n",
      "9     1000       3  0.209401  0.352236  0.758825  \n",
      "10    1000       3  0.169289  0.307702  0.805023  \n",
      "11    1000       3  0.641157  0.574757  0.261556  \n",
      "12    1000       1  0.194764  0.337885  0.775684  \n",
      "13    1000       1  0.220043  0.365925  0.746568  \n",
      "14    1000       1  0.155224  0.292152  0.821223  \n",
      "15    1000       1  0.195623  0.336096  0.774694  \n",
      "16    1000       2  0.179039  0.293845  0.793795  \n",
      "17    1000       2  0.170788  0.300756  0.803297  \n",
      "18    1000       2  0.272909  0.404962  0.685681  \n",
      "19    1000       2  0.202585  0.332734  0.766676  \n",
      "20    1000       3  0.185588  0.324785  0.786252  \n",
      "21    1000       3  0.173855  0.303473  0.799765  \n",
      "22    1000       3  0.210222  0.336391  0.757880  \n",
      "23    1000       3  0.673136  0.711975  0.224725  \n",
      "  hidden_layers optimizer  momentum  learning_rate  learning_rate_adjust  \\\n",
      "0          [10]        gd       0.0           0.01                0.0005   \n",
      "1          [10]        gd       0.9           0.01                0.0005   \n",
      "2          [10]  momentum       0.0           0.01                0.0005   \n",
      "3          [10]  momentum       0.9           0.01                0.0005   \n",
      "4          [10]        gd       0.0           0.01                0.0005   \n",
      "\n",
      "   epochs  repeat       mse       mae        r2  \n",
      "0    1000       1  0.167228  0.291325  0.807398  \n",
      "1    1000       1  0.196579  0.342025  0.773593  \n",
      "2    1000       1  0.148805  0.283710  0.828616  \n",
      "3    1000       1  0.175968  0.321114  0.797332  \n",
      "4    1000       2  0.248759  0.385579  0.713495  \n",
      "  hidden_layers  learning_rate optimizer  momentum   avg_mse   avg_mae  \\\n",
      "0      [10, 10]           0.01        gd       0.0  0.186464  0.318838   \n",
      "1      [10, 10]           0.01        gd       0.9  0.188229  0.323385   \n",
      "2      [10, 10]           0.01  momentum       0.0  0.212785  0.344501   \n",
      "3      [10, 10]           0.01  momentum       0.9  0.357115  0.460269   \n",
      "4          [10]           0.01        gd       0.0  0.198370  0.334465   \n",
      "5          [10]           0.01        gd       0.9  0.185862  0.324634   \n",
      "6          [10]           0.01  momentum       0.0  0.177552  0.317023   \n",
      "7          [10]           0.01  momentum       0.9  0.339682  0.409953   \n",
      "\n",
      "     avg_r2  best_mse   best_r2  \n",
      "0  0.785243  0.179039  0.793795  \n",
      "1  0.783210  0.170788  0.803297  \n",
      "2  0.754928  0.155224  0.821223  \n",
      "3  0.588698  0.195623  0.774694  \n",
      "4  0.771530  0.167228  0.807398  \n",
      "5  0.785936  0.151605  0.825391  \n",
      "6  0.795507  0.148805  0.828616  \n",
      "7  0.608776  0.175968  0.797332  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Inicjalizacja wag sieci dla wielu warstw ukrytych\n",
    "# def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "#     weights = []\n",
    "#     layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "#     for i in range(len(layer_sizes) - 1):\n",
    "#         weights.append(2 * np.random.random((layer_sizes[i], layer_sizes[i+1])) - 1)\n",
    "#     return weights\n",
    "\n",
    "def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "    return weights\n",
    "\n",
    "# Podział danych na zbiory treningowe, generalizacyjne i walidacyjne\n",
    "def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "    np.random.shuffle(data) # tasowanie danych\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio) \n",
    "    validation_size = int(len(data) * validation_ratio)\n",
    "\n",
    "    train_data = data[:train_size] # wybiera obserwacje do liczby \"train_size\"\n",
    "    validation_data = data[train_size:train_size + validation_size] # wybiera obserwacje od \"train_size\" do sumy \"train_size\" i \"validation_size\"\n",
    "    test_data = data[train_size + validation_size:] # wybiera obserwacje od powyzszej sumy do końca\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Funkcja dostosowująca tempa nauki\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=1e-6):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.05\n",
    "    else:\n",
    "        learning_rate *= 0.7\n",
    "\n",
    "    if abs(mse - previous_mse) < threshold:\n",
    "        learning_rate *= learning_rate_adjust\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "# Trening sieci z wieloma warstwami ukrytymi\n",
    "def train(X, y, learning_rate, learning_rate_adjust, epochs,\n",
    "          hidden_layers_sizes, optimizer, momentum):\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights = initialize_weights(input_size, hidden_layers_sizes, output_size)\n",
    "    velocities = [np.zeros_like(w) for w in weights]\n",
    "    prev_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i, w in enumerate(weights):\n",
    "            z = np.dot(activations[-1], w)\n",
    "            if i == len(weights) - 1:\n",
    "                activations.append(z)   # brak softmax\n",
    "            else:\n",
    "                activations.append(relu(z))\n",
    "        predicted_output = activations[-1]\n",
    "\n",
    "        # Backpropagation\n",
    "        error = predicted_output - y\n",
    "        loss = np.mean(error ** 2)\n",
    "\n",
    "        learning_rate = adjust_learning_rate(\n",
    "            learning_rate,\n",
    "            loss,\n",
    "            prev_loss,\n",
    "            learning_rate_adjust\n",
    "        )\n",
    "        prev_loss = loss\n",
    "\n",
    "        deltas = [error] \n",
    "        for i in range(len(weights) - 1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "\n",
    "        deltas.reverse() \n",
    "\n",
    "        # Aktualizacja wag\n",
    "        for i in range(len(weights)):\n",
    "            grad = activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "\n",
    "            if optimizer == 'gd':\n",
    "                weights[i] -= learning_rate * grad\n",
    "\n",
    "            elif optimizer == 'momentum':\n",
    "                velocities[i] = momentum * velocities[i] - learning_rate * grad\n",
    "                weights[i] += velocities[i]\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def predict(X, weights):\n",
    "    output = X\n",
    "    for i, w in enumerate(weights):\n",
    "        output = np.dot(output, w)\n",
    "        if i < len(weights) - 1:\n",
    "            output = relu(output)\n",
    "        else:\n",
    "            output = output  # liniowe wyjście\n",
    "    return output\n",
    "\n",
    "def mae_np(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Parametry sieci\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.0005]\n",
    "epochses = [1000]\n",
    "repeat = 3\n",
    "optimizers = ['gd', 'momentum']\n",
    "momentums = [0.0, 0.9]\n",
    "# gd → zwykły gradient prosty\n",
    "# momentum → gradient z momentem\n",
    "\n",
    "# Warstwy\n",
    "hidden_layers_sizes_list = [\n",
    "    [10],         \n",
    "    [10, 10]       \n",
    "]\n",
    "\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "# Przechowywanie wyników dla różnych konfiguracji warstw\n",
    "results = []\n",
    "\n",
    "# Testowanie\n",
    "for hidden_layers_sizes in hidden_layers_sizes_list:  \n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for lr_adj in learning_rate_adjusts:\n",
    "                for epochs in epochses:\n",
    "                    for optimizer in optimizers:\n",
    "                        for momentum in momentums:\n",
    "                            trained_weights = train(\n",
    "                                X_train, y_train, lr, lr_adj, epochs, hidden_layers_sizes, optimizer=optimizer, momentum=momentum\n",
    "                            )\n",
    "                            predictions_train = predict(X_train, trained_weights)\n",
    "                            predictions_validation = predict(X_validation, trained_weights)\n",
    "                            predictions_test = predict(X_test, trained_weights)\n",
    "\n",
    "                            # Odwracanie skalowania y\n",
    "                            y_pred_test_real = scaler_y.inverse_transform(predictions_test)\n",
    "                            y_test_real = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "                            # Obliczamy metryki\n",
    "                            mse, mae, r2 = calculate_regression_metrics(y_test, predictions_test)\n",
    "\n",
    "                            # Dodanie wyników do tabeli\n",
    "                            results.append({\n",
    "                                'hidden_layers': str(hidden_layers_sizes),\n",
    "                                'optimizer': optimizer,\n",
    "                                'momentum': momentum,\n",
    "                                'learning_rate': lr,\n",
    "                                'learning_rate_adjust': lr_adj,\n",
    "                                'epochs': epochs,\n",
    "                                'repeat': r,\n",
    "                                'mse': mse,\n",
    "                                'mae': mae,\n",
    "                                'r2': r2\n",
    "                            })\n",
    "\n",
    "# Tworzenie DataFrame z wynikami\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Wyświetlanie wyników\n",
    "print(results_df)\n",
    "print(results_df.head())\n",
    "\n",
    "summary = results_df.groupby(\n",
    "    ['hidden_layers', 'learning_rate', 'optimizer', 'momentum']\n",
    ").agg(\n",
    "    avg_mse=('mse', 'mean'),\n",
    "    avg_mae=('mae', 'mean'),\n",
    "    avg_r2=('r2', 'mean'),\n",
    "    best_mse=('mse', 'min'),\n",
    "    best_r2=('r2', 'max')\n",
    ").reset_index()\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
