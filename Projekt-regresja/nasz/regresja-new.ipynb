{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98495354",
   "metadata": {},
   "source": [
    "Sieć perceptronowa - nasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b07acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiary: Train: (3669, 5), Val: (786, 5), Test: (787, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Ustawienie ziarna dla powtarzalności\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---- WCZYTYWANIE DANYCH ----\n",
    "data = pd.read_csv('XAU_1d_data.csv', delimiter=\";\")\n",
    "data['Date'] = pd.to_datetime(data['Date']).dt.date\n",
    "\n",
    "# ---- FEATURE ENGINEERING (Zgodnie z wytycznymi dla szeregów czasowych) ----\n",
    "# Przewidujemy 'Close' na jutro na podstawie dzisiejszych danych\n",
    "data['Target'] = data['Close'].shift(-1)\n",
    "data = data.dropna()\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Volume', 'Close']\n",
    "target = 'Target'\n",
    "\n",
    "# Usuwanie outlierów \n",
    "for col in features + [target]:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3*std) & (data[col] <= mean + 3*std)]\n",
    "\n",
    "X_raw = data[features].values\n",
    "y_raw = data[target].values.reshape(-1, 1)\n",
    "\n",
    "# ---- PODZIAŁ CHRONOLOGICZNY (Wymóg: pierwsze x% train, potem y% val, z% test) ----\n",
    "def split_time_series(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    return (X[:train_end], y[:train_end], \n",
    "            X[train_end:val_end], y[train_end:val_end], \n",
    "            X[val_end:], y[val_end:])\n",
    "\n",
    "X_train_raw, y_train_raw, X_val_raw, y_val_raw, X_test_raw, y_test_raw = split_time_series(X_raw, y_raw)\n",
    "\n",
    "# ---- SKALOWANIE ----\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train = scaler_x.fit_transform(X_train_raw)\n",
    "X_val = scaler_x.transform(X_val_raw)\n",
    "X_test = scaler_x.transform(X_test_raw)\n",
    "\n",
    "y_train = scaler_y.fit_transform(y_train_raw)\n",
    "y_val = scaler_y.transform(y_val_raw)\n",
    "y_test = scaler_y.transform(y_test_raw)\n",
    "\n",
    "print(f\"Rozmiary: Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e61364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynam trening MLP (Własna implementacja)...\n"
     ]
    }
   ],
   "source": [
    "# Funkcje aktywacji i pomocnicze \n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "    return weights\n",
    "\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=1e-6):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.05\n",
    "    else:\n",
    "        learning_rate *= 0.7\n",
    "    if abs(mse - previous_mse) < threshold:\n",
    "        learning_rate *= learning_rate_adjust\n",
    "    return learning_rate\n",
    "\n",
    "def train_mlp_custom(X, y, learning_rate, learning_rate_adjust, epochs, hidden_layers_sizes, optimizer, momentum):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights = initialize_weights(input_size, hidden_layers_sizes, output_size)\n",
    "    velocities = [np.zeros_like(w) for w in weights]\n",
    "    prev_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        activations = [X]\n",
    "        for i, w in enumerate(weights):\n",
    "            z = np.dot(activations[-1], w)\n",
    "            activations.append(z if i == len(weights) - 1 else relu(z))\n",
    "        \n",
    "        # Backpropagation\n",
    "        error = activations[-1] - y\n",
    "        loss = np.mean(error ** 2)\n",
    "        learning_rate = adjust_learning_rate(learning_rate, loss, prev_loss, learning_rate_adjust)\n",
    "        prev_loss = loss\n",
    "\n",
    "        deltas = [error] \n",
    "        for i in range(len(weights) - 1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "        deltas.reverse()\n",
    "\n",
    "        # Aktualizacja wag\n",
    "        for i in range(len(weights)):\n",
    "            grad = activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "            if optimizer == 'gd':\n",
    "                weights[i] -= learning_rate * grad\n",
    "            elif optimizer == 'momentum':\n",
    "                velocities[i] = momentum * velocities[i] - learning_rate * grad\n",
    "                weights[i] += velocities[i]\n",
    "    return weights\n",
    "\n",
    "def predict_mlp_custom(X, weights):\n",
    "    output = X\n",
    "    for i, w in enumerate(weights):\n",
    "        output = np.dot(output, w)\n",
    "        if i < len(weights) - 1: output = relu(output)\n",
    "    return output\n",
    "\n",
    "# ---- EKSPERYMENTY MLP (Zgodnie z wymogami: 4 wartości parametrów, 5 powtórzeń) ----\n",
    "results_mlp = []\n",
    "lrs = [0.001, 0.01, 0.05, 0.1] # 4 wartości\n",
    "epochs_options = [500, 1000, 1500, 2000] # 4 wartości\n",
    "hidden_configs = [[10], [10, 10], [20, 10], [32, 16, 8]] # 4 wartości\n",
    "\n",
    "print(\"Rozpoczynam trening MLP (Własna implementacja)...\")\n",
    "for lr in lrs: \n",
    "    for hid in hidden_configs:\n",
    "        for r in range(5): # Wymóg: 5 powtórzeń\n",
    "            w = train_mlp_custom(X_train, y_train, lr, 0.0005, 1000, hid, 'momentum', 0.9)\n",
    "            pred = predict_mlp_custom(X_test, w)\n",
    "            # Metryki na danych odskalowanych\n",
    "            p_real = scaler_y.inverse_transform(pred)\n",
    "            t_real = scaler_y.inverse_transform(y_test)\n",
    "            mse = mean_squared_error(t_real, p_real)\n",
    "            results_mlp.append({'model': 'Custom_MLP', 'lr': lr, 'layers': str(hid), 'run': r, 'mse': mse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93587b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=5):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS = 7 # Analiza ostatniego tygodnia (7 dni)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, TIME_STEPS)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, TIME_STEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0382be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 20:09:22.317457: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Conv1D, Flatten, Input\n",
    "\n",
    "def run_keras_experiment(model_type, param_list, param_name):\n",
    "    results = []\n",
    "    for val in param_list:\n",
    "        for r in range(5):\n",
    "            model = Sequential()\n",
    "            model.add(Input(shape=(TIME_STEPS, X_train.shape[1])))\n",
    "            \n",
    "            if model_type == 'LSTM':\n",
    "                model.add(LSTM(val, activation='relu'))\n",
    "            elif model_type == 'CNN':\n",
    "                model.add(Conv1D(filters=val, kernel_size=3, activation='relu'))\n",
    "                model.add(Flatten())\n",
    "            \n",
    "            model.add(Dense(1))\n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            \n",
    "            model.fit(X_train_seq, y_train_seq, epochs=20, verbose=0, batch_size=32)\n",
    "            \n",
    "            pred = model.predict(X_test_seq, verbose=0)\n",
    "            p_real = scaler_y.inverse_transform(pred)\n",
    "            t_real = scaler_y.inverse_transform(y_test_seq)\n",
    "            \n",
    "            mse = mean_squared_error(t_real, p_real)\n",
    "            r2 = r2_score(t_real, p_real)\n",
    "            results.append({param_name: val, 'run': r, 'mse': mse, 'r2': r2})\n",
    "    return results\n",
    "\n",
    "# Wykonanie badań\n",
    "lstm_results = run_keras_experiment('LSTM', [16, 32, 64, 128], 'units')\n",
    "cnn_results = run_keras_experiment('CNN', [16, 32, 64, 128], 'filters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3e0a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PODSUMOWANIE LSTM:\n",
      "                mse                     r2\n",
      "               mean          min      mean\n",
      "units                                     \n",
      "16      9787.000649  3153.343998  0.899299\n",
      "32     10159.380027  5340.790149  0.895468\n",
      "64     11804.270649  3274.055325  0.878543\n",
      "128     9370.512709  2503.672161  0.903584\n",
      "\n",
      "Plik 'wyniki_regresja_final.xlsx' został wygenerowany.\n"
     ]
    }
   ],
   "source": [
    "# Łączenie wyników w tabele\n",
    "df_mlp = pd.DataFrame(results_mlp)\n",
    "df_lstm = pd.DataFrame(lstm_results)\n",
    "df_cnn = pd.DataFrame(cnn_results)\n",
    "\n",
    "# wyciągania średnich i najlepszych wyników \n",
    "summary_lstm = df_lstm.groupby('units').agg({'mse': ['mean', 'min'], 'r2': 'mean'})\n",
    "print(\"PODSUMOWANIE LSTM:\")\n",
    "print(summary_lstm)\n",
    "\n",
    "# Eksport do Excela dla sprawozdania\n",
    "with pd.ExcelWriter('wyniki_regresja_final.xlsx') as writer:\n",
    "    df_mlp.to_excel(writer, sheet_name='MLP_Wlasny')\n",
    "    df_lstm.to_excel(writer, sheet_name='LSTM')\n",
    "    df_cnn.to_excel(writer, sheet_name='CNN')\n",
    "\n",
    "print(\"\\nPlik 'wyniki_regresja_final.xlsx' został wygenerowany.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
