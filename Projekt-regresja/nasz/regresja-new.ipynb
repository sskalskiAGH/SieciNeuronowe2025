{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98495354",
   "metadata": {},
   "source": [
    "Sieć perceptronowa - nasz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9b07acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rozmiary zbiorów:\n",
      "Train: X=(3145, 4), y=(3145, 1)\n",
      "Validation: X=(1049, 4), y=(1049, 1)\n",
      "Test: X=(1049, 4), y=(1049, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "data = pd.read_csv('XAU_1d_data.csv', delimiter=\";\")\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date']).dt.date\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Volume']\n",
    "target = 'Close'\n",
    "\n",
    "\n",
    "for col in features + [target]:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3*std) & (data[col] <= mean + 3*std)]\n",
    "\n",
    "X = data[features].values\n",
    "y = data[target].values.reshape(-1, 1)\n",
    "\n",
    "combined = np.concatenate((X, y), axis=1)\n",
    "\n",
    "def split_time_series(data, train_ratio=0.6, val_ratio=0.2):\n",
    "    n = len(data)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "\n",
    "    train_data = data[:train_end]\n",
    "    validation_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "train_data, validation_data, test_data = split_time_series(combined)\n",
    "\n",
    "X_train = train_data[:, :len(features)]\n",
    "y_train = train_data[:, len(features):]\n",
    "\n",
    "X_validation = validation_data[:, :len(features)]\n",
    "y_validation = validation_data[:, len(features):]\n",
    "\n",
    "X_test = test_data[:, :len(features)]\n",
    "y_test = test_data[:, len(features):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_validation = scaler_y.transform(y_validation)\n",
    "y_test = scaler_y.transform(y_test)\n",
    "\n",
    "print(\"\\nRozmiary zbiorów:\")\n",
    "print(f\"Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation: X={X_validation.shape}, y={y_validation.shape}\")\n",
    "print(f\"Test: X={X_test.shape}, y={y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4e61364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layers optimizer  momentum  learning_rate  learning_rate_adjust  \\\n",
      "0           [10]        gd       0.0           0.01                0.0005   \n",
      "1           [10]        gd       0.9           0.01                0.0005   \n",
      "2           [10]  momentum       0.0           0.01                0.0005   \n",
      "3           [10]  momentum       0.9           0.01                0.0005   \n",
      "4           [10]        gd       0.0           0.01                0.0005   \n",
      "5           [10]        gd       0.9           0.01                0.0005   \n",
      "6           [10]  momentum       0.0           0.01                0.0005   \n",
      "7           [10]  momentum       0.9           0.01                0.0005   \n",
      "8           [10]        gd       0.0           0.01                0.0005   \n",
      "9           [10]        gd       0.9           0.01                0.0005   \n",
      "10          [10]  momentum       0.0           0.01                0.0005   \n",
      "11          [10]  momentum       0.9           0.01                0.0005   \n",
      "12      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "13      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "14      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "15      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "16      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "17      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "18      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "19      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "20      [10, 10]        gd       0.0           0.01                0.0005   \n",
      "21      [10, 10]        gd       0.9           0.01                0.0005   \n",
      "22      [10, 10]  momentum       0.0           0.01                0.0005   \n",
      "23      [10, 10]  momentum       0.9           0.01                0.0005   \n",
      "\n",
      "    epochs  repeat        mse       mae         r2  \n",
      "0     1000       1   0.006414  0.068206   0.988182  \n",
      "1     1000       1   0.001472  0.028222   0.997288  \n",
      "2     1000       1   0.014041  0.101743   0.974132  \n",
      "3     1000       1  10.907521  3.155351 -19.095133  \n",
      "4     1000       2   0.007632  0.073512   0.985939  \n",
      "5     1000       2   0.005995  0.066712   0.988955  \n",
      "6     1000       2   0.002121  0.037181   0.996092  \n",
      "7     1000       2   0.051279  0.160784   0.905528  \n",
      "8     1000       3   0.002601  0.041387   0.995209  \n",
      "9     1000       3   0.001405  0.032443   0.997412  \n",
      "10    1000       3   0.001637  0.032156   0.996984  \n",
      "11    1000       3   0.086061  0.213964   0.841448  \n",
      "12    1000       1   0.002649  0.040646   0.995119  \n",
      "13    1000       1   0.004911  0.061495   0.990952  \n",
      "14    1000       1   0.004849  0.053186   0.991067  \n",
      "15    1000       1   1.186903  1.032530  -1.186654  \n",
      "16    1000       2   0.014836  0.097659   0.972667  \n",
      "17    1000       2   0.003292  0.048433   0.993935  \n",
      "18    1000       2   0.003795  0.046318   0.993009  \n",
      "19    1000       2   0.165815  0.388747   0.694516  \n",
      "20    1000       3   0.002687  0.039664   0.995050  \n",
      "21    1000       3   0.002115  0.033673   0.996104  \n",
      "22    1000       3   0.010469  0.090480   0.980713  \n",
      "23    1000       3   0.356934  0.485175   0.342414  \n",
      "  hidden_layers optimizer  momentum  learning_rate  learning_rate_adjust  \\\n",
      "0          [10]        gd       0.0           0.01                0.0005   \n",
      "1          [10]        gd       0.9           0.01                0.0005   \n",
      "2          [10]  momentum       0.0           0.01                0.0005   \n",
      "3          [10]  momentum       0.9           0.01                0.0005   \n",
      "4          [10]        gd       0.0           0.01                0.0005   \n",
      "\n",
      "   epochs  repeat        mse       mae         r2  \n",
      "0    1000       1   0.006414  0.068206   0.988182  \n",
      "1    1000       1   0.001472  0.028222   0.997288  \n",
      "2    1000       1   0.014041  0.101743   0.974132  \n",
      "3    1000       1  10.907521  3.155351 -19.095133  \n",
      "4    1000       2   0.007632  0.073512   0.985939  \n",
      "  hidden_layers  learning_rate optimizer  momentum   avg_mse   avg_mae  \\\n",
      "0      [10, 10]           0.01        gd       0.0  0.006724  0.059323   \n",
      "1      [10, 10]           0.01        gd       0.9  0.003439  0.047867   \n",
      "2      [10, 10]           0.01  momentum       0.0  0.006371  0.063328   \n",
      "3      [10, 10]           0.01  momentum       0.9  0.569884  0.635484   \n",
      "4          [10]           0.01        gd       0.0  0.005549  0.061035   \n",
      "5          [10]           0.01        gd       0.9  0.002957  0.042459   \n",
      "6          [10]           0.01  momentum       0.0  0.005933  0.057027   \n",
      "7          [10]           0.01  momentum       0.9  3.681621  1.176700   \n",
      "\n",
      "     avg_r2  best_mse   best_r2  \n",
      "0  0.987612  0.002649  0.995119  \n",
      "1  0.993664  0.002115  0.996104  \n",
      "2  0.988263  0.003795  0.993009  \n",
      "3 -0.049908  0.165815  0.694516  \n",
      "4  0.989777  0.002601  0.995209  \n",
      "5  0.994551  0.001405  0.997412  \n",
      "6  0.989069  0.001637  0.996984  \n",
      "7 -5.782719  0.051279  0.905528  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Inicjalizacja wag sieci dla wielu warstw ukrytych\n",
    "# def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "#     weights = []\n",
    "#     layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "#     for i in range(len(layer_sizes) - 1):\n",
    "#         weights.append(2 * np.random.random((layer_sizes[i], layer_sizes[i+1])) - 1)\n",
    "#     return weights\n",
    "\n",
    "def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "    return weights\n",
    "\n",
    "# Podział danych na zbiory treningowe, generalizacyjne i walidacyjne\n",
    "def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "    np.random.shuffle(data) # tasowanie danych\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio) \n",
    "    validation_size = int(len(data) * validation_ratio)\n",
    "\n",
    "    train_data = data[:train_size] # wybiera obserwacje do liczby \"train_size\"\n",
    "    validation_data = data[train_size:train_size + validation_size] # wybiera obserwacje od \"train_size\" do sumy \"train_size\" i \"validation_size\"\n",
    "    test_data = data[train_size + validation_size:] # wybiera obserwacje od powyzszej sumy do końca\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Funkcja dostosowująca tempa nauki\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=1e-6):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.05\n",
    "    else:\n",
    "        learning_rate *= 0.7\n",
    "\n",
    "    if abs(mse - previous_mse) < threshold:\n",
    "        learning_rate *= learning_rate_adjust\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "# Trening sieci z wieloma warstwami ukrytymi\n",
    "def train(X, y, learning_rate, learning_rate_adjust, epochs,\n",
    "          hidden_layers_sizes, optimizer, momentum):\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights = initialize_weights(input_size, hidden_layers_sizes, output_size)\n",
    "    velocities = [np.zeros_like(w) for w in weights]\n",
    "    prev_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i, w in enumerate(weights):\n",
    "            z = np.dot(activations[-1], w)\n",
    "            if i == len(weights) - 1:\n",
    "                activations.append(z)   # brak softmax\n",
    "            else:\n",
    "                activations.append(relu(z))\n",
    "        predicted_output = activations[-1]\n",
    "\n",
    "        # Backpropagation\n",
    "        error = predicted_output - y\n",
    "        loss = np.mean(error ** 2)\n",
    "\n",
    "        learning_rate = adjust_learning_rate(\n",
    "            learning_rate,\n",
    "            loss,\n",
    "            prev_loss,\n",
    "            learning_rate_adjust\n",
    "        )\n",
    "        prev_loss = loss\n",
    "\n",
    "        deltas = [error] \n",
    "        for i in range(len(weights) - 1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "\n",
    "        deltas.reverse() \n",
    "\n",
    "        # Aktualizacja wag\n",
    "        for i in range(len(weights)):\n",
    "            grad = activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "\n",
    "            if optimizer == 'gd':\n",
    "                weights[i] -= learning_rate * grad\n",
    "\n",
    "            elif optimizer == 'momentum':\n",
    "                velocities[i] = momentum * velocities[i] - learning_rate * grad\n",
    "                weights[i] += velocities[i]\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def predict(X, weights):\n",
    "    output = X\n",
    "    for i, w in enumerate(weights):\n",
    "        output = np.dot(output, w)\n",
    "        if i < len(weights) - 1:\n",
    "            output = relu(output)\n",
    "        else:\n",
    "            output = output  # liniowe wyjście\n",
    "    return output\n",
    "\n",
    "def mae_np(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Parametry sieci\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.0005]\n",
    "epochses = [1000]\n",
    "repeat = 3\n",
    "optimizers = ['gd', 'momentum']\n",
    "momentums = [0.0, 0.9]\n",
    "# gd → zwykły gradient prosty\n",
    "# momentum → gradient z momentem\n",
    "\n",
    "# Warstwy\n",
    "hidden_layers_sizes_list = [\n",
    "    [10],         \n",
    "    [10, 10]       \n",
    "]\n",
    "\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, mae, r2\n",
    "\n",
    "# Przechowywanie wyników dla różnych konfiguracji warstw\n",
    "results = []\n",
    "\n",
    "# Testowanie\n",
    "for hidden_layers_sizes in hidden_layers_sizes_list:  \n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for lr_adj in learning_rate_adjusts:\n",
    "                for epochs in epochses:\n",
    "                    for optimizer in optimizers:\n",
    "                        for momentum in momentums:\n",
    "                            trained_weights = train(\n",
    "                                X_train, y_train, lr, lr_adj, epochs, hidden_layers_sizes, optimizer=optimizer, momentum=momentum\n",
    "                            )\n",
    "                            predictions_train = predict(X_train, trained_weights)\n",
    "                            predictions_validation = predict(X_validation, trained_weights)\n",
    "                            predictions_test = predict(X_test, trained_weights)\n",
    "\n",
    "                            # Odwracanie skalowania y\n",
    "                            y_pred_test_real = scaler_y.inverse_transform(predictions_test)\n",
    "                            y_test_real = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "                            # Obliczamy metryki\n",
    "                            mse, mae, r2 = calculate_regression_metrics(y_test, predictions_test)\n",
    "\n",
    "                            # Dodanie wyników do tabeli\n",
    "                            results.append({\n",
    "                                'hidden_layers': str(hidden_layers_sizes),\n",
    "                                'optimizer': optimizer,\n",
    "                                'momentum': momentum,\n",
    "                                'learning_rate': lr,\n",
    "                                'learning_rate_adjust': lr_adj,\n",
    "                                'epochs': epochs,\n",
    "                                'repeat': r,\n",
    "                                'mse': mse,\n",
    "                                'mae': mae,\n",
    "                                'r2': r2\n",
    "                            })\n",
    "\n",
    "# Tworzenie DataFrame z wynikami\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Wyświetlanie wyników\n",
    "print(results_df)\n",
    "print(results_df.head())\n",
    "\n",
    "summary = results_df.groupby(\n",
    "    ['hidden_layers', 'learning_rate', 'optimizer', 'momentum']\n",
    ").agg(\n",
    "    avg_mse=('mse', 'mean'),\n",
    "    avg_mae=('mae', 'mean'),\n",
    "    avg_r2=('r2', 'mean'),\n",
    "    best_mse=('mse', 'min'),\n",
    "    best_r2=('r2', 'max')\n",
    ").reset_index()\n",
    "\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
