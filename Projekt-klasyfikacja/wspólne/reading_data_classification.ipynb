{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4d9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informacje o wymiarach ---\n",
      "Liczba wierszy: 5456\n",
      "Liczba kolumn: 25\n",
      "\n",
      "--- Analiza kolumny class ---\n",
      "Liczba unikalnych wartości w ostatniej kolumnie: 4\n",
      "Te wartości to: ['Slight-Right-Turn' 'Sharp-Right-Turn' 'Move-Forward' 'Slight-Left-Turn']\n",
      "\n",
      "========================================\n",
      "ROZKŁAD KLAS W CAŁYM ZBIORZE:\n",
      "========================================\n",
      "-- Move-Forward: 2205 samples (40.41%).\n",
      "-- Sharp-Right-Turn: 2097 samples (38.43%).\n",
      "-- Slight-Right-Turn: 826 samples (15.14%).\n",
      "-- Slight-Left-Turn: 328 samples (6.01%).\n",
      "\n",
      "--- Podgląd po zmianach (pierwsze 5 wierszy) ---\n",
      "     US1    US2    US3    US4  US5    US6  US7    US8    US9   US10  ...  \\\n",
      "0  0.438  0.498  3.625  3.645  5.0  2.918  5.0  2.351  2.332  2.643  ...   \n",
      "1  0.438  0.498  3.625  3.648  5.0  2.918  5.0  2.637  2.332  2.649  ...   \n",
      "2  0.438  0.498  3.625  3.629  5.0  2.918  5.0  2.637  2.334  2.643  ...   \n",
      "3  0.437  0.501  3.625  3.626  5.0  2.918  5.0  2.353  2.334  2.642  ...   \n",
      "4  0.438  0.498  3.626  3.629  5.0  2.918  5.0  2.640  2.334  2.639  ...   \n",
      "\n",
      "    US17   US18   US19   US20   US21   US22   US23   US24              Class  \\\n",
      "0  0.502  0.493  0.504  0.445  0.431  0.444  0.440  0.429  Slight-Right-Turn   \n",
      "1  0.502  0.493  0.504  0.449  0.431  0.444  0.443  0.429  Slight-Right-Turn   \n",
      "2  0.502  0.493  0.504  0.449  0.431  0.444  0.446  0.429  Slight-Right-Turn   \n",
      "3  0.502  0.493  0.504  0.449  0.431  0.444  0.444  0.429  Slight-Right-Turn   \n",
      "4  0.502  0.493  0.504  0.449  0.431  0.444  0.441  0.429  Slight-Right-Turn   \n",
      "\n",
      "     Set  \n",
      "0  train  \n",
      "1  train  \n",
      "2   test  \n",
      "3  train  \n",
      "4  train  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "========================================\n",
      "PODSUMOWANIE PODZIAŁU (TRAIN vs TEST):\n",
      "========================================\n",
      "Zbiór 'train': 4364 wierszy (79.99% całości)\n",
      "Zbiór 'test': 1092 wierszy (20.01% całości)\n",
      "\n",
      "--- Weryfikacja proporcji klas w poszczególnych zbiorach (%) ---\n",
      "Set                 test  train\n",
      "Class                          \n",
      "Move-Forward       40.38  40.42\n",
      "Sharp-Right-Turn   38.46  38.43\n",
      "Slight-Left-Turn    6.04   6.00\n",
      "Slight-Right-Turn  15.11  15.15\n",
      "\n",
      "--> Zapisano gotowy plik: sensor_readings_24.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nazwa_pliku = 'sensor_readings_24.data'\n",
    "\n",
    "try:\n",
    "\n",
    "    df = pd.read_csv(nazwa_pliku, sep=',', header=None)\n",
    "    \n",
    "    nowe_nazwy = [f\"US{i}\" for i in range(1, 25)]\n",
    "    nowe_nazwy.append('Class')\n",
    "    df.columns = nowe_nazwy\n",
    "    \n",
    "    print(\"\\n--- Informacje o wymiarach ---\")\n",
    "    wiersze, kolumny = df.shape\n",
    "    print(f\"Liczba wierszy: {wiersze}\")\n",
    "    print(f\"Liczba kolumn: {kolumny}\")\n",
    "    \n",
    "    print(\"\\n--- Analiza kolumny class ---\")\n",
    "    liczba_unikalnych = df.iloc[:, -1].nunique()\n",
    "    print(f\"Liczba unikalnych wartości w ostatniej kolumnie: {liczba_unikalnych}\")\n",
    "    \n",
    "    print(f\"Te wartości to: {df.iloc[:, -1].unique()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ROZKŁAD KLAS W CAŁYM ZBIORZE:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    counts = df['Class'].value_counts()\n",
    "    percents = df['Class'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for klasa in counts.index:\n",
    "        liczba = counts[klasa]\n",
    "        procent = percents[klasa]\n",
    "        print(f\"-- {klasa}: {liczba} samples ({procent:.2f}%).\")\n",
    "\n",
    "    df['Set'] = \"\"\n",
    "    \n",
    "    train_indices, test_indices = train_test_split(\n",
    "        df.index, \n",
    "        test_size=0.2, # dla proporcji 80/20\n",
    "        stratify=df['Class'], # dla zachowania proporcjonalnego rozkładu klas w zbiorach train i test\n",
    "        random_state=42 # inaczej seed\n",
    "    )\n",
    "    \n",
    "    df.loc[train_indices, 'Set'] = 'train'\n",
    "    df.loc[test_indices, 'Set'] = 'test'\n",
    "\n",
    "    print(\"\\n--- Podgląd po zmianach (pierwsze 5 wierszy) ---\")\n",
    "    print(df.head())\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PODSUMOWANIE PODZIAŁU (TRAIN vs TEST):\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    set_counts = df['Set'].value_counts()\n",
    "    set_percents = df['Set'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for nazwa_setu in set_counts.index:\n",
    "        liczba = set_counts[nazwa_setu]\n",
    "        procent = set_percents[nazwa_setu]\n",
    "        print(f\"Zbiór '{nazwa_setu}': {liczba} wierszy ({procent:.2f}% całości)\")\n",
    "\n",
    "    # Dodatkowe sprawdzenie: Czy proporcje klas zostały zachowane wewnątrz zbiorów?\n",
    "    print(\"\\n--- Weryfikacja proporcji klas w poszczególnych zbiorach (%) ---\")\n",
    "    proporcje = pd.crosstab(df['Class'], df['Set'], normalize='columns') * 100\n",
    "    print(proporcje.round(2))\n",
    "    \n",
    "    df.to_excel(\"sensor_readings_24.xlsx\", index=False)\n",
    "    print(\"\\n--> Zapisano gotowy plik: sensor_readings_24.xlsx\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Nie znaleziono pliku. Sprawdź czy nazwa i ścieżka są poprawne.\")\n",
    "except Exception as e:\n",
    "    print(f\"Wystąpił błąd: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ccce8",
   "metadata": {},
   "source": [
    "## Nasze wczytanie danych itp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f98b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Wczytanie danych z gotowym podziałem train/test\n",
    "df = pd.read_excel(\"sensor_readings_24.xlsx\")\n",
    "\n",
    "features = [f\"US{i}\" for i in range(1, 25)]\n",
    "\n",
    "# Podział na zbiory według kolumny 'Set'\n",
    "X_train = df[df['Set']=='train'][features].values\n",
    "X_test = df[df['Set']=='test'][features].values\n",
    "\n",
    "y_train_labels = df[df['Set']=='train']['Class'].values\n",
    "y_test_labels = df[df['Set']=='test']['Class'].values\n",
    "\n",
    "# LabelEncoder dla zamiany nazw klas na liczby\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train_labels)\n",
    "y_test_int = le.transform(y_test_labels)\n",
    "\n",
    "# One-hot encoding dla sieci\n",
    "num_classes = len(le.classes_)\n",
    "y_train = np.eye(num_classes)[y_train_int]\n",
    "y_test = np.eye(num_classes)[y_test_int]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c183ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hidden_layers optimizer  momentum  learning_rate  learning_rate_adjust  \\\n",
      "0           [24]        gd       0.0           0.01                0.0005   \n",
      "1           [24]        gd       0.9           0.01                0.0005   \n",
      "2           [24]  momentum       0.0           0.01                0.0005   \n",
      "3           [24]  momentum       0.9           0.01                0.0005   \n",
      "4           [24]        gd       0.0           0.01                0.0005   \n",
      "5           [24]        gd       0.9           0.01                0.0005   \n",
      "6           [24]  momentum       0.0           0.01                0.0005   \n",
      "7           [24]  momentum       0.9           0.01                0.0005   \n",
      "8           [24]        gd       0.0           0.01                0.0005   \n",
      "9           [24]        gd       0.9           0.01                0.0005   \n",
      "10          [24]  momentum       0.0           0.01                0.0005   \n",
      "11          [24]  momentum       0.9           0.01                0.0005   \n",
      "12      [24, 10]        gd       0.0           0.01                0.0005   \n",
      "13      [24, 10]        gd       0.9           0.01                0.0005   \n",
      "14      [24, 10]  momentum       0.0           0.01                0.0005   \n",
      "15      [24, 10]  momentum       0.9           0.01                0.0005   \n",
      "16      [24, 10]        gd       0.0           0.01                0.0005   \n",
      "17      [24, 10]        gd       0.9           0.01                0.0005   \n",
      "18      [24, 10]  momentum       0.0           0.01                0.0005   \n",
      "19      [24, 10]  momentum       0.9           0.01                0.0005   \n",
      "20      [24, 10]        gd       0.0           0.01                0.0005   \n",
      "21      [24, 10]        gd       0.9           0.01                0.0005   \n",
      "22      [24, 10]  momentum       0.0           0.01                0.0005   \n",
      "23      [24, 10]  momentum       0.9           0.01                0.0005   \n",
      "\n",
      "    epochs  repeat  accuracy  precision    recall        f1  \n",
      "0     1000       1  0.696886   0.671634  0.645725  0.652500  \n",
      "1     1000       1  0.716117   0.709794  0.652592  0.669743  \n",
      "2     1000       1  0.718864   0.707351  0.629708  0.649854  \n",
      "3     1000       1  0.859890   0.857877  0.835689  0.846150  \n",
      "4     1000       2  0.694139   0.677919  0.606798  0.621920  \n",
      "5     1000       2  0.684066   0.666540  0.632996  0.633234  \n",
      "6     1000       2  0.673077   0.639813  0.598371  0.587261  \n",
      "7     1000       2  0.836081   0.838564  0.822941  0.830149  \n",
      "8     1000       3  0.716117   0.696735  0.640809  0.648551  \n",
      "9     1000       3  0.717033   0.710135  0.649029  0.661564  \n",
      "10    1000       3  0.697802   0.680869  0.611781  0.625771  \n",
      "11    1000       3  0.838828   0.832236  0.830690  0.831179  \n",
      "12    1000       1  0.649267   0.605877  0.465486  0.474805  \n",
      "13    1000       1  0.664835   0.675478  0.605718  0.627149  \n",
      "14    1000       1  0.695055   0.677862  0.627788  0.630286  \n",
      "15    1000       1  0.826007   0.829068  0.783253  0.800707  \n",
      "16    1000       2  0.688645   0.663377  0.651381  0.654901  \n",
      "17    1000       2  0.710623   0.675683  0.639814  0.631076  \n",
      "18    1000       2  0.650183   0.597413  0.577541  0.571603  \n",
      "19    1000       2  0.869048   0.861353  0.850673  0.855827  \n",
      "20    1000       3  0.659341   0.586439  0.571570  0.540724  \n",
      "21    1000       3  0.673993   0.675200  0.609604  0.609543  \n",
      "22    1000       3  0.702381   0.658585  0.639757  0.639008  \n",
      "23    1000       3  0.844322   0.835603  0.826951  0.831149  \n",
      "  hidden_layers optimizer  momentum  learning_rate  learning_rate_adjust  \\\n",
      "0          [24]        gd       0.0           0.01                0.0005   \n",
      "1          [24]        gd       0.9           0.01                0.0005   \n",
      "2          [24]  momentum       0.0           0.01                0.0005   \n",
      "3          [24]  momentum       0.9           0.01                0.0005   \n",
      "4          [24]        gd       0.0           0.01                0.0005   \n",
      "\n",
      "   epochs  repeat  accuracy  precision    recall        f1  \n",
      "0    1000       1  0.696886   0.671634  0.645725  0.652500  \n",
      "1    1000       1  0.716117   0.709794  0.652592  0.669743  \n",
      "2    1000       1  0.718864   0.707351  0.629708  0.649854  \n",
      "3    1000       1  0.859890   0.857877  0.835689  0.846150  \n",
      "4    1000       2  0.694139   0.677919  0.606798  0.621920  \n",
      "  hidden_layers  learning_rate optimizer  momentum  avg_accuracy  \\\n",
      "0      [24, 10]           0.01        gd       0.0      0.665751   \n",
      "1      [24, 10]           0.01        gd       0.9      0.683150   \n",
      "2      [24, 10]           0.01  momentum       0.0      0.682540   \n",
      "3      [24, 10]           0.01  momentum       0.9      0.846459   \n",
      "4          [24]           0.01        gd       0.0      0.702381   \n",
      "5          [24]           0.01        gd       0.9      0.705739   \n",
      "6          [24]           0.01  momentum       0.0      0.696581   \n",
      "7          [24]           0.01  momentum       0.9      0.844933   \n",
      "\n",
      "   avg_precision  avg_recall    avg_f1  best_accuracy   best_f1  \n",
      "0       0.618564    0.562813  0.556810       0.688645  0.654901  \n",
      "1       0.675454    0.618379  0.622589       0.710623  0.631076  \n",
      "2       0.644620    0.615029  0.613632       0.702381  0.639008  \n",
      "3       0.842008    0.820292  0.829228       0.869048  0.855827  \n",
      "4       0.682096    0.631110  0.640990       0.716117  0.652500  \n",
      "5       0.695490    0.644872  0.654847       0.717033  0.669743  \n",
      "6       0.676011    0.613287  0.620962       0.718864  0.649854  \n",
      "7       0.842892    0.829773  0.835826       0.859890  0.846150  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Funkcja aktywacji: Sigmoid\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Inicjalizacja wag sieci dla wielu warstw ukrytych\n",
    "# def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "#     weights = []\n",
    "#     layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "#     for i in range(len(layer_sizes) - 1):\n",
    "#         weights.append(2 * np.random.random((layer_sizes[i], layer_sizes[i+1])) - 1)\n",
    "#     return weights\n",
    "\n",
    "def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "    return weights\n",
    "\n",
    "# Podział danych na zbiory treningowe, generalizacyjne i walidacyjne\n",
    "def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "    np.random.shuffle(data) # tasowanie danych\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio) \n",
    "    validation_size = int(len(data) * validation_ratio)\n",
    "\n",
    "    train_data = data[:train_size] # wybiera obserwacje do liczby \"train_size\"\n",
    "    validation_data = data[train_size:train_size + validation_size] # wybiera obserwacje od \"train_size\" do sumy \"train_size\" i \"validation_size\"\n",
    "    test_data = data[train_size + validation_size:] # wybiera obserwacje od powyzszej sumy do końca\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Funkcja dostosowująca tempa nauki\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=0.001):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.1  # jeśli błąd maleje, to zwiększa współczynnik uczenia o 10%\n",
    "    else:\n",
    "        learning_rate *= 0.5  # jeśli błąd nie maleje, to zmniejszamy współczynnik uczenia o 50%\n",
    "\n",
    "    if np.abs(mse - previous_mse) < threshold: #jeśli różnica między błędami jest mniejsza niż dany próg to zmniejszamy learning rate, bo zbliżamy się do optymalnej konfiguracji\n",
    "        learning_rate *= learning_rate_adjust\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "# Trening sieci z wieloma warstwami ukrytymi\n",
    "def train(X, y, learning_rate, learning_rate_adjust, epochs,\n",
    "          hidden_layers_sizes, optimizer, momentum):\n",
    "\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights = initialize_weights(input_size, hidden_layers_sizes, output_size)\n",
    "    velocities = [np.zeros_like(w) for w in weights]\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i, w in enumerate(weights):\n",
    "            z = np.dot(activations[-1], w)\n",
    "            if i == len(weights) - 1:\n",
    "                activations.append(softmax(z))\n",
    "            else:\n",
    "                activations.append(relu(z))\n",
    "        predicted_output = activations[-1]\n",
    "\n",
    "        # Backpropagation\n",
    "        error = predicted_output - y  # cross-entropy gradient\n",
    "        deltas = [error] \n",
    "        for i in range(len(weights) - 1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "\n",
    "        deltas.reverse() \n",
    "\n",
    "        # Aktualizacja wag\n",
    "        # for i in range(len(weights)):\n",
    "        #     weights[i] += activations[i].T.dot(deltas[i]) * learning_rate\n",
    "\n",
    "        # # Dostosowanie learning rate\n",
    "        # learning_rate = adjust_learning_rate(\n",
    "        #     learning_rate,\n",
    "        #     np.mean(error ** 2),\n",
    "        #     np.mean((y - predicted_output) ** 2),\n",
    "        #     learning_rate_adjust\n",
    "        # )\n",
    "\n",
    "        # for i in range(len(weights)):\n",
    "        #     weights[i] -= learning_rate * activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "        for i in range(len(weights)):\n",
    "            grad = activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "\n",
    "            if optimizer == 'gd':\n",
    "                weights[i] -= learning_rate * grad\n",
    "\n",
    "            elif optimizer == 'momentum':\n",
    "                velocities[i] = momentum * velocities[i] - learning_rate * grad\n",
    "                weights[i] += velocities[i]\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def predict(X, weights):\n",
    "    output = X\n",
    "    for i, w in enumerate(weights):\n",
    "        output = np.dot(output, w)\n",
    "        if i < len(weights) - 1:\n",
    "            output = relu(output)\n",
    "        else:\n",
    "            output = softmax(output)\n",
    "    return output\n",
    "\n",
    "# Funkcja obliczająca błąd predykcji\n",
    "def calculate_error(predictions, labels):\n",
    "    return np.mean(np.abs(predictions - labels))\n",
    "\n",
    "def correct(y, predictions):\n",
    "    # y i predictions są one-hot encoded\n",
    "    y_class = np.argmax(y, axis=1)\n",
    "    pred_class = np.argmax(predictions, axis=1)\n",
    "    correct_predictions = np.sum(y_class == pred_class)\n",
    "    return correct_predictions / len(y)\n",
    "\n",
    "# Parametry sieci\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.0005]\n",
    "epochses = [1000]\n",
    "repeat = 3\n",
    "optimizers = ['gd', 'momentum']\n",
    "momentums = [0.0, 0.9]\n",
    "# gd → zwykły gradient prosty\n",
    "# momentum → gradient z momentem\n",
    "\n",
    "# Warstwy\n",
    "hidden_layers_sizes_list = [\n",
    "    [24],         \n",
    "    [24, 10]       \n",
    "]\n",
    "\n",
    "# Funkcja obliczająca wyniki dla accuracy, precision, recall, F1\n",
    "def calculate_metrics(y_test_class, y_pred_class):\n",
    "    accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "    precision = precision_score(y_test_class, y_pred_class, average='macro')\n",
    "    recall = recall_score(y_test_class, y_pred_class, average='macro')\n",
    "    f1 = f1_score(y_test_class, y_pred_class, average='macro')\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Przechowywanie wyników dla różnych konfiguracji warstw\n",
    "results = []\n",
    "\n",
    "# Testowanie\n",
    "for hidden_layers_sizes in hidden_layers_sizes_list:  \n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for lr_adj in learning_rate_adjusts:\n",
    "                for epochs in epochses:\n",
    "                    for optimizer in optimizers:\n",
    "                        for momentum in momentums:\n",
    "                            trained_weights = train(\n",
    "                                X_train, y_train, lr, lr_adj, epochs, hidden_layers_sizes, optimizer=optimizer, momentum=momentum\n",
    "                            )\n",
    "                            predictions_train = predict(X_train, trained_weights)\n",
    "                            # predictions_validation = predict(X_validation, trained_weights)\n",
    "                            predictions_test = predict(X_test, trained_weights)\n",
    "\n",
    "                            # Zaokrąglamy wyniki do wartości 0 lub 1 dla porównań\n",
    "                            # y_pred = (predictions_test > 0.5).astype(int)\n",
    "                            y_pred_class = np.argmax(predictions_test, axis=1)\n",
    "                            y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "                            # Obliczamy metryki\n",
    "                            accuracy, precision, recall, f1 = calculate_metrics(y_test_class, y_pred_class)\n",
    "\n",
    "                            # Dodanie wyników do tabeli\n",
    "                            results.append({\n",
    "                                'hidden_layers': str(hidden_layers_sizes),\n",
    "                                'optimizer': optimizer,\n",
    "                                'momentum': momentum,\n",
    "                                'learning_rate': lr,\n",
    "                                'learning_rate_adjust': lr_adj,\n",
    "                                'epochs': epochs,\n",
    "                                'repeat': r,\n",
    "                                'accuracy': accuracy,\n",
    "                                'precision': precision,\n",
    "                                'recall': recall,\n",
    "                                'f1': f1\n",
    "                            })\n",
    "\n",
    "# Tworzenie DataFrame z wynikami\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Wyświetlanie wyników\n",
    "print(results_df)\n",
    "print(results_df.head())\n",
    "\n",
    "summary = results_df.groupby(\n",
    "    ['hidden_layers', 'learning_rate', 'optimizer', 'momentum']\n",
    ").agg(\n",
    "    avg_accuracy=('accuracy', 'mean'),\n",
    "    avg_precision=('precision', 'mean'),\n",
    "    avg_recall=('recall', 'mean'),\n",
    "    avg_f1=('f1', 'mean'),\n",
    "    best_accuracy=('accuracy', 'max'),\n",
    "    best_f1=('f1', 'max')\n",
    ").reset_index()\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808ce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\julia\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# =======================\n",
    "# Parametry symulacji\n",
    "# =======================\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.5]\n",
    "epochses = [2000]\n",
    "repeat = 3\n",
    "hidden_layers_sizes_list = [[10], [10, 10]]\n",
    "optimizers = ['gd', 'momentum']\n",
    "momentums = [0.0, 0.9]\n",
    "\n",
    "results = []\n",
    "\n",
    "# =======================\n",
    "# Wczytanie danych\n",
    "# =======================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Wczytanie gotowego pliku z podziałem\n",
    "df = pd.read_excel(\"sensor_readings_24.xlsx\")\n",
    "\n",
    "# Cechy\n",
    "features = [f\"US{i}\" for i in range(1, 25)]\n",
    "\n",
    "# Podział na X i y według kolumny 'Set'\n",
    "X_train = df[df['Set']=='train'][features].values\n",
    "y_train = df[df['Set']=='train']['Class'].values\n",
    "\n",
    "X_test = df[df['Set']=='test'][features].values\n",
    "y_test = df[df['Set']=='test']['Class'].values\n",
    "\n",
    "# =======================\n",
    "# LabelEncoder dla etykiet\n",
    "# =======================\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train)\n",
    "y_test_int = le.transform(y_test)\n",
    "\n",
    "# =======================\n",
    "# Tworzymy DataFrame treningowy z X i y razem\n",
    "# =======================\n",
    "train_df = pd.DataFrame(X_train, columns=features)\n",
    "train_df['class'] = y_train_int\n",
    "\n",
    "# =======================\n",
    "# Balansowanie zbioru treningowego\n",
    "# =======================\n",
    "min_count = train_df['class'].value_counts().min()\n",
    "balanced_train_df = pd.concat([\n",
    "    grp.sample(min_count, random_state=42)\n",
    "    for _, grp in train_df.groupby('class')\n",
    "])\n",
    "\n",
    "# Oddzielamy cechy i etykiety po balansowaniu\n",
    "X_train_balanced = balanced_train_df[features].values\n",
    "y_train_balanced = balanced_train_df['class'].values\n",
    "\n",
    "# =======================\n",
    "# Standaryzacja\n",
    "# =======================\n",
    "scaler = StandardScaler()\n",
    "X_train_balanced = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)  # Test nie jest balansowany\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lr_adj in learning_rate_adjusts:\n",
    "        adjusted_lr = lr * lr_adj\n",
    "        for epochs in epochses:\n",
    "            for hidden_layers in hidden_layers_sizes_list:\n",
    "                for r in range(repeat):\n",
    "                    for optimizer in optimizers:\n",
    "                        for momentum in momentums:\n",
    "                            # Dobór parametrów dla MLPClassifier\n",
    "                            if optimizer == 'gd':\n",
    "                                clf_momentum = 0.0\n",
    "                            else:  # 'momentum'\n",
    "                                clf_momentum = momentum\n",
    "                            \n",
    "                            mlp = MLPClassifier(\n",
    "                                hidden_layer_sizes=hidden_layers,\n",
    "                                activation='logistic',\n",
    "                                max_iter=epochs,\n",
    "                                solver='sgd',\n",
    "                                learning_rate_init=adjusted_lr,\n",
    "                                momentum=clf_momentum,\n",
    "                                random_state=r\n",
    "                            )\n",
    "                            mlp.fit(X_train_balanced, y_train_balanced)\n",
    "                            \n",
    "                            y_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "                            # Obliczanie metryk\n",
    "                            accuracy = accuracy_score(y_test_int, y_pred)\n",
    "                            precision = precision_score(y_test_int, y_pred, average='macro', zero_division=0)\n",
    "                            recall = recall_score(y_test_int, y_pred, average='macro', zero_division=0)\n",
    "                            f1 = f1_score(y_test_int, y_pred, average='macro', zero_division=0)\n",
    "                            \n",
    "                            results.append({\n",
    "                                \"hidden_layers\": str(hidden_layers),\n",
    "                                \"optimizer\": optimizer,\n",
    "                                \"momentum\": momentum,\n",
    "                                \"learning_rate\": lr,\n",
    "                                \"learning_rate_adjust\": lr_adj,\n",
    "                                \"adjusted_lr\": adjusted_lr,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"repeat\": r+1,\n",
    "                                \"accuracy\": accuracy,\n",
    "                                \"precision\": precision,\n",
    "                                \"recall\": recall,\n",
    "                                \"f1_score\": f1\n",
    "                            })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# Opcjonalnie zapis\n",
    "# df_results.to_csv(\"mlp_results.csv\", index=False)\n",
    "\n",
    "summary = df_results.groupby(\n",
    "    ['hidden_layers', 'learning_rate', 'optimizer', 'momentum']\n",
    ").agg(\n",
    "    avg_accuracy=('accuracy', 'mean'),\n",
    "    avg_precision=('precision', 'mean'),\n",
    "    avg_recall=('recall', 'mean'),\n",
    "    avg_f1=('f1_score', 'mean'),\n",
    "    best_accuracy=('accuracy', 'max'),\n",
    "    best_f1=('f1_score', 'max')\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nPodsumowanie wyników:\")\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
