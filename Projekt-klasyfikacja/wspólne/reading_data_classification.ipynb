{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4d9944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informacje o wymiarach ---\n",
      "Liczba wierszy: 5456\n",
      "Liczba kolumn: 25\n",
      "\n",
      "--- Analiza kolumny class ---\n",
      "Liczba unikalnych wartości w ostatniej kolumnie: 4\n",
      "Te wartości to: ['Slight-Right-Turn' 'Sharp-Right-Turn' 'Move-Forward' 'Slight-Left-Turn']\n",
      "\n",
      "========================================\n",
      "ROZKŁAD KLAS W CAŁYM ZBIORZE:\n",
      "========================================\n",
      "-- Move-Forward: 2205 samples (40.41%).\n",
      "-- Sharp-Right-Turn: 2097 samples (38.43%).\n",
      "-- Slight-Right-Turn: 826 samples (15.14%).\n",
      "-- Slight-Left-Turn: 328 samples (6.01%).\n",
      "\n",
      "--- Podgląd po zmianach (pierwsze 5 wierszy) ---\n",
      "     US1    US2    US3    US4  US5    US6  US7    US8    US9   US10  ...  \\\n",
      "0  0.438  0.498  3.625  3.645  5.0  2.918  5.0  2.351  2.332  2.643  ...   \n",
      "1  0.438  0.498  3.625  3.648  5.0  2.918  5.0  2.637  2.332  2.649  ...   \n",
      "2  0.438  0.498  3.625  3.629  5.0  2.918  5.0  2.637  2.334  2.643  ...   \n",
      "3  0.437  0.501  3.625  3.626  5.0  2.918  5.0  2.353  2.334  2.642  ...   \n",
      "4  0.438  0.498  3.626  3.629  5.0  2.918  5.0  2.640  2.334  2.639  ...   \n",
      "\n",
      "    US17   US18   US19   US20   US21   US22   US23   US24              Class  \\\n",
      "0  0.502  0.493  0.504  0.445  0.431  0.444  0.440  0.429  Slight-Right-Turn   \n",
      "1  0.502  0.493  0.504  0.449  0.431  0.444  0.443  0.429  Slight-Right-Turn   \n",
      "2  0.502  0.493  0.504  0.449  0.431  0.444  0.446  0.429  Slight-Right-Turn   \n",
      "3  0.502  0.493  0.504  0.449  0.431  0.444  0.444  0.429  Slight-Right-Turn   \n",
      "4  0.502  0.493  0.504  0.449  0.431  0.444  0.441  0.429  Slight-Right-Turn   \n",
      "\n",
      "     Set  \n",
      "0  train  \n",
      "1  train  \n",
      "2   test  \n",
      "3  train  \n",
      "4  train  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "========================================\n",
      "PODSUMOWANIE PODZIAŁU (TRAIN vs TEST):\n",
      "========================================\n",
      "Zbiór 'train': 4364 wierszy (79.99% całości)\n",
      "Zbiór 'test': 1092 wierszy (20.01% całości)\n",
      "\n",
      "--- Weryfikacja proporcji klas w poszczególnych zbiorach (%) ---\n",
      "Set                 test  train\n",
      "Class                          \n",
      "Move-Forward       40.38  40.42\n",
      "Sharp-Right-Turn   38.46  38.43\n",
      "Slight-Left-Turn    6.04   6.00\n",
      "Slight-Right-Turn  15.11  15.15\n",
      "\n",
      "--> Zapisano gotowy plik: sensor_readings_24.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nazwa_pliku = 'sensor_readings_24.data'\n",
    "\n",
    "try:\n",
    "\n",
    "    df = pd.read_csv(nazwa_pliku, sep=',', header=None)\n",
    "    \n",
    "    nowe_nazwy = [f\"US{i}\" for i in range(1, 25)]\n",
    "    nowe_nazwy.append('Class')\n",
    "    df.columns = nowe_nazwy\n",
    "    \n",
    "    print(\"\\n--- Informacje o wymiarach ---\")\n",
    "    wiersze, kolumny = df.shape\n",
    "    print(f\"Liczba wierszy: {wiersze}\")\n",
    "    print(f\"Liczba kolumn: {kolumny}\")\n",
    "    \n",
    "    print(\"\\n--- Analiza kolumny class ---\")\n",
    "    liczba_unikalnych = df.iloc[:, -1].nunique()\n",
    "    print(f\"Liczba unikalnych wartości w ostatniej kolumnie: {liczba_unikalnych}\")\n",
    "    \n",
    "    print(f\"Te wartości to: {df.iloc[:, -1].unique()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ROZKŁAD KLAS W CAŁYM ZBIORZE:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    counts = df['Class'].value_counts()\n",
    "    percents = df['Class'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for klasa in counts.index:\n",
    "        liczba = counts[klasa]\n",
    "        procent = percents[klasa]\n",
    "        print(f\"-- {klasa}: {liczba} samples ({procent:.2f}%).\")\n",
    "\n",
    "    df['Set'] = \"\"\n",
    "    \n",
    "    train_indices, test_indices = train_test_split(\n",
    "        df.index, \n",
    "        test_size=0.2, # dla proporcji 80/20\n",
    "        stratify=df['Class'], # dla zachowania proporcjonalnego rozkładu klas w zbiorach train i test\n",
    "        random_state=42 # inaczej seed\n",
    "    )\n",
    "    \n",
    "    df.loc[train_indices, 'Set'] = 'train'\n",
    "    df.loc[test_indices, 'Set'] = 'test'\n",
    "\n",
    "    print(\"\\n--- Podgląd po zmianach (pierwsze 5 wierszy) ---\")\n",
    "    print(df.head())\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PODSUMOWANIE PODZIAŁU (TRAIN vs TEST):\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    set_counts = df['Set'].value_counts()\n",
    "    set_percents = df['Set'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for nazwa_setu in set_counts.index:\n",
    "        liczba = set_counts[nazwa_setu]\n",
    "        procent = set_percents[nazwa_setu]\n",
    "        print(f\"Zbiór '{nazwa_setu}': {liczba} wierszy ({procent:.2f}% całości)\")\n",
    "\n",
    "    # Dodatkowe sprawdzenie: Czy proporcje klas zostały zachowane wewnątrz zbiorów?\n",
    "    print(\"\\n--- Weryfikacja proporcji klas w poszczególnych zbiorach (%) ---\")\n",
    "    proporcje = pd.crosstab(df['Class'], df['Set'], normalize='columns') * 100\n",
    "    print(proporcje.round(2))\n",
    "    \n",
    "    df.to_excel(\"sensor_readings_24.xlsx\", index=False)\n",
    "    print(\"\\n--> Zapisano gotowy plik: sensor_readings_24.xlsx\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Nie znaleziono pliku. Sprawdź czy nazwa i ścieżka są poprawne.\")\n",
    "except Exception as e:\n",
    "    print(f\"Wystąpił błąd: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ccce8",
   "metadata": {},
   "source": [
    "## Nasze wczytanie danych itp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f98b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Wczytanie danych z gotowym podziałem train/test\n",
    "df = pd.read_excel(\"sensor_readings_24.xlsx\")\n",
    "\n",
    "features = [f\"US{i}\" for i in range(1, 25)]\n",
    "\n",
    "# Podział na zbiory według kolumny 'Set'\n",
    "X_train = df[df['Set']=='train'][features].values\n",
    "X_test = df[df['Set']=='test'][features].values\n",
    "\n",
    "y_train_labels = df[df['Set']=='train']['Class'].values\n",
    "y_test_labels = df[df['Set']=='test']['Class'].values\n",
    "\n",
    "# LabelEncoder dla zamiany nazw klas na liczby\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train_labels)\n",
    "y_test_int = le.transform(y_test_labels)\n",
    "\n",
    "# One-hot encoding dla sieci\n",
    "num_classes = len(le.classes_)\n",
    "y_train = np.eye(num_classes)[y_train_int]\n",
    "y_test = np.eye(num_classes)[y_test_int]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9c183ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hidden_layers  avg_accuracy  avg_precision  avg_recall    avg_f1  \\\n",
      "0          [24]      0.696276       0.677132    0.630186  0.636177   \n",
      "1      [24, 24]      0.707265       0.699820    0.657815  0.671055   \n",
      "\n",
      "   best_accuracy  best_precision  best_recall   best_f1  \n",
      "0       0.704212        0.703571     0.639479  0.653518  \n",
      "1       0.722527        0.728770     0.694316  0.705387  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Funkcja aktywacji: Sigmoid\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Inicjalizacja wag sieci dla wielu warstw ukrytych\n",
    "# def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "#     weights = []\n",
    "#     layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "#     for i in range(len(layer_sizes) - 1):\n",
    "#         weights.append(2 * np.random.random((layer_sizes[i], layer_sizes[i+1])) - 1)\n",
    "#     return weights\n",
    "\n",
    "def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "    return weights\n",
    "\n",
    "# Podział danych na zbiory treningowe, generalizacyjne i walidacyjne\n",
    "def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "    np.random.shuffle(data) # tasowanie danych\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio) \n",
    "    validation_size = int(len(data) * validation_ratio)\n",
    "\n",
    "    train_data = data[:train_size] # wybiera obserwacje do liczby \"train_size\"\n",
    "    validation_data = data[train_size:train_size + validation_size] # wybiera obserwacje od \"train_size\" do sumy \"train_size\" i \"validation_size\"\n",
    "    test_data = data[train_size + validation_size:] # wybiera obserwacje od powyzszej sumy do końca\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Funkcja dostosowująca tempa nauki\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=0.001):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.1  # jeśli błąd maleje, to zwiększa współczynnik uczenia o 10%\n",
    "    else:\n",
    "        learning_rate *= 0.5  # jeśli błąd nie maleje, to zmniejszamy współczynnik uczenia o 50%\n",
    "\n",
    "    if np.abs(mse - previous_mse) < threshold: #jeśli różnica między błędami jest mniejsza niż dany próg to zmniejszamy learning rate, bo zbliżamy się do optymalnej konfiguracji\n",
    "        learning_rate *= learning_rate_adjust\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "# Trening sieci z wieloma warstwami ukrytymi\n",
    "def train(X, y, learning_rate, learning_rate_adjust, epochs, hidden_layers_sizes):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights = initialize_weights(input_size, hidden_layers_sizes, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i, w in enumerate(weights):\n",
    "            z = np.dot(activations[-1], w)\n",
    "            if i == len(weights) - 1:\n",
    "                activations.append(softmax(z))\n",
    "            else:\n",
    "                activations.append(relu(z))\n",
    "        predicted_output = activations[-1]\n",
    "\n",
    "        # Backpropagation\n",
    "        error = predicted_output - y  # cross-entropy gradient\n",
    "        deltas = [error] \n",
    "        for i in range(len(weights) - 1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "\n",
    "        deltas.reverse() \n",
    "\n",
    "        # Aktualizacja wag\n",
    "        # for i in range(len(weights)):\n",
    "        #     weights[i] += activations[i].T.dot(deltas[i]) * learning_rate\n",
    "\n",
    "        # # Dostosowanie learning rate\n",
    "        # learning_rate = adjust_learning_rate(\n",
    "        #     learning_rate,\n",
    "        #     np.mean(error ** 2),\n",
    "        #     np.mean((y - predicted_output) ** 2),\n",
    "        #     learning_rate_adjust\n",
    "        # )\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] -= learning_rate * activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def predict(X, weights):\n",
    "    output = X\n",
    "    for i, w in enumerate(weights):\n",
    "        output = np.dot(output, w)\n",
    "        if i < len(weights) - 1:\n",
    "            output = relu(output)\n",
    "        else:\n",
    "            output = softmax(output)\n",
    "    return output\n",
    "\n",
    "# Funkcja obliczająca błąd predykcji\n",
    "def calculate_error(predictions, labels):\n",
    "    return np.mean(np.abs(predictions - labels))\n",
    "\n",
    "def correct(y, predictions):\n",
    "    # y i predictions są one-hot encoded\n",
    "    y_class = np.argmax(y, axis=1)\n",
    "    pred_class = np.argmax(predictions, axis=1)\n",
    "    correct_predictions = np.sum(y_class == pred_class)\n",
    "    return correct_predictions / len(y)\n",
    "\n",
    "# Parametry sieci\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.5]\n",
    "epochses = [1000]\n",
    "repeat = 3\n",
    "\n",
    "# Warstwy\n",
    "hidden_layers_sizes_list = [\n",
    "    [24],         \n",
    "    [24, 24]       \n",
    "]\n",
    "\n",
    "# Funkcja obliczająca wyniki dla accuracy, precision, recall, F1\n",
    "def calculate_metrics(y_test_class, y_pred_class):\n",
    "    accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "    precision = precision_score(y_test_class, y_pred_class, average='macro')\n",
    "    recall = recall_score(y_test_class, y_pred_class, average='macro')\n",
    "    f1 = f1_score(y_test_class, y_pred_class, average='macro')\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Przechowywanie wyników dla różnych konfiguracji warstw\n",
    "results = []\n",
    "\n",
    "# Testowanie\n",
    "for hidden_layers_sizes in hidden_layers_sizes_list:\n",
    "    # Zbieramy metryki dla tej konkretnej konfiguracji warstw\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    best_accuracy = -np.inf\n",
    "    best_precision = -np.inf\n",
    "    best_recall = -np.inf\n",
    "    best_f1 = -np.inf\n",
    "\n",
    "  \n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for lr_adj in learning_rate_adjusts:\n",
    "                for epochs in epochses:\n",
    "                        trained_weights = train(\n",
    "                            X_train, y_train, lr, lr_adj, epochs, hidden_layers_sizes\n",
    "                        )\n",
    "                        predictions_train = predict(X_train, trained_weights)\n",
    "                        predictions_validation = predict(X_validation, trained_weights)\n",
    "                        predictions_test = predict(X_test, trained_weights)\n",
    "\n",
    "                        # Zaokrąglamy wyniki do wartości 0 lub 1 dla porównań\n",
    "                        # y_pred = (predictions_test > 0.5).astype(int)\n",
    "                        y_pred_class = np.argmax(predictions_test, axis=1)\n",
    "                        y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "                        # Obliczamy metryki\n",
    "                        accuracy, precision, recall, f1 = calculate_metrics(y_test_class, y_pred_class)\n",
    "\n",
    "                        # Dodajemy metryki do list\n",
    "                        accuracies.append(accuracy)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        f1_scores.append(f1)\n",
    "\n",
    "                        # Zbieramy najlepsze wyniki\n",
    "                        best_accuracy = max(best_accuracy, accuracy)\n",
    "                        best_precision = max(best_precision, precision)\n",
    "                        best_recall = max(best_recall, recall)\n",
    "                        best_f1 = max(best_f1, f1)\n",
    "\n",
    "    # Obliczanie średnich wartości dla tej konfiguracji\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "    # Dodanie wyników do tabeli\n",
    "    results.append({\n",
    "        'hidden_layers': hidden_layers_sizes,\n",
    "        'avg_accuracy': avg_accuracy,\n",
    "        'avg_precision': avg_precision,\n",
    "        'avg_recall': avg_recall,\n",
    "        'avg_f1': avg_f1,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_precision': best_precision,\n",
    "        'best_recall': best_recall,\n",
    "        'best_f1': best_f1\n",
    "    })\n",
    "\n",
    "# Tworzenie DataFrame z wynikami\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Wyświetlanie wyników\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f808ce6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   learning_rate  lr_adjust  adjusted_lr  epochs hidden_layers  repeat  \\\n",
      "0           0.01        0.5        0.005    1000          [10]       1   \n",
      "1           0.01        0.5        0.005    1000          [10]       2   \n",
      "2           0.01        0.5        0.005    1000          [10]       3   \n",
      "3           0.01        0.5        0.005    1000      [10, 10]       1   \n",
      "4           0.01        0.5        0.005    1000      [10, 10]       2   \n",
      "5           0.01        0.5        0.005    1000      [10, 10]       3   \n",
      "\n",
      "   accuracy  precision    recall  f1_score  \n",
      "0  0.945055   0.937405  0.938283  0.937776  \n",
      "1  0.906593   0.898534  0.903984  0.901140  \n",
      "2  0.934066   0.915314  0.929151  0.921961  \n",
      "3  0.927656   0.914266  0.920790  0.917019  \n",
      "4  0.941392   0.932171  0.933970  0.933051  \n",
      "5  0.926740   0.923966  0.909235  0.916296  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# =======================\n",
    "# Parametry symulacji\n",
    "# =======================\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.5]\n",
    "epochses = [1000]\n",
    "repeat = 3\n",
    "hidden_layers_sizes_list = [[10], [10, 10]]\n",
    "\n",
    "# =======================\n",
    "# Wczytanie danych\n",
    "# =======================\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Wczytanie gotowego pliku z podziałem\n",
    "df = pd.read_excel(\"sensor_readings_24.xlsx\")\n",
    "\n",
    "# Cechy\n",
    "features = [f\"US{i}\" for i in range(1, 25)]\n",
    "\n",
    "# Podział na X i y według kolumny 'Set'\n",
    "X_train = df[df['Set']=='train'][features].values\n",
    "y_train = df[df['Set']=='train']['Class'].values\n",
    "\n",
    "X_test = df[df['Set']=='test'][features].values\n",
    "y_test = df[df['Set']=='test']['Class'].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lr_adj in learning_rate_adjusts:\n",
    "        adjusted_lr = lr * lr_adj\n",
    "        for epochs in epochses:\n",
    "            for hidden_layers in hidden_layers_sizes_list:\n",
    "                for r in range(repeat):\n",
    "                    mlp = MLPClassifier(\n",
    "                        hidden_layer_sizes=hidden_layers,\n",
    "                        activation='logistic',\n",
    "                        max_iter=epochs,\n",
    "                        learning_rate_init=adjusted_lr,\n",
    "                        random_state=r\n",
    "                    )\n",
    "                    mlp.fit(X_train, y_train)\n",
    "                    \n",
    "                    y_pred = mlp.predict(X_test)\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"lr_adjust\": lr_adj,\n",
    "                        \"adjusted_lr\": adjusted_lr,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"hidden_layers\": hidden_layers,\n",
    "                        \"repeat\": r+1,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1_score\": f1\n",
    "                    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# Opcjonalnie zapis\n",
    "# df_results.to_csv(\"mlp_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
