{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687626d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ustawienie ziarna dla powtarzalności podziałów\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x): \n",
    "    return (x > 0).astype(np.float32)\n",
    "\n",
    "def softmax(x):\n",
    "    # Stabilność numeryczna (odejmowanie max)\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        # Inicjalizacja He\n",
    "        W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float32) * np.sqrt(2 / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i + 1]), dtype=np.float32)\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "def train_custom_mlp(X, y_oh, hidden_layers, lr, epochs, optimizer=\"sgd\", momentum=0.9):\n",
    "    layer_sizes = [X.shape[1]] + hidden_layers + [y_oh.shape[1]]\n",
    "    weights, biases = initialize_parameters(layer_sizes)\n",
    "    \n",
    "    # Inicjalizacja buforów (raz, przed pętlą epok)\n",
    "    vW = [np.zeros_like(w) for w in weights]; vb = [np.zeros_like(b) for b in biases]\n",
    "    mW = [np.zeros_like(w) for w in weights]; mb = [np.zeros_like(b) for b in biases]\n",
    "    sW = [np.zeros_like(w) for w in weights]; sb = [np.zeros_like(b) for b in biases]\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # --- Forward Pass ---\n",
    "        activations = [X]\n",
    "        for i in range(len(weights) - 1):\n",
    "            activations.append(relu(activations[-1] @ weights[i] + biases[i]))\n",
    "        output = softmax(activations[-1] @ weights[-1] + biases[-1])\n",
    "        activations.append(output)\n",
    "\n",
    "        # --- Backward Pass ---\n",
    "        deltas = [output - y_oh]\n",
    "        for i in reversed(range(len(weights) - 1)):\n",
    "            delta = (deltas[0] @ weights[i + 1].T) * relu_derivative(activations[i + 1])\n",
    "            deltas.insert(0, delta)\n",
    "\n",
    "        # --- Update Weights ---\n",
    "        for i in range(len(weights)):\n",
    "            grad_W = (activations[i].T @ deltas[i]) / X.shape[0]\n",
    "            grad_b = np.mean(deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "            if optimizer == \"sgd\":\n",
    "                weights[i] -= lr * grad_W\n",
    "                biases[i] -= lr * grad_b\n",
    "            elif optimizer == \"momentum\":\n",
    "                vW[i] = momentum * vW[i] - lr * grad_W\n",
    "                vb[i] = momentum * vb[i] - lr * grad_b\n",
    "                weights[i] += vW[i]\n",
    "                biases[i] += vb[i]\n",
    "            elif optimizer == \"adam\":\n",
    "                # Uproszczony Adam dla wydajności\n",
    "                mW[i] = 0.9 * mW[i] + (1 - 0.9) * grad_W\n",
    "                mb[i] = 0.9 * mb[i] + (1 - 0.9) * grad_b\n",
    "                sW[i] = 0.999 * sW[i] + (1 - 0.999) * (grad_W**2)\n",
    "                sb[i] = 0.999 * sb[i] + (1 - 0.999) * (grad_b**2)\n",
    "                m_hat = mW[i] / (1 - 0.9**epoch)\n",
    "                mb_hat = mb[i] / (1 - 0.9**epoch)\n",
    "                s_hat = sW[i] / (1 - 0.999**epoch)\n",
    "                sb_hat = sb[i] / (1 - 0.999**epoch)\n",
    "                weights[i] -= lr * m_hat / (np.sqrt(s_hat) + 1e-8)\n",
    "                biases[i] -= lr * mb_hat / (np.sqrt(sb_hat) + 1e-8)\n",
    "                \n",
    "    return weights, biases\n",
    "\n",
    "def predict_custom(X, weights, biases):\n",
    "    A = X\n",
    "    for i in range(len(weights) - 1):\n",
    "        A = relu(A @ weights[i] + biases[i])\n",
    "    return np.argmax(A @ weights[-1] + biases[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_variants(filepath):\n",
    "    df = pd.read_excel(filepath)\n",
    "    features = [f\"US{i}\" for i in range(1, 25)]\n",
    "    X = df[features].values.astype(np.float32)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['Class'])\n",
    "    num_classes = len(le.classes_)\n",
    "    \n",
    "    # WARIANT 1: 80% UCZĄCY, 20% TESTOWY\n",
    "    X_train80, X_test20, y_train80, y_test20 = train_test_split(\n",
    "        X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # WARIANT 2: 70% UCZĄCY, 15% WALIDACYJNY, 15% TESTOWY\n",
    "    X_train70, X_temp, y_train70, y_temp = train_test_split(\n",
    "        X, y, test_size=0.30, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "    X_val15, X_test15, y_val15, y_test15 = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    def process_bundle(xt, yt, xv=None, yv=None, xte=None, yte=None):\n",
    "        xt_s = scaler.fit_transform(xt)\n",
    "        y_oh = np.eye(num_classes, dtype=np.float32)[yt]\n",
    "        return (xt_s, y_oh, yt, \n",
    "                scaler.transform(xv) if xv is not None else None, yv,\n",
    "                scaler.transform(xte) if xte is not None else None, yte)\n",
    "\n",
    "    bundle8020 = process_bundle(X_train80, y_train80, xte=X_test20, yte=y_test20)\n",
    "    bundle7015 = process_bundle(X_train70, y_train70, xv=X_val15, yv=y_val15, xte=X_test15, yte=y_test15)\n",
    "    \n",
    "    return bundle8020, bundle7015, num_classes\n",
    "\n",
    "def get_all_metrics(y_true, y_pred, prefix):\n",
    "    return {\n",
    "        f\"{prefix}_acc\": accuracy_score(y_true, y_pred),\n",
    "        f\"{prefix}_prec\": precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        f\"{prefix}_rec\": recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        f\"{prefix}_f1\": f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_experiment(params):\n",
    "    (split_name, lr, ep, hid, opt, mom, rep_idx, bundle, num_classes) = params\n",
    "    (X_tr, y_tr_oh, y_tr_idx, X_val, y_val_idx, X_te, y_te_idx) = bundle\n",
    "\n",
    "    # 1. TRENING WŁASNEJ SIECI\n",
    "    w, b = train_custom_mlp(X_tr, y_tr_oh, hid, lr, ep, opt, mom if mom else 0.9)\n",
    "    \n",
    "    # Metryki własnej sieci dla wszystkich zbiorów\n",
    "    res = {\"split\": split_name, \"lr\": lr, \"epochs\": ep, \"hidden\": str(hid), \"opt\": opt, \"mom\": mom, \"rep\": rep_idx}\n",
    "    res.update(get_all_metrics(y_tr_idx, predict_custom(X_tr, w, b), \"custom_train\"))\n",
    "    res.update(get_all_metrics(y_te_idx, predict_custom(X_te, w, b), \"custom_test\"))\n",
    "    if X_val is not None:\n",
    "        res.update(get_all_metrics(y_val_idx, predict_custom(X_val, w, b), \"custom_val\"))\n",
    "\n",
    "    # 2. GOTOWA BIBLIOTEKA (SKLEARN)\n",
    "    sk_opt = 'sgd' if opt in ['sgd', 'momentum'] else 'adam'\n",
    "    clf = MLPClassifier(hidden_layer_sizes=tuple(hid), learning_rate_init=lr, \n",
    "                        max_iter=ep, solver=sk_opt, momentum=(mom if mom else 0.9),\n",
    "                        nesterovs_momentum=(opt == 'momentum'), random_state=rep_idx, tol=1e-4)\n",
    "    clf.fit(X_tr, y_tr_idx)\n",
    "    res.update({\"sklearn_test_acc\": accuracy_score(y_te_idx, clf.predict(X_te))})\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc12558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2560 eksperymentów.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/sebastian/Studia/.venv/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStart: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m eksperymentów.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Pool(cpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     results = \u001b[43mpool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_full_experiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_tasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Zapis wszystkich wyników do Excela\u001b[39;00m\n\u001b[32m     26\u001b[39m df_results = pd.DataFrame(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/pool.py:367\u001b[39m, in \u001b[36mPool.map\u001b[39m\u001b[34m(self, func, iterable, chunksize)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    363\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[33;03m    in a list that is returned.\u001b[39;00m\n\u001b[32m    366\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/pool.py:768\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ready():\n\u001b[32m    770\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/pool.py:765\u001b[39m, in \u001b[36mApplyResult.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    b8020, b7015, n_cls = load_data_variants(\"sensor_readings_24.xlsx\")\n",
    "    \n",
    "    # Parametry zgodne z wymogiem (min. 4 wartości)\n",
    "    lrs = [0.001, 0.01, 0.05, 0.1]\n",
    "    epochs = [100, 250, 500, 1000]\n",
    "    configs = [[64, 32], [32, 16], [16, 8], [64, 32, 16]]\n",
    "    opts = [\"sgd\", \"momentum\", \"adam\"]\n",
    "    moms = [0.7, 0.9]\n",
    "    repeats = range(5) \n",
    "\n",
    "    all_tasks = []\n",
    "    for split_name, bundle in [(\"80/20\", b8020), (\"70/15/15\", b7015)]:\n",
    "        for lr, ep, hid, opt in product(lrs, epochs, configs, opts):\n",
    "            current_moms = moms if opt == \"momentum\" else [None]\n",
    "            for mom in current_moms:\n",
    "                for r in repeats:\n",
    "                    all_tasks.append((split_name, lr, ep, hid, opt, mom, r, bundle, n_cls))\n",
    "\n",
    "    print(f\"Start: {len(all_tasks)} eksperymentów.\")\n",
    "    \n",
    "    with Pool(cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(run_full_experiment, all_tasks), total=len(all_tasks)))\n",
    "\n",
    "    # Zapis wszystkich wyników do Excela\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_excel(\"wyniki_koncowe_projekt.xlsx\", index=False)\n",
    "    \n",
    "    # Agregacja do wniosków (średnie i najlepsze wyniki)\n",
    "    summary = df_results.groupby(['split', 'hidden', 'opt', 'lr']).agg({\n",
    "        'custom_test_acc': ['mean', 'max'],\n",
    "        'custom_test_f1': ['mean', 'max'],\n",
    "        'sklearn_test_acc': ['mean', 'max']\n",
    "    }).reset_index()\n",
    "    summary.to_excel(\"podsumowanie_do_raportu.xlsx\")\n",
    "    \n",
    "    print(\"Zakończono! Pliki 'wyniki_koncowe_projekt.xlsx' i 'podsumowanie_do_raportu.xlsx' są gotowe.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
