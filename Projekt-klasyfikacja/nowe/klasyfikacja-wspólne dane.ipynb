{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# ===============================\n",
    "# FUNKCJE AKTYWACJI\n",
    "# ===============================\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# ===============================\n",
    "# INICJALIZACJA WAG + BIASÓW\n",
    "# ===============================\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i + 1]))\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "# ===============================\n",
    "# FORWARD / BACKWARD\n",
    "# ===============================\n",
    "def forward(X, weights, biases):\n",
    "    activations = [X]\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(activations[-1] @ W + b)\n",
    "        activations.append(A)\n",
    "    A_out = softmax(activations[-1] @ weights[-1] + biases[-1])\n",
    "    activations.append(A_out)\n",
    "    return activations\n",
    "\n",
    "def backward(activations, weights, y):\n",
    "    deltas = [activations[-1] - y]\n",
    "    for i in reversed(range(len(weights) - 1)):\n",
    "        delta = deltas[0] @ weights[i + 1].T\n",
    "        delta *= relu(activations[i + 1], derivative=True)\n",
    "        deltas.insert(0, delta)\n",
    "\n",
    "    grads_W, grads_b = [], []\n",
    "    for i in range(len(weights)):\n",
    "        grads_W.append(activations[i].T @ deltas[i] / len(y))\n",
    "        grads_b.append(np.mean(deltas[i], axis=0, keepdims=True))\n",
    "    return grads_W, grads_b\n",
    "\n",
    "# ===============================\n",
    "# TRENING\n",
    "# ===============================\n",
    "def train(X, y, hidden_layers, lr, epochs,\n",
    "          optimizer=\"sgd\", momentum=0.9, beta1=0.9, beta2=0.999):\n",
    "\n",
    "    layer_sizes = [X.shape[1]] + hidden_layers + [y.shape[1]]\n",
    "    weights, biases = initialize_parameters(layer_sizes)\n",
    "\n",
    "    vW = [np.zeros_like(w) for w in weights]\n",
    "    vb = [np.zeros_like(b) for b in biases]\n",
    "    mW = [np.zeros_like(w) for w in weights]\n",
    "    mb = [np.zeros_like(b) for b in biases]\n",
    "    sW = [np.zeros_like(w) for w in weights]\n",
    "    sb = [np.zeros_like(b) for b in biases]\n",
    "    eps = 1e-8\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        activations = forward(X, weights, biases)\n",
    "        grads_W, grads_b = backward(activations, weights, y)\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            if optimizer == \"sgd\":\n",
    "                weights[i] -= lr * grads_W[i]\n",
    "                biases[i] -= lr * grads_b[i]\n",
    "\n",
    "            elif optimizer == \"momentum\":\n",
    "                vW[i] = momentum * vW[i] - lr * grads_W[i]\n",
    "                vb[i] = momentum * vb[i] - lr * grads_b[i]\n",
    "                weights[i] += vW[i]\n",
    "                biases[i] += vb[i]\n",
    "\n",
    "            elif optimizer == \"adam\":\n",
    "                t += 1  # zwiększamy licznik iteracji\n",
    "\n",
    "                # aktualizacja momentów\n",
    "                mW[i] = beta1 * mW[i] + (1 - beta1) * grads_W[i]\n",
    "                mb[i] = beta1 * mb[i] + (1 - beta1) * grads_b[i]\n",
    "                sW[i] = beta2 * sW[i] + (1 - beta2) * (grads_W[i] ** 2)\n",
    "                sb[i] = beta2 * sb[i] + (1 - beta2) * (grads_b[i] ** 2)\n",
    "\n",
    "                # korekcja biasu\n",
    "                mW_hat = mW[i] / (1 - beta1 ** t)\n",
    "                mb_hat = mb[i] / (1 - beta1 ** t)\n",
    "                sW_hat = sW[i] / (1 - beta2 ** t)\n",
    "                sb_hat = sb[i] / (1 - beta2 ** t)\n",
    "\n",
    "                # update wag i biasów\n",
    "                weights[i] -= lr * mW_hat / (np.sqrt(sW_hat) + eps)\n",
    "                biases[i] -= lr * mb_hat / (np.sqrt(sb_hat) + eps)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# ===============================\n",
    "# PREDYKCJA\n",
    "# ===============================\n",
    "def predict(X, weights, biases):\n",
    "    A = X\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(A @ W + b)\n",
    "    A = softmax(A @ weights[-1] + biases[-1])\n",
    "    return np.argmax(A, axis=1)\n",
    "\n",
    "# ===============================\n",
    "# JEDNA KOMBINACJA\n",
    "# ===============================\n",
    "def run_combination(params):\n",
    "    lr, epochs, hidden, opt, mom, repeat, X_train, y_train_oh, y_train_lbl, X_test, y_test_lbl = params\n",
    "\n",
    "    acc_train, acc_test, prec, rec, f1 = [], [], [], [], []\n",
    "\n",
    "    for _ in range(repeat):\n",
    "        weights, biases = train(\n",
    "            X_train, y_train_oh,\n",
    "            hidden_layers=hidden,\n",
    "            lr=lr,\n",
    "            epochs=epochs,\n",
    "            optimizer=opt,\n",
    "            momentum=mom if mom else 0.0\n",
    "        )\n",
    "\n",
    "        pred_train = predict(X_train, weights, biases)\n",
    "        pred_test = predict(X_test, weights, biases)\n",
    "\n",
    "        acc_train.append(accuracy_score(y_train_lbl, pred_train))\n",
    "        acc_test.append(accuracy_score(y_test_lbl, pred_test))\n",
    "        prec.append(precision_score(y_test_lbl, pred_test, average=\"macro\", zero_division=0))\n",
    "        rec.append(recall_score(y_test_lbl, pred_test, average=\"macro\", zero_division=0))\n",
    "        f1.append(f1_score(y_test_lbl, pred_test, average=\"macro\", zero_division=0))\n",
    "\n",
    "    return {\n",
    "        \"hidden_layers\": str(hidden),\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"optimizer\": opt,\n",
    "        \"momentum\": mom,\n",
    "        \"acc_train_mean\": np.mean(acc_train),\n",
    "        \"acc_train_best\": np.max(acc_train),\n",
    "        \"acc_test_mean\": np.mean(acc_test),\n",
    "        \"acc_test_best\": np.max(acc_test),\n",
    "        \"precision_mean\": np.mean(prec),\n",
    "        \"recall_mean\": np.mean(rec),\n",
    "        \"f1_mean\": np.mean(f1)\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# MAIN\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ---- DANE ----\n",
    "    df = pd.read_excel(\"sensor_readings_24_outcome.xlsx\")\n",
    "    X = df[[f\"US{i}\" for i in range(1, 25)]].values\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "    y = df[\"Class\"].values\n",
    "    classes = np.unique(y)\n",
    "    y_idx = np.array([np.where(classes == c)[0][0] for c in y])\n",
    "    y_oh = np.eye(len(classes))[y_idx]\n",
    "\n",
    "    train_mask = df[\"Set\"] == \"train\"\n",
    "    test_mask = df[\"Set\"] == \"test\"\n",
    "\n",
    "    X_train, X_test = X[train_mask], X[test_mask]\n",
    "    y_train_oh = y_oh[train_mask]\n",
    "    y_train_lbl = y_idx[train_mask]\n",
    "    y_test_lbl = y_idx[test_mask]\n",
    "\n",
    "    # ---- HIPERPARAMETRY ----\n",
    "    learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "    epochs_list = [1500, 1200, 900, 500]\n",
    "    hidden_layer_configs = [\n",
    "        [128, 64, 32, 16], [64, 32, 16, 8], [32, 16, 8, 4],\n",
    "        [64, 32, 16], [32, 16, 8], [16, 8, 4],\n",
    "        [64, 32], [32, 16], [16, 8], [8, 4],\n",
    "        [64], [32], [16], [8]\n",
    "    ]\n",
    "    optimizers = [\"sgd\", \"momentum\", \"adam\"]\n",
    "    momentum_values = [0.6, 0.7, 0.8, 0.9]\n",
    "    repeat = 4\n",
    "\n",
    "    all_combinations = []\n",
    "    for lr, ep, hid, opt in product(learning_rates, epochs_list, hidden_layer_configs, optimizers):\n",
    "        moms = [None] if opt != \"momentum\" else momentum_values\n",
    "        for m in moms:\n",
    "            all_combinations.append(\n",
    "                (lr, ep, hid, opt, m, repeat,\n",
    "                 X_train, y_train_oh, y_train_lbl,\n",
    "                 X_test, y_test_lbl)\n",
    "            )\n",
    "\n",
    "    total = len(all_combinations)\n",
    "    print(f\"Liczba kombinacji: {total}\")\n",
    "\n",
    "    # ---- PLIK WYNIKÓW ----\n",
    "    output_file = \"porownanie_optymalizatorow_multiproc.xlsx\"\n",
    "    columns = [\n",
    "        \"hidden_layers\", \"learning_rate\", \"epochs\", \"optimizer\", \"momentum\",\n",
    "        \"acc_train_mean\", \"acc_train_best\",\n",
    "        \"acc_test_mean\", \"acc_test_best\",\n",
    "        \"precision_mean\", \"recall_mean\", \"f1_mean\"\n",
    "    ]\n",
    "    pd.DataFrame(columns=columns).to_excel(output_file, index=False)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # ---- MULTIPROCESSING + ZAPIS NA BIEŻĄCO ----\n",
    "    with Pool(cpu_count()) as pool:\n",
    "        for i, res in enumerate(pool.imap_unordered(run_combination, all_combinations), 1):\n",
    "\n",
    "            wb = load_workbook(output_file)\n",
    "            ws = wb.active\n",
    "            ws.append(list(res.values()))\n",
    "            wb.save(output_file)\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "            avg = elapsed / i\n",
    "            remaining = avg * (total - i)\n",
    "\n",
    "            h, rem = divmod(remaining, 3600)\n",
    "            m, s = divmod(rem, 60)\n",
    "\n",
    "            print(\n",
    "                f\"[{i}/{total}] {res['hidden_layers']} | {res['optimizer']} | \"\n",
    "                f\"lr={res['learning_rate']} | ep={res['epochs']} | mom={res['momentum']} | \"\n",
    "                f\"TEST_mean={res['acc_test_mean']:.3f} | \"\n",
    "                f\"ETA {int(h)}h {int(m)}m {int(s)}s\"\n",
    "            )\n",
    "\n",
    "    print(\"\\nZapisano:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3d56bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pliki Excel zostały wygenerowane:\n",
      "1. wyniki_mlp_szczegolowe.xlsx\n",
      "2. wyniki_mlp_zagregowane.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# ===============================\n",
    "# FUNKCJE AKTYWACJI\n",
    "# ===============================\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# ===============================\n",
    "# INICJALIZACJA WAG + BIASÓW\n",
    "# ===============================\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i + 1]))\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "# ===============================\n",
    "# FORWARD / BACKWARD\n",
    "# ===============================\n",
    "def forward(X, weights, biases):\n",
    "    activations = [X]\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(activations[-1] @ W + b)\n",
    "        activations.append(A)\n",
    "    A_out = softmax(activations[-1] @ weights[-1] + biases[-1])\n",
    "    activations.append(A_out)\n",
    "    return activations\n",
    "\n",
    "def backward(activations, weights, y):\n",
    "    deltas = [activations[-1] - y]\n",
    "    for i in reversed(range(len(weights) - 1)):\n",
    "        delta = deltas[0] @ weights[i + 1].T\n",
    "        delta *= relu(activations[i + 1], derivative=True)\n",
    "        deltas.insert(0, delta)\n",
    "\n",
    "    grads_W, grads_b = [], []\n",
    "    for i in range(len(weights)):\n",
    "        grads_W.append(activations[i].T @ deltas[i] / len(y))\n",
    "        grads_b.append(np.mean(deltas[i], axis=0, keepdims=True))\n",
    "    return grads_W, grads_b\n",
    "\n",
    "# ===============================\n",
    "# TRENING\n",
    "# ===============================\n",
    "def train(X, y, hidden_layers, lr, epochs,\n",
    "          optimizer=\"sgd\", momentum=0.9, beta1=0.9, beta2=0.999):\n",
    "    t = 0\n",
    "    layer_sizes = [X.shape[1]] + hidden_layers + [y.shape[1]]\n",
    "    weights, biases = initialize_parameters(layer_sizes)\n",
    "\n",
    "    vW = [np.zeros_like(w) for w in weights]\n",
    "    vb = [np.zeros_like(b) for b in biases]\n",
    "    mW = [np.zeros_like(w) for w in weights]\n",
    "    mb = [np.zeros_like(b) for b in biases]\n",
    "    sW = [np.zeros_like(w) for w in weights]\n",
    "    sb = [np.zeros_like(b) for b in biases]\n",
    "    eps = 1e-8\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t += 1  # licznik epok\n",
    "        activations = forward(X, weights, biases)\n",
    "        grads_W, grads_b = backward(activations, weights, y)\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            if optimizer == \"sgd\":\n",
    "                weights[i] -= lr * grads_W[i]\n",
    "                biases[i] -= lr * grads_b[i]\n",
    "\n",
    "            elif optimizer == \"momentum\":\n",
    "                vW[i] = momentum * vW[i] - lr * grads_W[i]\n",
    "                vb[i] = momentum * vb[i] - lr * grads_b[i]\n",
    "                weights[i] += vW[i]\n",
    "                biases[i] += vb[i]\n",
    "\n",
    "            elif optimizer == \"adam\":\n",
    "                t += 1  # zwiększamy licznik iteracji\n",
    "\n",
    "                # aktualizacja momentów\n",
    "                mW[i] = beta1 * mW[i] + (1 - beta1) * grads_W[i]\n",
    "                mb[i] = beta1 * mb[i] + (1 - beta1) * grads_b[i]\n",
    "                sW[i] = beta2 * sW[i] + (1 - beta2) * (grads_W[i] ** 2)\n",
    "                sb[i] = beta2 * sb[i] + (1 - beta2) * (grads_b[i] ** 2)\n",
    "\n",
    "                # korekcja biasu\n",
    "                mW_hat = mW[i] / (1 - beta1 ** t)\n",
    "                mb_hat = mb[i] / (1 - beta1 ** t)\n",
    "                sW_hat = sW[i] / (1 - beta2 ** t)\n",
    "                sb_hat = sb[i] / (1 - beta2 ** t)\n",
    "\n",
    "                # update wag i biasów\n",
    "                weights[i] -= lr * mW_hat / (np.sqrt(sW_hat) + eps)\n",
    "                biases[i] -= lr * mb_hat / (np.sqrt(sb_hat) + eps)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# ===============================\n",
    "# PREDYKCJA\n",
    "# ===============================\n",
    "def predict(X, weights, biases):\n",
    "    A = X\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(A @ W + b)\n",
    "    A = softmax(A @ weights[-1] + biases[-1])\n",
    "    return np.argmax(A, axis=1)\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# ---- DANE ----\n",
    "# df = pd.read_excel(\"sensor_readings_24_outcome.xlsx\")\n",
    "# X = df[[f\"US{i}\" for i in range(1, 25)]].values\n",
    "# X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# y = df[\"Class\"].values\n",
    "# classes = np.unique(y)\n",
    "# y_idx = np.array([np.where(classes == c)[0][0] for c in y])\n",
    "# y_oh = np.eye(len(classes))[y_idx]\n",
    "\n",
    "# train_mask = df[\"Set\"] == \"train\"\n",
    "# test_mask = df[\"Set\"] == \"test\"\n",
    "\n",
    "# X_train, X_test = X[train_mask], X[test_mask]\n",
    "# y_train_oh = y_oh[train_mask]\n",
    "# y_train_lbl = y_idx[train_mask]\n",
    "# y_test_lbl = y_idx[test_mask]\n",
    "\n",
    "df = pd.read_excel(\"sensor_readings_24.xlsx\")\n",
    "features = [f\"US{i}\" for i in range(1, 25)]\n",
    "\n",
    "X_train = df[df['Set']=='train'][features].values\n",
    "X_test = df[df['Set']=='test'][features].values\n",
    "\n",
    "y_train_labels = df[df['Set']=='train']['Class'].values\n",
    "y_test_labels = df[df['Set']=='test']['Class'].values\n",
    "\n",
    "# LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train_labels)\n",
    "y_test_int = le.transform(y_test_labels)\n",
    "\n",
    "# One-hot encoding\n",
    "num_classes = len(le.classes_)\n",
    "y_train = np.eye(num_classes)[y_train_int]\n",
    "y_test = np.eye(num_classes)[y_test_int]\n",
    "\n",
    "# Normalizacja\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train_lbl = y_train_int\n",
    "y_test_lbl = y_test_int\n",
    "\n",
    "# ---- HIPERPARAMETRY ----\n",
    "# learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "# epochs_list = [1500, 1200, 900, 500]\n",
    "# hidden_layer_configs = [\n",
    "#         [128, 64, 32, 16], [64, 32, 16, 8], [32, 16, 8, 4],\n",
    "#         [64, 32, 16], [32, 16, 8], [16, 8, 4],\n",
    "#         [64, 32], [32, 16], [16, 8], [8, 4],\n",
    "#         [64], [32], [16], [8]\n",
    "# ]\n",
    "# optimizers = [\"sgd\", \"momentum\", \"adam\"]\n",
    "# momentum_values = [0.6, 0.7, 0.8, 0.9]\n",
    "# repeat = 4\n",
    "\n",
    "learning_rates = [0.1]\n",
    "epochs_list = [500]\n",
    "hidden_layer_configs = [\n",
    "        [64], [32]\n",
    "]\n",
    "optimizers = [\"sgd\", \"momentum\"]\n",
    "momentum_values = [0.9]\n",
    "repeat = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "for hidden_layers_sizes in hidden_layer_configs:  \n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for epochs in epochs_list:\n",
    "                for optimizer in optimizers:\n",
    "                    moms = momentum_values if optimizer == \"momentum\" else [None]\n",
    "                    for mom in moms:\n",
    "\n",
    "                        trained_weights, trained_biases = train(\n",
    "                            X_train, y_train_oh,\n",
    "                            hidden_layers=hidden_layers_sizes,\n",
    "                            lr=lr,\n",
    "                            epochs=epochs,\n",
    "                            optimizer=optimizer,\n",
    "                            momentum=mom or 0.0\n",
    "                        )\n",
    "\n",
    "                        # predykcje\n",
    "                        pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "                        pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "\n",
    "                        # metryki\n",
    "                        acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "                        acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "                        # zapis wyników\n",
    "                        results.append({\n",
    "                            'hidden_layers': str(hidden_layers_sizes),\n",
    "                            'optimizer': optimizer,\n",
    "                            'momentum': mom,\n",
    "                            'learning_rate': lr,\n",
    "                            'epochs': epochs,\n",
    "                            'repeat': r,\n",
    "                            'acc_train': acc_train,\n",
    "                            'precision_train': prec_train,\n",
    "                            'recall_train': rec_train,\n",
    "                            'f1_train': f1_train,\n",
    "                            'acc_test': acc_test,\n",
    "                            'precision_test': prec_test,\n",
    "                            'recall_test': rec_test,\n",
    "                            'f1_test': f1_test\n",
    "                        })\n",
    "\n",
    "# ===============================\n",
    "# Zapis do Excela\n",
    "# ===============================\n",
    "df_mlp = pd.DataFrame(results)\n",
    "df_mlp.to_excel('wyniki_mlp_szczegolowe.xlsx', sheet_name='MLP_szczegolowy', index=False)\n",
    "\n",
    "# Zagregowane wyniki (średnia po powtórzeniach)\n",
    "summary_mlp = df_mlp.groupby(['hidden_layers', 'optimizer', 'momentum', 'learning_rate', 'epochs']) \\\n",
    "    .agg({\n",
    "        'acc_train': ['mean', 'min', 'max'],\n",
    "        'precision_train': ['mean', 'min', 'max'],\n",
    "        'recall_train': ['mean', 'min', 'max'],\n",
    "        'f1_train': ['mean', 'min', 'max'],\n",
    "        'acc_test': ['mean', 'min', 'max'],\n",
    "        'precision_test': ['mean', 'min', 'max'],\n",
    "        'recall_test': ['mean', 'min', 'max'],\n",
    "        'f1_test': ['mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "\n",
    "# Spłaszczamy MultiIndex w kolumnach\n",
    "summary_mlp.columns = ['_'.join(col).strip('_') for col in summary_mlp.columns.values]\n",
    "\n",
    "# Teraz zapisujemy do Excela\n",
    "summary_mlp.to_excel('wyniki_mlp_zagregowane.xlsx', sheet_name='MLP_zagregowany', index=False)\n",
    "\n",
    "print(\"Pliki Excel zostały wygenerowane:\")\n",
    "print(\"1. wyniki_mlp_szczegolowe.xlsx\")\n",
    "print(\"2. wyniki_mlp_zagregowane.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
