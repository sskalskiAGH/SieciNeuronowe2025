{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8eaaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('star_classification.csv', delimiter=\",\")\n",
    "\n",
    "# Wybrane cechy\n",
    "features = ['alpha','delta','u','g','r','i','z','redshift']\n",
    "\n",
    "# Usuwanie wartości odstających\n",
    "for col in features:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3*std) & (data[col] <= mean + 3*std)]\n",
    "\n",
    "# Kodowanie klas\n",
    "le = LabelEncoder()\n",
    "data['class_encoded'] = le.fit_transform(data['class'])\n",
    "\n",
    "# Tworzymy X i y (one-hot)\n",
    "X = data[features].values\n",
    "y_encoded = data['class_encoded'].values\n",
    "y = np.eye(3)[y_encoded]  # one-hot\n",
    "\n",
    "# Łączymy X i y do wspólnej macierzy\n",
    "combined = np.concatenate((X, y), axis=1)\n",
    "\n",
    "def split_data(data, train_ratio=0.8):\n",
    "    np.random.shuffle(data)\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = split_data(combined)\n",
    "\n",
    "# Balansowanie tylko zbioru treningowego\n",
    "train_df = pd.DataFrame(train_data, columns=features + ['c0','c1','c2'])\n",
    "train_df['class'] = np.argmax(train_df[['c0','c1','c2']].values, axis=1)\n",
    "min_count = train_df['class'].value_counts().min()\n",
    "balanced_train_df = pd.concat([\n",
    "    df.sample(min_count, random_state=42)\n",
    "    for _, df in train_df.groupby('class')\n",
    "])\n",
    "train_data = balanced_train_df.drop(columns='class').values\n",
    "\n",
    "# Oddzielamy cechy i klasy\n",
    "X_train = train_data[:, :len(features)]\n",
    "y_train = train_data[:, len(features):]\n",
    "X_test = test_data[:, :len(features)]\n",
    "y_test = test_data[:, len(features):]\n",
    "\n",
    "# Skalowanie cech\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# etykiety numeryczne do metryk\n",
    "y_train_lbl = np.argmax(y_train, axis=1)\n",
    "y_test_lbl = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===============================\n",
    "# FUNKCJE AKTYWACJI\n",
    "# ===============================\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# ===============================\n",
    "# INICJALIZACJA WAG + BIASÓW\n",
    "# ===============================\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i + 1]))\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "# ===============================\n",
    "# FORWARD / BACKWARD\n",
    "# ===============================\n",
    "def forward(X, weights, biases):\n",
    "    activations = [X]\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(activations[-1] @ W + b)\n",
    "        activations.append(A)\n",
    "    A_out = softmax(activations[-1] @ weights[-1] + biases[-1])\n",
    "    activations.append(A_out)\n",
    "    return activations\n",
    "\n",
    "def backward(activations, weights, y):\n",
    "    deltas = [activations[-1] - y]\n",
    "    for i in reversed(range(len(weights) - 1)):\n",
    "        delta = deltas[0] @ weights[i + 1].T\n",
    "        delta *= relu(activations[i + 1], derivative=True)\n",
    "        deltas.insert(0, delta)\n",
    "\n",
    "    grads_W, grads_b = [], []\n",
    "    for i in range(len(weights)):\n",
    "        grads_W.append(activations[i].T @ deltas[i] / len(y))\n",
    "        grads_b.append(np.mean(deltas[i], axis=0, keepdims=True))\n",
    "    return grads_W, grads_b\n",
    "\n",
    "# ===============================\n",
    "# TRENING\n",
    "# ===============================\n",
    "def train(X, y, hidden_layers, lr, epochs,\n",
    "          optimizer=\"sgd\", momentum=0.9, beta1=0.9, beta2=0.999):\n",
    "    t = 0\n",
    "    layer_sizes = [X.shape[1]] + hidden_layers + [y.shape[1]]\n",
    "    weights, biases = initialize_parameters(layer_sizes)\n",
    "\n",
    "    vW = [np.zeros_like(w) for w in weights]\n",
    "    vb = [np.zeros_like(b) for b in biases]\n",
    "    mW = [np.zeros_like(w) for w in weights]\n",
    "    mb = [np.zeros_like(b) for b in biases]\n",
    "    sW = [np.zeros_like(w) for w in weights]\n",
    "    sb = [np.zeros_like(b) for b in biases]\n",
    "    eps = 1e-8\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        activations = forward(X, weights, biases)\n",
    "        grads_W, grads_b = backward(activations, weights, y)\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            if optimizer == \"sgd\":\n",
    "                weights[i] -= lr * grads_W[i]\n",
    "                biases[i] -= lr * grads_b[i]\n",
    "\n",
    "            elif optimizer == \"momentum\":\n",
    "                vW[i] = momentum * vW[i] - lr * grads_W[i]\n",
    "                vb[i] = momentum * vb[i] - lr * grads_b[i]\n",
    "                weights[i] += vW[i]\n",
    "                biases[i] += vb[i]\n",
    "\n",
    "            elif optimizer == \"adam\":\n",
    "                t += 1  # zwiększamy licznik iteracji\n",
    "\n",
    "                # aktualizacja momentów\n",
    "                mW[i] = beta1 * mW[i] + (1 - beta1) * grads_W[i]\n",
    "                mb[i] = beta1 * mb[i] + (1 - beta1) * grads_b[i]\n",
    "                sW[i] = beta2 * sW[i] + (1 - beta2) * (grads_W[i] ** 2)\n",
    "                sb[i] = beta2 * sb[i] + (1 - beta2) * (grads_b[i] ** 2)\n",
    "\n",
    "                # korekcja biasu\n",
    "                mW_hat = mW[i] / (1 - beta1 ** t)\n",
    "                mb_hat = mb[i] / (1 - beta1 ** t)\n",
    "                sW_hat = sW[i] / (1 - beta2 ** t)\n",
    "                sb_hat = sb[i] / (1 - beta2 ** t)\n",
    "\n",
    "                # update wag i biasów\n",
    "                weights[i] -= lr * mW_hat / (np.sqrt(sW_hat) + eps)\n",
    "                biases[i] -= lr * mb_hat / (np.sqrt(sb_hat) + eps)\n",
    "\n",
    "    return weights, biases\n",
    "\n",
    "# ===============================\n",
    "# PREDYKCJA\n",
    "# ===============================\n",
    "def predict(X, weights, biases):\n",
    "    A = X\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(A @ W + b)\n",
    "    A = softmax(A @ weights[-1] + biases[-1])\n",
    "    return np.argmax(A, axis=1)\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# ---- HIPERPARAMETRY ----\n",
    "# learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "# epochs_list = [1500, 1200, 900, 500]\n",
    "# hidden_layer_configs = [\n",
    "#         [128, 64, 32, 16], [64, 32, 16, 8], [32, 16, 8, 4],\n",
    "#         [64, 32, 16], [32, 16, 8], [16, 8, 4],\n",
    "#         [64, 32], [32, 16], [16, 8], [8, 4],\n",
    "#         [64], [32], [16], [8]\n",
    "# ]\n",
    "# optimizers = [\"sgd\", \"momentum\", \"adam\"]\n",
    "# momentum_values = [0.6, 0.7, 0.8, 0.9]\n",
    "# repeat = 4\n",
    "\n",
    "learning_rates = [0.1]\n",
    "epochs_list = [500]\n",
    "hidden_layer_configs = [\n",
    "        [64], [32]\n",
    "]\n",
    "optimizers = [\"sgd\", \"momentum\"]\n",
    "momentum_values = [0.9]\n",
    "repeat = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "for hidden_layers_sizes in hidden_layer_configs:  \n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for epochs in epochs_list:\n",
    "                for optimizer in optimizers:\n",
    "                    moms = momentum_values if optimizer == \"momentum\" else [None]\n",
    "                    for mom in moms:\n",
    "\n",
    "                        trained_weights, trained_biases = train(\n",
    "                            X_train, y_train,\n",
    "                            hidden_layers=hidden_layers_sizes,\n",
    "                            lr=lr,\n",
    "                            epochs=epochs,\n",
    "                            optimizer=optimizer,\n",
    "                            momentum=mom or 0.0\n",
    "                        )\n",
    "\n",
    "                        # predykcje\n",
    "                        pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "                        pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "\n",
    "                        # metryki\n",
    "                        acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "                        acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "                        # zapis wyników\n",
    "                        results.append({\n",
    "                            'hidden_layers': str(hidden_layers_sizes),\n",
    "                            'optimizer': optimizer,\n",
    "                            'momentum': mom,\n",
    "                            'learning_rate': lr,\n",
    "                            'epochs': epochs,\n",
    "                            'repeat': r,\n",
    "                            'acc_train': acc_train,\n",
    "                            'precision_train': prec_train,\n",
    "                            'recall_train': rec_train,\n",
    "                            'f1_train': f1_train,\n",
    "                            'acc_test': acc_test,\n",
    "                            'precision_test': prec_test,\n",
    "                            'recall_test': rec_test,\n",
    "                            'f1_test': f1_test\n",
    "                        })\n",
    "\n",
    "# ===============================\n",
    "# Zapis do Excela\n",
    "# ===============================\n",
    "df_mlp = pd.DataFrame(results)\n",
    "df_mlp.to_excel('wyniki_mlp_szczegolowe.xlsx', sheet_name='MLP_szczegolowy', index=False)\n",
    "\n",
    "# Zagregowane wyniki (średnia po powtórzeniach)\n",
    "summary_mlp = df_mlp.groupby(['hidden_layers', 'optimizer', 'momentum', 'learning_rate', 'epochs']) \\\n",
    "    .agg({\n",
    "        'acc_train': ['mean', 'min', 'max'],\n",
    "        'precision_train': ['mean', 'min', 'max'],\n",
    "        'recall_train': ['mean', 'min', 'max'],\n",
    "        'f1_train': ['mean', 'min', 'max'],\n",
    "        'acc_test': ['mean', 'min', 'max'],\n",
    "        'precision_test': ['mean', 'min', 'max'],\n",
    "        'recall_test': ['mean', 'min', 'max'],\n",
    "        'f1_test': ['mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "\n",
    "# Spłaszczamy MultiIndex w kolumnach\n",
    "summary_mlp.columns = ['_'.join(col).strip('_') for col in summary_mlp.columns.values]\n",
    "\n",
    "# Teraz zapisujemy do Excela\n",
    "summary_mlp.to_excel('wyniki_mlp_zagregowane.xlsx', sheet_name='MLP_zagregowany', index=False)\n",
    "\n",
    "print(\"Pliki Excel zostały wygenerowane:\")\n",
    "print(\"1. wyniki_mlp_szczegolowe.xlsx\")\n",
    "print(\"2. wyniki_mlp_zagregowane.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d378799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pliki Excel zostały wygenerowane:\n",
      "1. wyniki_mlp_szczegolowe.xlsx\n",
      "2. wyniki_mlp_zagregowane.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Hiperparametry zgodne z Twoim pierwszym kodem\n",
    "# =======================\n",
    "# learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "# epochs_list = [1500, 1200, 900, 500]\n",
    "# hidden_layer_configs = [\n",
    "#     [128, 64, 32, 16], [64, 32, 16, 8], [32, 16, 8, 4],\n",
    "#     [64, 32, 16], [32, 16, 8], [16, 8, 4],\n",
    "#     [64, 32], [32, 16], [16, 8], [8, 4],\n",
    "#     [64], [32], [16], [8]\n",
    "# ]\n",
    "# optimizers = ['sgd', 'momentum']\n",
    "# momentum_values = [0.6, 0.7, 0.8, 0.9]\n",
    "# repeat = 4\n",
    "\n",
    "learning_rates = [0.1]\n",
    "epochs_list = [500]\n",
    "hidden_layer_configs = [\n",
    "        [64], [32]\n",
    "]\n",
    "optimizers = [\"sgd\", \"momentum\"]\n",
    "momentum_values = [0.9]\n",
    "repeat = 2\n",
    "\n",
    "results = []\n",
    "\n",
    "# =======================\n",
    "# Testowanie wszystkich kombinacji\n",
    "# =======================\n",
    "for lr, epochs, hidden, opt in product(learning_rates, epochs_list, hidden_layer_configs, optimizers):\n",
    "    moms = [0.0] if opt == 'sgd' else momentum_values\n",
    "    for mom in moms:\n",
    "        for r in range(1, repeat+1):\n",
    "            mlp = MLPClassifier(\n",
    "                hidden_layer_sizes=tuple(hidden),\n",
    "                activation='relu',\n",
    "                solver='sgd',\n",
    "                learning_rate_init=lr,\n",
    "                momentum=mom,\n",
    "                max_iter=epochs,\n",
    "                random_state=r,\n",
    "            )\n",
    "            mlp.fit(X_train, y_train_lbl)\n",
    "\n",
    "            # Predykcje\n",
    "            y_pred_train = mlp.predict(X_train)\n",
    "            y_pred_test = mlp.predict(X_test)\n",
    "\n",
    "            # Metryki\n",
    "            acc_train = accuracy_score(y_train_lbl, y_pred_train)\n",
    "            acc_test = accuracy_score(y_test_lbl, y_pred_test)\n",
    "            precision_train = precision_score(y_train_lbl, y_pred_train, average='macro', zero_division=0)\n",
    "            recall_train = recall_score(y_train_lbl, y_pred_train, average='macro', zero_division=0)\n",
    "            f1_train = f1_score(y_train_lbl, y_pred_train, average='macro', zero_division=0)\n",
    "\n",
    "            precision_test = precision_score(y_test_lbl, y_pred_test, average='macro', zero_division=0)\n",
    "            recall_test = recall_score(y_test_lbl, y_pred_test, average='macro', zero_division=0)\n",
    "            f1_test = f1_score(y_test_lbl, y_pred_test, average='macro', zero_division=0)\n",
    "\n",
    "            # Zapis wyników szczegółowych\n",
    "            results.append({\n",
    "                \"hidden_layers\": str(hidden),\n",
    "                \"optimizer\": opt,\n",
    "                \"momentum\": mom,\n",
    "                \"learning_rate\": lr,\n",
    "                \"epochs\": epochs,\n",
    "                \"repeat\": r,\n",
    "                \"acc_train\": acc_train,\n",
    "                \"precision_train\": precision_train,\n",
    "                \"recall_train\": recall_train,\n",
    "                \"f1_train\": f1_train,\n",
    "                \"acc_test\": acc_test,\n",
    "                \"precision_test\": precision_test,\n",
    "                \"recall_test\": recall_test,\n",
    "                \"f1_test\": f1_test\n",
    "            })\n",
    "\n",
    "# =======================\n",
    "# Zapis szczegółowych wyników do Excela\n",
    "# =======================\n",
    "df_mlp = pd.DataFrame(results)\n",
    "df_mlp.to_excel('wyniki_mlp_szczegolowe.xlsx', sheet_name='MLP_szczegolowy', index=False)\n",
    "\n",
    "# =======================\n",
    "# Zagregowane wyniki (mean/min/max)\n",
    "# =======================\n",
    "summary_mlp = df_mlp.groupby(['hidden_layers', 'optimizer', 'momentum', 'learning_rate', 'epochs']) \\\n",
    "    .agg({\n",
    "        'acc_train': ['mean', 'min', 'max'],\n",
    "        'precision_train': ['mean', 'min', 'max'],\n",
    "        'recall_train': ['mean', 'min', 'max'],\n",
    "        'f1_train': ['mean', 'min', 'max'],\n",
    "        'acc_test': ['mean', 'min', 'max'],\n",
    "        'precision_test': ['mean', 'min', 'max'],\n",
    "        'recall_test': ['mean', 'min', 'max'],\n",
    "        'f1_test': ['mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "\n",
    "# Spłaszczamy MultiIndex w kolumnach\n",
    "summary_mlp.columns = ['_'.join(col).strip('_') for col in summary_mlp.columns.values]\n",
    "\n",
    "# Zapis do Excela\n",
    "summary_mlp.to_excel('wyniki_mlp_zagregowane.xlsx', sheet_name='MLP_zagregowany', index=False)\n",
    "\n",
    "print(\"Pliki Excel zostały wygenerowane:\")\n",
    "print(\"1. wyniki_mlp_szczegolowe.xlsx\")\n",
    "print(\"2. wyniki_mlp_zagregowane.xlsx\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
