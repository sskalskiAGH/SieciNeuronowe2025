{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1156712a",
   "metadata": {},
   "source": [
    "## Klasyfikacja\n",
    "Sieć perceptronowa, wielowarstwowa własna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd31306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiary zbiorów:\n",
      "Train: (30789, 8), Validation: (10263, 8), Test: (10263, 8)\n",
      "\n",
      "Liczebność klas w zbiorze treningowym:\n",
      "{0: 10330, 1: 10253, 2: 10206}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Wczytanie danych\n",
    "data = pd.read_csv('star_classification.csv', delimiter=\",\")\n",
    "\n",
    "# Wybrane cechy\n",
    "features = ['alpha','delta','u','g','r','i','z','redshift']\n",
    "\n",
    "for col in features:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3*std) & (data[col] <= mean + 3*std)]\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['class_encoded'] = le.fit_transform(data['class'])  # GALAXY=0, STAR=1, QSO=2\n",
    "\n",
    "min_count = data['class_encoded'].value_counts().min()\n",
    "balanced_data = pd.concat([\n",
    "    df.sample(min_count, random_state=42)\n",
    "    for _, df in data.groupby('class_encoded')\n",
    "])\n",
    "\n",
    "# Funkcja split_data działa na macierzy numpy, więc tworzymy macierz danych\n",
    "X = balanced_data[features].values\n",
    "y_encoded = balanced_data['class_encoded'].values\n",
    "y = np.eye(3)[y_encoded]  # one-hot\n",
    "\n",
    "# Łączymy X i y, żeby podział był zsynchronizowany\n",
    "combined = np.concatenate((X, y), axis=1)\n",
    "\n",
    "# def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "#     np.random.shuffle(data)\n",
    "#     train_size = int(len(data) * train_ratio)\n",
    "#     validation_size = int(len(data) * validation_ratio)\n",
    "#     train_data = data[:train_size]\n",
    "#     validation_data = data[train_size:train_size + validation_size]\n",
    "#     test_data = data[train_size + validation_size:]\n",
    "#     return train_data, validation_data, test_data\n",
    "\n",
    "# Podział danych na zbiory treningowe, generalizacyjne i walidacyjne\n",
    "def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "    np.random.shuffle(data) # tasowanie danych\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio) \n",
    "    validation_size = int(len(data) * validation_ratio)\n",
    "\n",
    "    train_data = data[:train_size] # wybiera obserwacje do liczby \"train_size\"\n",
    "    validation_data = data[train_size:train_size + validation_size] # wybiera obserwacje od \"train_size\" do sumy \"train_size\" i \"validation_size\"\n",
    "    test_data = data[train_size + validation_size:] # wybiera obserwacje od powyzszej sumy do końca\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "train_data, validation_data, test_data = split_data(combined)\n",
    "\n",
    "# Oddzielamy cechy i klasy\n",
    "X_train = train_data[:, :len(features)]\n",
    "y_train = train_data[:, len(features):]\n",
    "X_validation = validation_data[:, :len(features)]\n",
    "y_validation = validation_data[:, len(features):]\n",
    "X_test = test_data[:, :len(features)]\n",
    "y_test = test_data[:, len(features):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# --- PODSUMOWANIE ---\n",
    "print(\"Rozmiary zbiorów:\")\n",
    "print(f\"Train: {X_train.shape}, Validation: {X_validation.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Liczebność klas w train\n",
    "train_classes = np.argmax(y_train, axis=1)\n",
    "unique, counts = np.unique(train_classes, return_counts=True)\n",
    "print(\"\\nLiczebność klas w zbiorze treningowym:\")\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7dfa95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hidden_layers  avg_accuracy  avg_precision  avg_recall    avg_f1  \\\n",
      "0          [10]      0.796876       0.805232    0.794130  0.790145   \n",
      "1      [10, 10]      0.807821       0.817255    0.806535  0.805625   \n",
      "\n",
      "   best_accuracy  best_precision  best_recall   best_f1  \n",
      "0       0.814674        0.826860     0.812373  0.811486  \n",
      "1       0.823931        0.832897     0.822727  0.823841  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Funkcja aktywacji: Sigmoid\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Inicjalizacja wag sieci dla wielu warstw ukrytych\n",
    "# def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "#     weights = []\n",
    "#     layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "#     for i in range(len(layer_sizes) - 1):\n",
    "#         weights.append(2 * np.random.random((layer_sizes[i], layer_sizes[i+1])) - 1)\n",
    "#     return weights\n",
    "\n",
    "def initialize_weights(input_size, hidden_layers_sizes, output_size):\n",
    "    weights = []\n",
    "    layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2 / layer_sizes[i]))\n",
    "    return weights\n",
    "\n",
    "# Podział danych na zbiory treningowe, generalizacyjne i walidacyjne\n",
    "def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "    np.random.shuffle(data) # tasowanie danych\n",
    "    \n",
    "    train_size = int(len(data) * train_ratio) \n",
    "    validation_size = int(len(data) * validation_ratio)\n",
    "\n",
    "    train_data = data[:train_size] # wybiera obserwacje do liczby \"train_size\"\n",
    "    validation_data = data[train_size:train_size + validation_size] # wybiera obserwacje od \"train_size\" do sumy \"train_size\" i \"validation_size\"\n",
    "    test_data = data[train_size + validation_size:] # wybiera obserwacje od powyzszej sumy do końca\n",
    "\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "# Funkcja dostosowująca tempa nauki\n",
    "def adjust_learning_rate(learning_rate, mse, previous_mse, learning_rate_adjust, threshold=0.001):\n",
    "    if mse < previous_mse:\n",
    "        learning_rate *= 1.1  # jeśli błąd maleje, to zwiększa współczynnik uczenia o 10%\n",
    "    else:\n",
    "        learning_rate *= 0.5  # jeśli błąd nie maleje, to zmniejszamy współczynnik uczenia o 50%\n",
    "\n",
    "    if np.abs(mse - previous_mse) < threshold: #jeśli różnica między błędami jest mniejsza niż dany próg to zmniejszamy learning rate, bo zbliżamy się do optymalnej konfiguracji\n",
    "        learning_rate *= learning_rate_adjust\n",
    "\n",
    "    return learning_rate\n",
    "\n",
    "# Trening sieci z wieloma warstwami ukrytymi\n",
    "def train(X, y, learning_rate, learning_rate_adjust, epochs, hidden_layers_sizes):\n",
    "    input_size = X.shape[1]\n",
    "    output_size = y.shape[1]\n",
    "    weights = initialize_weights(input_size, hidden_layers_sizes, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        activations = [X]\n",
    "        zs = []\n",
    "        for i, w in enumerate(weights):\n",
    "            z = np.dot(activations[-1], w)\n",
    "            if i == len(weights) - 1:\n",
    "                activations.append(softmax(z))\n",
    "            else:\n",
    "                activations.append(relu(z))\n",
    "        predicted_output = activations[-1]\n",
    "\n",
    "        # Backpropagation\n",
    "        error = predicted_output - y  # cross-entropy gradient\n",
    "        deltas = [error] \n",
    "        for i in range(len(weights) - 1, 0, -1):\n",
    "            delta = deltas[-1].dot(weights[i].T) * relu(activations[i], derivative=True)\n",
    "            deltas.append(delta)\n",
    "\n",
    "        deltas.reverse() \n",
    "\n",
    "        # Aktualizacja wag\n",
    "        # for i in range(len(weights)):\n",
    "        #     weights[i] += activations[i].T.dot(deltas[i]) * learning_rate\n",
    "\n",
    "        # # Dostosowanie learning rate\n",
    "        # learning_rate = adjust_learning_rate(\n",
    "        #     learning_rate,\n",
    "        #     np.mean(error ** 2),\n",
    "        #     np.mean((y - predicted_output) ** 2),\n",
    "        #     learning_rate_adjust\n",
    "        # )\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] -= learning_rate * activations[i].T.dot(deltas[i]) / X.shape[0]\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "def predict(X, weights):\n",
    "    output = X\n",
    "    for i, w in enumerate(weights):\n",
    "        output = np.dot(output, w)\n",
    "        if i < len(weights) - 1:\n",
    "            output = relu(output)\n",
    "        else:\n",
    "            output = softmax(output)\n",
    "    return output\n",
    "\n",
    "# Funkcja obliczająca błąd predykcji\n",
    "def calculate_error(predictions, labels):\n",
    "    return np.mean(np.abs(predictions - labels))\n",
    "\n",
    "def correct(y, predictions):\n",
    "    # y i predictions są one-hot encoded\n",
    "    y_class = np.argmax(y, axis=1)\n",
    "    pred_class = np.argmax(predictions, axis=1)\n",
    "    correct_predictions = np.sum(y_class == pred_class)\n",
    "    return correct_predictions / len(y)\n",
    "\n",
    "# Parametry sieci\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.0005]\n",
    "epochses = [1000]\n",
    "repeat = 3\n",
    "\n",
    "# Warstwy\n",
    "hidden_layers_sizes_list = [\n",
    "    [10],         \n",
    "    [10, 10]       \n",
    "]\n",
    "\n",
    "# Funkcja obliczająca wyniki dla accuracy, precision, recall, F1\n",
    "def calculate_metrics(y_test_class, y_pred_class):\n",
    "    accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "    precision = precision_score(y_test_class, y_pred_class, average='macro')\n",
    "    recall = recall_score(y_test_class, y_pred_class, average='macro')\n",
    "    f1 = f1_score(y_test_class, y_pred_class, average='macro')\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Przechowywanie wyników dla różnych konfiguracji warstw\n",
    "results = []\n",
    "\n",
    "# Testowanie\n",
    "for hidden_layers_sizes in hidden_layers_sizes_list:\n",
    "    # Zbieramy metryki dla tej konkretnej konfiguracji warstw\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    best_accuracy = -np.inf\n",
    "    best_precision = -np.inf\n",
    "    best_recall = -np.inf\n",
    "    best_f1 = -np.inf\n",
    "\n",
    "  \n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for lr_adj in learning_rate_adjusts:\n",
    "                for epochs in epochses:\n",
    "                        trained_weights = train(\n",
    "                            X_train, y_train, lr, lr_adj, epochs, hidden_layers_sizes\n",
    "                        )\n",
    "                        predictions_train = predict(X_train, trained_weights)\n",
    "                        predictions_validation = predict(X_validation, trained_weights)\n",
    "                        predictions_test = predict(X_test, trained_weights)\n",
    "\n",
    "                        # Zaokrąglamy wyniki do wartości 0 lub 1 dla porównań\n",
    "                        # y_pred = (predictions_test > 0.5).astype(int)\n",
    "                        y_pred_class = np.argmax(predictions_test, axis=1)\n",
    "                        y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "                        # Obliczamy metryki\n",
    "                        accuracy, precision, recall, f1 = calculate_metrics(y_test_class, y_pred_class)\n",
    "\n",
    "                        # Dodajemy metryki do list\n",
    "                        accuracies.append(accuracy)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        f1_scores.append(f1)\n",
    "\n",
    "                        # Zbieramy najlepsze wyniki\n",
    "                        best_accuracy = max(best_accuracy, accuracy)\n",
    "                        best_precision = max(best_precision, precision)\n",
    "                        best_recall = max(best_recall, recall)\n",
    "                        best_f1 = max(best_f1, f1)\n",
    "\n",
    "    # Obliczanie średnich wartości dla tej konfiguracji\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "    # Dodanie wyników do tabeli\n",
    "    results.append({\n",
    "        'hidden_layers': hidden_layers_sizes,\n",
    "        'avg_accuracy': avg_accuracy,\n",
    "        'avg_precision': avg_precision,\n",
    "        'avg_recall': avg_recall,\n",
    "        'avg_f1': avg_f1,\n",
    "        'best_accuracy': best_accuracy,\n",
    "        'best_precision': best_precision,\n",
    "        'best_recall': best_recall,\n",
    "        'best_f1': best_f1\n",
    "    })\n",
    "\n",
    "# Tworzenie DataFrame z wynikami\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Wyświetlanie wyników\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacca4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hidden_layers  accuracy  precision_macro  recall_macro  f1_macro  \\\n",
      "0      [10, 10]    0.8268           0.8305        0.8257    0.8270   \n",
      "1          [10]    0.7993           0.8033        0.7977    0.7983   \n",
      "\n",
      "   precision_class_0  recall_class_0  f1_class_0  precision_class_1  \\\n",
      "0             0.7589          0.7710      0.7644             0.9616   \n",
      "1             0.7381          0.6928      0.7144             0.9405   \n",
      "\n",
      "   recall_class_1  f1_class_1  precision_class_2  recall_class_2  f1_class_2  \n",
      "0          0.8884      0.9235             0.7709          0.8178      0.7932  \n",
      "1          0.8642      0.9007             0.7314          0.8362      0.7799  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "\n",
    "# Funkcja do obliczania metryk globalnych i per klasę\n",
    "def calculate_classwise_metrics(y_test_class, y_pred_class):\n",
    "    accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "    precision_macro = precision_score(y_test_class, y_pred_class, average='macro')\n",
    "    recall_macro = recall_score(y_test_class, y_pred_class, average='macro')\n",
    "    f1_macro = f1_score(y_test_class, y_pred_class, average='macro')\n",
    "\n",
    "    precision_per_class = precision_score(y_test_class, y_pred_class, average=None)\n",
    "    recall_per_class = recall_score(y_test_class, y_pred_class, average=None)\n",
    "    f1_per_class = f1_score(y_test_class, y_pred_class, average=None)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'f1_per_class': f1_per_class\n",
    "    }\n",
    "\n",
    "# Zakładamy, że masz N klas\n",
    "n_classes = y_train.shape[1]  # liczba klas w danych\n",
    "\n",
    "results = []\n",
    "\n",
    "for hidden_layers_sizes in hidden_layers_sizes_list:\n",
    "    for r in range(1, repeat + 1):\n",
    "        for lr in learning_rates:\n",
    "            for lr_adj in learning_rate_adjusts:\n",
    "                for epochs in epochses:\n",
    "                    trained_weights = train(X_train, y_train, lr, lr_adj, epochs, hidden_layers_sizes)\n",
    "                    predictions_test = predict(X_test, trained_weights)\n",
    "                    y_pred_class = np.argmax(predictions_test, axis=1)\n",
    "                    y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "                    metrics = calculate_classwise_metrics(y_test_class, y_pred_class)\n",
    "\n",
    "                    # Tworzymy słownik do DataFrame\n",
    "                    row = {\n",
    "                        'hidden_layers': str(hidden_layers_sizes),\n",
    "                        'accuracy': metrics['accuracy'],\n",
    "                        'precision_macro': metrics['precision_macro'],\n",
    "                        'recall_macro': metrics['recall_macro'],\n",
    "                        'f1_macro': metrics['f1_macro'],\n",
    "                    }\n",
    "\n",
    "                    # Dodajemy metryki dla każdej klasy\n",
    "                    for i in range(n_classes):\n",
    "                        row[f'precision_class_{i}'] = metrics['precision_per_class'][i]\n",
    "                        row[f'recall_class_{i}'] = metrics['recall_per_class'][i]\n",
    "                        row[f'f1_class_{i}'] = metrics['f1_per_class'][i]\n",
    "\n",
    "                    results.append(row)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Średnie metryki dla każdej konfiguracji warstw\n",
    "summary_df = results_df.groupby('hidden_layers').mean().reset_index()\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28666b2c",
   "metadata": {},
   "source": [
    "Sieć z biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   learning_rate  lr_adjust  adjusted_lr  epochs hidden_layers  repeat  \\\n",
      "0           0.01        0.5        0.005    1000          [10]       1   \n",
      "1           0.01        0.5        0.005    1000          [10]       2   \n",
      "2           0.01        0.5        0.005    1000          [10]       3   \n",
      "3           0.01        0.5        0.005    1000      [10, 10]       1   \n",
      "4           0.01        0.5        0.005    1000      [10, 10]       2   \n",
      "5           0.01        0.5        0.005    1000      [10, 10]       3   \n",
      "\n",
      "   accuracy  precision    recall  f1_score  \n",
      "0  0.966189   0.965981  0.966015  0.965935  \n",
      "1  0.966774   0.966539  0.966592  0.966521  \n",
      "2  0.964533   0.964295  0.964335  0.964249  \n",
      "3  0.965994   0.965881  0.965843  0.965746  \n",
      "4  0.962487   0.962507  0.962331  0.962212  \n",
      "5  0.966384   0.966143  0.966192  0.966116  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# =======================\n",
    "# Parametry symulacji\n",
    "# =======================\n",
    "learning_rates = [0.01]\n",
    "learning_rate_adjusts = [0.5]\n",
    "epochses = [1000]\n",
    "repeat = 3\n",
    "hidden_layers_sizes_list = [[10], [10, 10]]\n",
    "\n",
    "# =======================\n",
    "# Wczytanie danych\n",
    "# =======================\n",
    "data = pd.read_csv('star_classification.csv', delimiter=\",\")\n",
    "\n",
    "features = ['alpha','delta','u','g','r','i','z','redshift']\n",
    "\n",
    "# Usuwanie wartości odstających\n",
    "for col in features:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3*std) & (data[col] <= mean + 3*std)]\n",
    "\n",
    "# Kodowanie klas\n",
    "le = LabelEncoder()\n",
    "data['class_encoded'] = le.fit_transform(data['class'])  # GALAXY=0, STAR=1, QSO=2\n",
    "\n",
    "# Balansowanie klas\n",
    "min_count = data['class_encoded'].value_counts().min()\n",
    "balanced_data = pd.concat([\n",
    "    df.sample(min_count, random_state=42)\n",
    "    for _, df in data.groupby('class_encoded')\n",
    "])\n",
    "\n",
    "# Dane i etykiety\n",
    "X = balanced_data[features].values\n",
    "y = balanced_data['class_encoded'].values  # WEKTOR etykiet, nie one-hot\n",
    "\n",
    "# Łączymy X i y, żeby podział był zsynchronizowany\n",
    "combined = np.concatenate((X, y.reshape(-1,1)), axis=1)\n",
    "\n",
    "# =======================\n",
    "# Podział na zbiory\n",
    "# =======================\n",
    "def split_data(data, train_ratio=0.6, validation_ratio=0.2):\n",
    "    np.random.shuffle(data)\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    validation_size = int(len(data) * validation_ratio)\n",
    "    train_data = data[:train_size]\n",
    "    validation_data = data[train_size:train_size + validation_size]\n",
    "    test_data = data[train_size + validation_size:]\n",
    "    return train_data, validation_data, test_data\n",
    "\n",
    "train_data, validation_data, test_data = split_data(combined)\n",
    "\n",
    "X_train = train_data[:, :-1]\n",
    "y_train = train_data[:, -1].astype(int)\n",
    "X_validation = validation_data[:, :-1]\n",
    "y_validation = validation_data[:, -1].astype(int)\n",
    "X_test = test_data[:, :-1]\n",
    "y_test = test_data[:, -1].astype(int)\n",
    "\n",
    "# Skalowanie\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lr_adj in learning_rate_adjusts:\n",
    "        adjusted_lr = lr * lr_adj\n",
    "        for epochs in epochses:\n",
    "            for hidden_layers in hidden_layers_sizes_list:\n",
    "                for r in range(repeat):\n",
    "                    mlp = MLPClassifier(\n",
    "                        hidden_layer_sizes=hidden_layers,\n",
    "                        activation='logistic',\n",
    "                        max_iter=epochs,\n",
    "                        learning_rate_init=adjusted_lr,\n",
    "                        random_state=r\n",
    "                    )\n",
    "                    mlp.fit(X_train, y_train)\n",
    "                    \n",
    "                    y_pred = mlp.predict(X_test)\n",
    "                    \n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"lr_adjust\": lr_adj,\n",
    "                        \"adjusted_lr\": adjusted_lr,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"hidden_layers\": hidden_layers,\n",
    "                        \"repeat\": r+1,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"precision\": precision,\n",
    "                        \"recall\": recall,\n",
    "                        \"f1_score\": f1\n",
    "                    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# Opcjonalnie zapis\n",
    "# df_results.to_csv(\"mlp_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d206b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   learning_rate  lr_adjust  adjusted_lr  epochs hidden_layers  repeat  \\\n",
      "0           0.01        0.5        0.005    1000          [10]       1   \n",
      "1           0.01        0.5        0.005    1000          [10]       2   \n",
      "2           0.01        0.5        0.005    1000          [10]       3   \n",
      "3           0.01        0.5        0.005    1000      [10, 10]       1   \n",
      "4           0.01        0.5        0.005    1000      [10, 10]       2   \n",
      "5           0.01        0.5        0.005    1000      [10, 10]       3   \n",
      "\n",
      "   accuracy  precision_GALAXY  precision_STAR  precision_QSO  recall_GALAXY  \\\n",
      "0  0.966189          0.943890        0.966607       0.987447       0.953695   \n",
      "1  0.966774          0.946623        0.965548       0.987447       0.952805   \n",
      "2  0.964533          0.943346        0.966021       0.983518       0.948946   \n",
      "3  0.965994          0.940233        0.970243       0.987165       0.957257   \n",
      "4  0.962487          0.932985        0.973260       0.981277       0.954586   \n",
      "5  0.966384          0.947088        0.966419       0.984922       0.951024   \n",
      "\n",
      "   recall_STAR  recall_QSO  f1_GALAXY   f1_STAR    f1_QSO  \n",
      "0     0.944639    0.999711   0.948767  0.955497  0.993541  \n",
      "1     0.947261    0.999711   0.949704  0.956317  0.993541  \n",
      "2     0.944347    0.999711   0.946138  0.955061  0.991548  \n",
      "3     0.940559    0.999711   0.948669  0.955171  0.993398  \n",
      "4     0.933275    0.999133   0.943662  0.952848  0.990125  \n",
      "5     0.947552    1.000000   0.949052  0.956893  0.992404  \n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Symulacje różnych parametrów z wynikami po klasach\n",
    "# =======================\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lr_adj in learning_rate_adjusts:\n",
    "        adjusted_lr = lr * lr_adj\n",
    "        for epochs in epochses:\n",
    "            for hidden_layers in hidden_layers_sizes_list:\n",
    "                for r in range(repeat):\n",
    "                    mlp = MLPClassifier(\n",
    "                        hidden_layer_sizes=hidden_layers,\n",
    "                        activation='logistic',\n",
    "                        max_iter=epochs,\n",
    "                        learning_rate_init=adjusted_lr,\n",
    "                        random_state=r\n",
    "                    )\n",
    "                    mlp.fit(X_train, y_train)\n",
    "                    \n",
    "                    y_pred = mlp.predict(X_test)\n",
    "                    \n",
    "                    # Ogólne miary\n",
    "                    accuracy = accuracy_score(y_test, y_pred)\n",
    "                    \n",
    "                    # Miary dla poszczególnych klas\n",
    "                    precision_classes = precision_score(y_test, y_pred, average=None, zero_division=0)\n",
    "                    recall_classes = recall_score(y_test, y_pred, average=None, zero_division=0)\n",
    "                    f1_classes = f1_score(y_test, y_pred, average=None, zero_division=0)\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"learning_rate\": lr,\n",
    "                        \"lr_adjust\": lr_adj,\n",
    "                        \"adjusted_lr\": adjusted_lr,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"hidden_layers\": hidden_layers,\n",
    "                        \"repeat\": r+1,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"precision_GALAXY\": precision_classes[0],\n",
    "                        \"precision_STAR\": precision_classes[1],\n",
    "                        \"precision_QSO\": precision_classes[2],\n",
    "                        \"recall_GALAXY\": recall_classes[0],\n",
    "                        \"recall_STAR\": recall_classes[1],\n",
    "                        \"recall_QSO\": recall_classes[2],\n",
    "                        \"f1_GALAXY\": f1_classes[0],\n",
    "                        \"f1_STAR\": f1_classes[1],\n",
    "                        \"f1_QSO\": f1_classes[2]\n",
    "                    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# Opcjonalnie zapis\n",
    "# df_results.to_csv(\"mlp_results_by_class.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
