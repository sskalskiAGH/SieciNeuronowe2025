{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8eaaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = pd.read_csv('star_classification.csv', delimiter=\",\")\n",
    "\n",
    "features = ['alpha','delta','u','g','r','i','z','redshift']\n",
    "\n",
    "for col in features:\n",
    "    mean = data[col].mean()\n",
    "    std = data[col].std()\n",
    "    data = data[(data[col] >= mean - 3*std) & (data[col] <= mean + 3*std)]\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['class_encoded'] = le.fit_transform(data['class'])\n",
    "\n",
    "X = data[features].values\n",
    "y_encoded = data['class_encoded'].values\n",
    "y = np.eye(3)[y_encoded]  # one-hot\n",
    "\n",
    "combined = np.concatenate((X, y), axis=1)\n",
    "\n",
    "def split_data(data, train_ratio=0.8):\n",
    "    np.random.shuffle(data)\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = split_data(combined)\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=features + ['c0','c1','c2'])\n",
    "train_df['class'] = np.argmax(train_df[['c0','c1','c2']].values, axis=1)\n",
    "min_count = train_df['class'].value_counts().min()\n",
    "balanced_train_df = pd.concat([\n",
    "    df.sample(min_count, random_state=42)\n",
    "    for _, df in train_df.groupby('class')\n",
    "])\n",
    "train_data = balanced_train_df.drop(columns='class').values\n",
    "\n",
    "X_train = train_data[:, :len(features)]\n",
    "y_train = train_data[:, len(features):]\n",
    "X_test = test_data[:, :len(features)]\n",
    "y_test = test_data[:, len(features):]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "y_train_lbl = np.argmax(y_train, axis=1)\n",
    "y_test_lbl = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i + 1]))\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "def forward(X, weights, biases):\n",
    "    activations = [X]\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(activations[-1] @ W + b)\n",
    "        activations.append(A)\n",
    "    A_out = softmax(activations[-1] @ weights[-1] + biases[-1])\n",
    "    activations.append(A_out)\n",
    "    return activations\n",
    "\n",
    "def backward(activations, weights, y):\n",
    "    deltas = [activations[-1] - y]\n",
    "    for i in reversed(range(len(weights) - 1)):\n",
    "        delta = deltas[0] @ weights[i + 1].T\n",
    "        delta *= relu(activations[i + 1], derivative=True)\n",
    "        deltas.insert(0, delta)\n",
    "\n",
    "    grads_W, grads_b = [], []\n",
    "    for i in range(len(weights)):\n",
    "        grads_W.append(activations[i].T @ deltas[i] / len(y))\n",
    "        grads_b.append(np.mean(deltas[i], axis=0, keepdims=True))\n",
    "    return grads_W, grads_b\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    eps = 1e-9\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n",
    "\n",
    "def create_mini_batches(X, y, batch_size, shuffle=True): # Ważne dla momentum\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(X))\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "    for start in range(0, len(X), batch_size):\n",
    "        end = start + batch_size\n",
    "        yield X[start:end], y[start:end]\n",
    "\n",
    "def train(X, y, hidden_layers, lr, epochs, batch_size,\n",
    "          optimizer, momentum):\n",
    "    layer_sizes = [X.shape[1]] + hidden_layers + [y.shape[1]]\n",
    "    weights, biases = initialize_parameters(layer_sizes)\n",
    "    loss_history = []\n",
    "\n",
    "    vW = [np.zeros_like(w) for w in weights]\n",
    "    vb = [np.zeros_like(b) for b in biases]\n",
    "    mW = [np.zeros_like(w) for w in weights]\n",
    "    mb = [np.zeros_like(b) for b in biases]\n",
    "    sW = [np.zeros_like(w) for w in weights]\n",
    "    sb = [np.zeros_like(b) for b in biases]\n",
    "    eps = 1e-8\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for X_batch, y_batch in create_mini_batches(X, y, batch_size):\n",
    "            activations = forward(X_batch, weights, biases)\n",
    "            grads_W, grads_b = backward(activations, weights, y_batch)\n",
    "\n",
    "            for i in range(len(weights)):\n",
    "                if optimizer == \"sgd\":\n",
    "                    weights[i] -= lr * grads_W[i]\n",
    "                    biases[i] -= lr * grads_b[i]\n",
    "\n",
    "                elif optimizer == \"momentum\":\n",
    "                    vW[i] = momentum * vW[i] - lr * grads_W[i]\n",
    "                    vb[i] = momentum * vb[i] - lr * grads_b[i]\n",
    "                    weights[i] += vW[i]\n",
    "                    biases[i] += vb[i]\n",
    "\n",
    "            batch_loss = cross_entropy(y_batch, activations[-1])\n",
    "            epoch_loss += batch_loss\n",
    "            batch_count += 1\n",
    "        \n",
    "        loss_history.append(epoch_loss / batch_count)\n",
    "        \n",
    "    return weights, biases, loss_history\n",
    "\n",
    "def predict(X, weights, biases):\n",
    "    A = X\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(A @ W + b)\n",
    "    A = softmax(A @ weights[-1] + biases[-1])\n",
    "    return np.argmax(A, axis=1)\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "epochs_list = [1500, 1200, 900, 500]\n",
    "hidden_layer_configs = [\n",
    "        [128, 64, 32, 16], [64, 32, 16, 8], [32, 16, 8, 4],\n",
    "        [64, 32, 16], [32, 16, 8], [16, 8, 4],\n",
    "        [64, 32], [32, 16], [16, 8], [8, 4],\n",
    "        [64], [32], [16], [8]\n",
    "]\n",
    "optimizers = [\"sgd\", \"momentum\"]\n",
    "momentum_values = [0.6, 0.7, 0.8, 0.9]\n",
    "repeat = 5\n",
    "\n",
    "base_params = {\n",
    "    'hidden_layers': [32, 16, 8],\n",
    "    'optimizer': 'sgd',\n",
    "    'momentum': 0.9,\n",
    "    'learning_rate': 0.1,\n",
    "    'epochs': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for r in range(1, repeat + 1):\n",
    "    # 1. Iteracja po hidden_layers\n",
    "    for hl in hidden_layer_configs:\n",
    "        params = base_params.copy()\n",
    "        params['hidden_layers'] = hl\n",
    "        \n",
    "        trained_weights, trained_biases, loss_history = train(\n",
    "            X_train, y_train,\n",
    "            hidden_layers=params['hidden_layers'],\n",
    "            lr=params['learning_rate'],\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=100, \n",
    "            optimizer=params['optimizer'],\n",
    "            momentum=params['momentum']\n",
    "        )\n",
    "        final_loss = loss_history[-1]  # bierzemy wartość loss z ostatniej epok\n",
    "\n",
    "        pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "        pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "        acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "        acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "        results.append({\n",
    "            'hidden_layers': str(params['hidden_layers']),\n",
    "            'optimizer': params['optimizer'],\n",
    "            'momentum': params['momentum'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epochs': params['epochs'],\n",
    "            'repeat': r,\n",
    "            'acc_train': acc_train,\n",
    "            'precision_train': prec_train,\n",
    "            'recall_train': rec_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_test': acc_test,\n",
    "            'precision_test': prec_test,\n",
    "            'recall_test': rec_test,\n",
    "            'f1_test': f1_test,\n",
    "            'final_loss': final_loss\n",
    "        })\n",
    "\n",
    "    # 2. Iteracja po learning_rate\n",
    "    for lr in learning_rates:\n",
    "        params = base_params.copy()\n",
    "        params['learning_rate'] = lr\n",
    "        \n",
    "        trained_weights, trained_biases, loss_history = train(\n",
    "            X_train, y_train,\n",
    "            hidden_layers=params['hidden_layers'],\n",
    "            lr=params['learning_rate'],\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=100, \n",
    "            optimizer=params['optimizer'],\n",
    "            momentum=params['momentum']\n",
    "        )\n",
    "        final_loss = loss_history[-1]\n",
    "\n",
    "        pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "        pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "        acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "        acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "        results.append({\n",
    "            'hidden_layers': str(params['hidden_layers']),\n",
    "            'optimizer': params['optimizer'],\n",
    "            'momentum': params['momentum'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epochs': params['epochs'],\n",
    "            'repeat': r,\n",
    "            'acc_train': acc_train,\n",
    "            'precision_train': prec_train,\n",
    "            'recall_train': rec_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_test': acc_test,\n",
    "            'precision_test': prec_test,\n",
    "            'recall_test': rec_test,\n",
    "            'f1_test': f1_test,\n",
    "            'final_loss': final_loss\n",
    "        })\n",
    "\n",
    "    # 3. Iteracja po epochs\n",
    "    for ep in epochs_list:\n",
    "        params = base_params.copy()\n",
    "        params['epochs'] = ep\n",
    "        \n",
    "        trained_weights, trained_biases, loss_history = train(\n",
    "            X_train, y_train,\n",
    "            hidden_layers=params['hidden_layers'],\n",
    "            lr=params['learning_rate'],\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=100, \n",
    "            optimizer=params['optimizer'],\n",
    "            momentum=params['momentum']\n",
    "        )\n",
    "        final_loss = loss_history[-1]\n",
    "\n",
    "        pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "        pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "        acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "        acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "        results.append({\n",
    "            'hidden_layers': str(params['hidden_layers']),\n",
    "            'optimizer': params['optimizer'],\n",
    "            'momentum': params['momentum'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epochs': params['epochs'],\n",
    "            'repeat': r,\n",
    "            'acc_train': acc_train,\n",
    "            'precision_train': prec_train,\n",
    "            'recall_train': rec_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_test': acc_test,\n",
    "            'precision_test': prec_test,\n",
    "            'recall_test': rec_test,\n",
    "            'f1_test': f1_test,\n",
    "            'final_loss': final_loss\n",
    "        })\n",
    "\n",
    "    # 4. Iteracja po optimizer i momentum\n",
    "    for opt in optimizers:\n",
    "        moms = momentum_values if opt == \"momentum\" else [0.0]\n",
    "        for mom in moms:\n",
    "            params = base_params.copy()\n",
    "            params['optimizer'] = opt\n",
    "            params['momentum'] = mom\n",
    "            \n",
    "            trained_weights, trained_biases, loss_history = train(\n",
    "                X_train, y_train,\n",
    "                hidden_layers=params['hidden_layers'],\n",
    "                lr=params['learning_rate'],\n",
    "                epochs=params['epochs'],\n",
    "                batch_size=100, \n",
    "                optimizer=params['optimizer'],\n",
    "                momentum=params['momentum']\n",
    "            )\n",
    "            final_loss = loss_history[-1]\n",
    "\n",
    "            pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "            pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "            acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "            acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "            results.append({\n",
    "                'hidden_layers': str(params['hidden_layers']),\n",
    "                'optimizer': params['optimizer'],\n",
    "                'momentum': params['momentum'],\n",
    "                'learning_rate': params['learning_rate'],\n",
    "                'epochs': params['epochs'],\n",
    "                'repeat': r,\n",
    "                'acc_train': acc_train,\n",
    "                'precision_train': prec_train,\n",
    "                'recall_train': rec_train,\n",
    "                'f1_train': f1_train,\n",
    "                'acc_test': acc_test,\n",
    "                'precision_test': prec_test,\n",
    "                'recall_test': rec_test,\n",
    "                'f1_test': f1_test,\n",
    "                'final_loss': final_loss\n",
    "            })\n",
    "\n",
    "df_mlp = pd.DataFrame(results)\n",
    "df_mlp.to_excel('mlp_sekwencyjne_nasze_dane.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9364885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zagregowane wyniki (średnia po powtórzeniach)\n",
    "summary_mlp = df_mlp.groupby(['hidden_layers', 'optimizer', 'momentum', 'learning_rate', 'epochs']) \\\n",
    "    .agg({\n",
    "        'acc_train': ['mean', 'min', 'max'],\n",
    "        'precision_train': ['mean', 'min', 'max'],\n",
    "        'recall_train': ['mean', 'min', 'max'],\n",
    "        'f1_train': ['mean', 'min', 'max'],\n",
    "        'acc_test': ['mean', 'min', 'max'],\n",
    "        'precision_test': ['mean', 'min', 'max'],\n",
    "        'recall_test': ['mean', 'min', 'max'],\n",
    "        'f1_test': ['mean', 'min', 'max'],\n",
    "        'final_loss': ['mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "\n",
    "summary_mlp.columns = ['_'.join(col).strip('_') for col in summary_mlp.columns.values]\n",
    "\n",
    "summary_mlp.to_excel('mlp_sekwencyjnie_nasze_dane_zagregowane.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2626aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame({\n",
    "    \"epoch\": np.arange(len(loss_history)),\n",
    "    \"loss\": loss_history\n",
    "})\n",
    "\n",
    "loss_df.to_excel(\"loss_history_klasyfikacja_nasze_dane.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "base_params = {\n",
    "    'hidden_layer_sizes': [32, 16, 8],\n",
    "    'optimizer': 'sgd',\n",
    "    'momentum': 0.9,\n",
    "    'learning_rate': 0.1,\n",
    "    'epochs': 500\n",
    "}\n",
    "\n",
    "hidden_layer_sizes = [\n",
    "        [128, 64, 32, 16], [64, 32, 16, 8], [32, 16, 8, 4],\n",
    "        [64, 32, 16], [32, 16, 8], [16, 8, 4],\n",
    "        [64, 32], [32, 16], [16, 8], [8, 4],\n",
    "        [64], [32], [16], [8]\n",
    "]\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "epochs_list = [1500, 1200, 900, 500]\n",
    "optimizers = [\"sgd\", \"adam\"]\n",
    "momentum_values = [0.6, 0.7, 0.8, 0.9]\n",
    "repeat = 5\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "def train_and_evaluate(params, r):\n",
    "    solver = params['optimizer']  # 'sgd' lub 'adam'\n",
    "    mom = params['momentum'] if solver == 'sgd' else 0.0\n",
    "    \n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=tuple(params['hidden_layer_sizes']),\n",
    "        activation='relu',\n",
    "        solver=solver,\n",
    "        learning_rate_init=params['learning_rate'],\n",
    "        momentum=mom,\n",
    "        max_iter=params['epochs'],\n",
    "        random_state=r\n",
    "    )\n",
    "    \n",
    "    mlp.fit(X_train, y_train_lbl)\n",
    "    \n",
    "    y_pred_train = mlp.predict(X_train)\n",
    "    y_pred_test = mlp.predict(X_test)\n",
    "    \n",
    "    results.append({\n",
    "        'hidden_layers': str(params['hidden_layer_sizes']),\n",
    "        'optimizer': solver,\n",
    "        'momentum': mom,\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'epochs': params['epochs'],\n",
    "        'repeat': r,\n",
    "        'acc_train': accuracy_score(y_train_lbl, y_pred_train),\n",
    "        'precision_train': precision_score(y_train_lbl, y_pred_train, average='macro', zero_division=0),\n",
    "        'recall_train': recall_score(y_train_lbl, y_pred_train, average='macro', zero_division=0),\n",
    "        'f1_train': f1_score(y_train_lbl, y_pred_train, average='macro', zero_division=0),\n",
    "        'acc_test': accuracy_score(y_test_lbl, y_pred_test),\n",
    "        'precision_test': precision_score(y_test_lbl, y_pred_test, average='macro', zero_division=0),\n",
    "        'recall_test': recall_score(y_test_lbl, y_pred_test, average='macro', zero_division=0),\n",
    "        'f1_test': f1_score(y_test_lbl, y_pred_test, average='macro', zero_division=0)\n",
    "    })\n",
    "\n",
    "# Hidden layers\n",
    "for h in hidden_layer_configs:\n",
    "    params = base_params.copy()\n",
    "    params['hidden_layer_sizes'] = h\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "# Learning rate\n",
    "for lr in learning_rates:\n",
    "    params = base_params.copy()\n",
    "    params['learning_rate'] = lr\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "# Optimizer\n",
    "for opt in optimizers:\n",
    "    params = base_params.copy()\n",
    "    params['optimizer'] = opt\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "# Momentum (tylko dla SGD)\n",
    "for mom in momentum_values:\n",
    "    params = base_params.copy()\n",
    "    params['momentum'] = mom\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "# Epochs\n",
    "for ep in epochs_list:\n",
    "    params = base_params.copy()\n",
    "    params['epochs'] = ep\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_excel('mlp_biblioteka_sekwencyjnie_nasze_dane.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5b1afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zagregowane wyniki (średnia po powtórzeniach)\n",
    "summary_mlp = df_mlp.groupby(['hidden_layers', 'optimizer', 'momentum', 'learning_rate', 'epochs']) \\\n",
    "    .agg({\n",
    "        'acc_train': ['mean', 'min', 'max'],\n",
    "        'precision_train': ['mean', 'min', 'max'],\n",
    "        'recall_train': ['mean', 'min', 'max'],\n",
    "        'f1_train': ['mean', 'min', 'max'],\n",
    "        'acc_test': ['mean', 'min', 'max'],\n",
    "        'precision_test': ['mean', 'min', 'max'],\n",
    "        'recall_test': ['mean', 'min', 'max'],\n",
    "        'f1_test': ['mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "\n",
    "summary_mlp.columns = ['_'.join(col).strip('_') for col in summary_mlp.columns.values]\n",
    "\n",
    "df_results.to_excel('mlp_biblioteka_sekwencyjnie_nasze_dane_zagregowane.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
