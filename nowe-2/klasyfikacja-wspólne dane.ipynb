{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c0d6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informacje o wymiarach ---\n",
      "Liczba wierszy: 5456\n",
      "Liczba kolumn: 25\n",
      "\n",
      "--- Analiza kolumny class ---\n",
      "Liczba unikalnych wartości w ostatniej kolumnie: 4\n",
      "Te wartości to: ['Slight-Right-Turn' 'Sharp-Right-Turn' 'Move-Forward' 'Slight-Left-Turn']\n",
      "\n",
      "========================================\n",
      "ROZKŁAD KLAS W CAŁYM ZBIORZE:\n",
      "========================================\n",
      "-- Move-Forward: 2205 samples (40.41%).\n",
      "-- Sharp-Right-Turn: 2097 samples (38.43%).\n",
      "-- Slight-Right-Turn: 826 samples (15.14%).\n",
      "-- Slight-Left-Turn: 328 samples (6.01%).\n",
      "\n",
      "--- Podgląd po zmianach (pierwsze 5 wierszy) ---\n",
      "     US1    US2    US3    US4  US5    US6  US7    US8    US9   US10  ...  \\\n",
      "0  0.438  0.498  3.625  3.645  5.0  2.918  5.0  2.351  2.332  2.643  ...   \n",
      "1  0.438  0.498  3.625  3.648  5.0  2.918  5.0  2.637  2.332  2.649  ...   \n",
      "2  0.438  0.498  3.625  3.629  5.0  2.918  5.0  2.637  2.334  2.643  ...   \n",
      "3  0.437  0.501  3.625  3.626  5.0  2.918  5.0  2.353  2.334  2.642  ...   \n",
      "4  0.438  0.498  3.626  3.629  5.0  2.918  5.0  2.640  2.334  2.639  ...   \n",
      "\n",
      "    US17   US18   US19   US20   US21   US22   US23   US24              Class  \\\n",
      "0  0.502  0.493  0.504  0.445  0.431  0.444  0.440  0.429  Slight-Right-Turn   \n",
      "1  0.502  0.493  0.504  0.449  0.431  0.444  0.443  0.429  Slight-Right-Turn   \n",
      "2  0.502  0.493  0.504  0.449  0.431  0.444  0.446  0.429  Slight-Right-Turn   \n",
      "3  0.502  0.493  0.504  0.449  0.431  0.444  0.444  0.429  Slight-Right-Turn   \n",
      "4  0.502  0.493  0.504  0.449  0.431  0.444  0.441  0.429  Slight-Right-Turn   \n",
      "\n",
      "     Set  \n",
      "0  train  \n",
      "1  train  \n",
      "2   test  \n",
      "3  train  \n",
      "4  train  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "========================================\n",
      "PODSUMOWANIE PODZIAŁU (TRAIN vs TEST):\n",
      "========================================\n",
      "Zbiór 'train': 4364 wierszy (79.99% całości)\n",
      "Zbiór 'test': 1092 wierszy (20.01% całości)\n",
      "\n",
      "--- Weryfikacja proporcji klas w poszczególnych zbiorach (%) ---\n",
      "Set                 test  train\n",
      "Class                          \n",
      "Move-Forward       40.38  40.42\n",
      "Sharp-Right-Turn   38.46  38.43\n",
      "Slight-Left-Turn    6.04   6.00\n",
      "Slight-Right-Turn  15.11  15.15\n",
      "\n",
      "--> Zapisano gotowy plik: sensor_readings_24.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nazwa_pliku = 'sensor_readings_24.data'\n",
    "\n",
    "try:\n",
    "\n",
    "    df = pd.read_csv(nazwa_pliku, sep=',', header=None)\n",
    "    \n",
    "    nowe_nazwy = [f\"US{i}\" for i in range(1, 25)]\n",
    "    nowe_nazwy.append('Class')\n",
    "    df.columns = nowe_nazwy\n",
    "    \n",
    "    print(\"\\n--- Informacje o wymiarach ---\")\n",
    "    wiersze, kolumny = df.shape\n",
    "    print(f\"Liczba wierszy: {wiersze}\")\n",
    "    print(f\"Liczba kolumn: {kolumny}\")\n",
    "    \n",
    "    print(\"\\n--- Analiza kolumny class ---\")\n",
    "    liczba_unikalnych = df.iloc[:, -1].nunique()\n",
    "    print(f\"Liczba unikalnych wartości w ostatniej kolumnie: {liczba_unikalnych}\")\n",
    "    \n",
    "    print(f\"Te wartości to: {df.iloc[:, -1].unique()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ROZKŁAD KLAS W CAŁYM ZBIORZE:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    counts = df['Class'].value_counts()\n",
    "    percents = df['Class'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for klasa in counts.index:\n",
    "        liczba = counts[klasa]\n",
    "        procent = percents[klasa]\n",
    "        print(f\"-- {klasa}: {liczba} samples ({procent:.2f}%).\")\n",
    "\n",
    "    df['Set'] = \"\"\n",
    "    \n",
    "    train_indices, test_indices = train_test_split(\n",
    "        df.index, \n",
    "        test_size=0.2, # dla proporcji 80/20\n",
    "        stratify=df['Class'], # dla zachowania proporcjonalnego rozkładu klas w zbiorach train i test\n",
    "        random_state=42 # inaczej seed\n",
    "    )\n",
    "    \n",
    "    df.loc[train_indices, 'Set'] = 'train'\n",
    "    df.loc[test_indices, 'Set'] = 'test'\n",
    "\n",
    "    print(\"\\n--- Podgląd po zmianach (pierwsze 5 wierszy) ---\")\n",
    "    print(df.head())\n",
    "    \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"PODSUMOWANIE PODZIAŁU (TRAIN vs TEST):\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    set_counts = df['Set'].value_counts()\n",
    "    set_percents = df['Set'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    for nazwa_setu in set_counts.index:\n",
    "        liczba = set_counts[nazwa_setu]\n",
    "        procent = set_percents[nazwa_setu]\n",
    "        print(f\"Zbiór '{nazwa_setu}': {liczba} wierszy ({procent:.2f}% całości)\")\n",
    "\n",
    "    # Dodatkowe sprawdzenie: Czy proporcje klas zostały zachowane wewnątrz zbiorów?\n",
    "    print(\"\\n--- Weryfikacja proporcji klas w poszczególnych zbiorach (%) ---\")\n",
    "    proporcje = pd.crosstab(df['Class'], df['Set'], normalize='columns') * 100\n",
    "    print(proporcje.round(2))\n",
    "    \n",
    "    df.to_excel(\"sensor_readings_24_outcome.xlsx\", index=False)\n",
    "    print(\"\\n--> Zapisano gotowy plik: sensor_readings_24.xlsx\")\n",
    "    \n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Nie znaleziono pliku. Sprawdź czy nazwa i ścieżka są poprawne.\")\n",
    "except Exception as e:\n",
    "    print(f\"Wystąpił błąd: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94738b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (4364, 24), X_test: (1092, 24)\n",
      "y_train: (4364, 4), y_test: (1092, 4)\n",
      "y_train_lbl: (4364,), y_test_lbl: (1092,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "features = [f\"US{i}\" for i in range(1, 25)]\n",
    "class_col = 'Class'\n",
    "\n",
    "train_mask = df['Set'] == 'train'\n",
    "test_mask  = df['Set'] == 'test'\n",
    "\n",
    "X_train = df.loc[train_mask, features].values\n",
    "X_test  = df.loc[test_mask, features].values\n",
    "\n",
    "y_train_lbl = df.loc[train_mask, class_col].values\n",
    "y_test_lbl  = df.loc[test_mask, class_col].values\n",
    "\n",
    "# konwersja etykiet na liczby całkowite\n",
    "le = LabelEncoder()\n",
    "y_train_int = le.fit_transform(y_train_lbl)\n",
    "y_test_int  = le.transform(y_test_lbl)\n",
    "\n",
    "# liczba klas\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# one-hot encoding\n",
    "y_train = np.eye(num_classes)[y_train_int]\n",
    "y_test  = np.eye(num_classes)[y_test_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2d6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (x > 0).astype(float)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def initialize_parameters(layer_sizes):\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        W = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * np.sqrt(2 / layer_sizes[i])\n",
    "        b = np.zeros((1, layer_sizes[i + 1]))\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    return weights, biases\n",
    "\n",
    "def forward(X, weights, biases):\n",
    "    activations = [X]\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(activations[-1] @ W + b)\n",
    "        activations.append(A)\n",
    "    A_out = softmax(activations[-1] @ weights[-1] + biases[-1])\n",
    "    activations.append(A_out)\n",
    "    return activations\n",
    "\n",
    "def backward(activations, weights, y):\n",
    "    deltas = [activations[-1] - y]\n",
    "    for i in reversed(range(len(weights) - 1)):\n",
    "        delta = deltas[0] @ weights[i + 1].T\n",
    "        delta *= relu(activations[i + 1], derivative=True)\n",
    "        deltas.insert(0, delta)\n",
    "\n",
    "    grads_W, grads_b = [], []\n",
    "    for i in range(len(weights)):\n",
    "        grads_W.append(activations[i].T @ deltas[i] / len(y))\n",
    "        grads_b.append(np.mean(deltas[i], axis=0, keepdims=True))\n",
    "    return grads_W, grads_b\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    eps = 1e-9\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n",
    "\n",
    "def create_mini_batches(X, y, batch_size, shuffle=True): # Ważne dla momentum\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(X))\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "    for start in range(0, len(X), batch_size):\n",
    "        end = start + batch_size\n",
    "        yield X[start:end], y[start:end]\n",
    "\n",
    "def train(X, y, hidden_layers, lr, epochs, batch_size,\n",
    "          optimizer, momentum):\n",
    "    layer_sizes = [X.shape[1]] + hidden_layers + [y.shape[1]]\n",
    "    weights, biases = initialize_parameters(layer_sizes)\n",
    "    loss_history = []\n",
    "\n",
    "    vW = [np.zeros_like(w) for w in weights]\n",
    "    vb = [np.zeros_like(b) for b in biases]\n",
    "    mW = [np.zeros_like(w) for w in weights]\n",
    "    mb = [np.zeros_like(b) for b in biases]\n",
    "    sW = [np.zeros_like(w) for w in weights]\n",
    "    sb = [np.zeros_like(b) for b in biases]\n",
    "    eps = 1e-8\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        for X_batch, y_batch in create_mini_batches(X, y, batch_size):\n",
    "            activations = forward(X_batch, weights, biases)\n",
    "            grads_W, grads_b = backward(activations, weights, y_batch)\n",
    "\n",
    "            for i in range(len(weights)):\n",
    "                if optimizer == \"sgd\":\n",
    "                    weights[i] -= lr * grads_W[i]\n",
    "                    biases[i] -= lr * grads_b[i]\n",
    "\n",
    "                elif optimizer == \"momentum\":\n",
    "                    vW[i] = momentum * vW[i] - lr * grads_W[i]\n",
    "                    vb[i] = momentum * vb[i] - lr * grads_b[i]\n",
    "                    weights[i] += vW[i]\n",
    "                    biases[i] += vb[i]\n",
    "\n",
    "            batch_loss = cross_entropy(y_batch, activations[-1])\n",
    "            epoch_loss += batch_loss\n",
    "            batch_count += 1\n",
    "        \n",
    "        loss_history.append(epoch_loss / batch_count)\n",
    "        \n",
    "    return weights, biases, loss_history\n",
    "\n",
    "def predict(X, weights, biases):\n",
    "    A = X\n",
    "    for W, b in zip(weights[:-1], biases[:-1]):\n",
    "        A = relu(A @ W + b)\n",
    "    A = softmax(A @ weights[-1] + biases[-1])\n",
    "    return np.argmax(A, axis=1)\n",
    "\n",
    "def calculate_classification_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "epochs_list = [1500, 1200, 900, 500]\n",
    "hidden_layer_configs = [\n",
    "        [128, 64, 32, 16], [64, 32, 16, 8], [32, 16, 8, 4],\n",
    "        [64, 32, 16], [32, 16, 8], [16, 8, 4],\n",
    "        [64, 32], [32, 16], [16, 8], [8, 4],\n",
    "        [64], [32], [16], [8]\n",
    "]\n",
    "optimizers = [\"sgd\", \"momentum\"]\n",
    "momentum_values = [0.6, 0.7, 0.8, 0.9]\n",
    "repeat = 5\n",
    "\n",
    "base_params = {\n",
    "    'hidden_layers': [32, 16, 8],\n",
    "    'optimizer': 'sgd',\n",
    "    'momentum': 0.9,\n",
    "    'learning_rate': 0.1,\n",
    "    'epochs': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43db7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for r in range(1, repeat + 1):\n",
    "    # 1. Iteracja po hidden_layers\n",
    "    for hl in hidden_layer_configs:\n",
    "        params = base_params.copy()\n",
    "        params['hidden_layers'] = hl\n",
    "        \n",
    "        trained_weights, trained_biases, loss_history = train(\n",
    "            X_train, y_train,\n",
    "            hidden_layers=params['hidden_layers'],\n",
    "            lr=params['learning_rate'],\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=100, \n",
    "            optimizer=params['optimizer'],\n",
    "            momentum=params['momentum']\n",
    "        )\n",
    "        final_loss = loss_history[-1]  # bierzemy wartość loss z ostatniej epok\n",
    "\n",
    "        pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "        pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "        acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "        acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "        results.append({\n",
    "            'hidden_layers': str(params['hidden_layers']),\n",
    "            'optimizer': params['optimizer'],\n",
    "            'momentum': params['momentum'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epochs': params['epochs'],\n",
    "            'repeat': r,\n",
    "            'acc_train': acc_train,\n",
    "            'precision_train': prec_train,\n",
    "            'recall_train': rec_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_test': acc_test,\n",
    "            'precision_test': prec_test,\n",
    "            'recall_test': rec_test,\n",
    "            'f1_test': f1_test,\n",
    "            'final_loss': final_loss\n",
    "        })\n",
    "\n",
    "    # 2. Iteracja po learning_rate\n",
    "    for lr in learning_rates:\n",
    "        params = base_params.copy()\n",
    "        params['learning_rate'] = lr\n",
    "        \n",
    "        trained_weights, trained_biases, loss_history = train(\n",
    "            X_train, y_train,\n",
    "            hidden_layers=params['hidden_layers'],\n",
    "            lr=params['learning_rate'],\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=100, \n",
    "            optimizer=params['optimizer'],\n",
    "            momentum=params['momentum']\n",
    "        )\n",
    "        final_loss = loss_history[-1]\n",
    "\n",
    "        pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "        pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "        acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "        acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "        results.append({\n",
    "            'hidden_layers': str(params['hidden_layers']),\n",
    "            'optimizer': params['optimizer'],\n",
    "            'momentum': params['momentum'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epochs': params['epochs'],\n",
    "            'repeat': r,\n",
    "            'acc_train': acc_train,\n",
    "            'precision_train': prec_train,\n",
    "            'recall_train': rec_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_test': acc_test,\n",
    "            'precision_test': prec_test,\n",
    "            'recall_test': rec_test,\n",
    "            'f1_test': f1_test,\n",
    "            'final_loss': final_loss\n",
    "        })\n",
    "\n",
    "    # 3. Iteracja po epochs\n",
    "    for ep in epochs_list:\n",
    "        params = base_params.copy()\n",
    "        params['epochs'] = ep\n",
    "        \n",
    "        trained_weights, trained_biases, loss_history = train(\n",
    "            X_train, y_train,\n",
    "            hidden_layers=params['hidden_layers'],\n",
    "            lr=params['learning_rate'],\n",
    "            epochs=params['epochs'],\n",
    "            batch_size=100, \n",
    "            optimizer=params['optimizer'],\n",
    "            momentum=params['momentum']\n",
    "        )\n",
    "        final_loss = loss_history[-1]\n",
    "\n",
    "        pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "        pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "        acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "        acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "        results.append({\n",
    "            'hidden_layers': str(params['hidden_layers']),\n",
    "            'optimizer': params['optimizer'],\n",
    "            'momentum': params['momentum'],\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epochs': params['epochs'],\n",
    "            'repeat': r,\n",
    "            'acc_train': acc_train,\n",
    "            'precision_train': prec_train,\n",
    "            'recall_train': rec_train,\n",
    "            'f1_train': f1_train,\n",
    "            'acc_test': acc_test,\n",
    "            'precision_test': prec_test,\n",
    "            'recall_test': rec_test,\n",
    "            'f1_test': f1_test,\n",
    "            'final_loss': final_loss\n",
    "        })\n",
    "\n",
    "    # 4. Iteracja po optimizer i momentum\n",
    "    for opt in optimizers:\n",
    "        moms = momentum_values if opt == \"momentum\" else [0.0]\n",
    "        for mom in moms:\n",
    "            params = base_params.copy()\n",
    "            params['optimizer'] = opt\n",
    "            params['momentum'] = mom\n",
    "            \n",
    "            trained_weights, trained_biases, loss_history = train(\n",
    "                X_train, y_train,\n",
    "                hidden_layers=params['hidden_layers'],\n",
    "                lr=params['learning_rate'],\n",
    "                epochs=params['epochs'],\n",
    "                batch_size=100, \n",
    "                optimizer=params['optimizer'],\n",
    "                momentum=params['momentum']\n",
    "            )\n",
    "            final_loss = loss_history[-1]\n",
    "\n",
    "            pred_train = predict(X_train, trained_weights, trained_biases)\n",
    "            pred_test = predict(X_test, trained_weights, trained_biases)\n",
    "            acc_train, prec_train, rec_train, f1_train = calculate_classification_metrics(y_train_lbl, pred_train)\n",
    "            acc_test, prec_test, rec_test, f1_test = calculate_classification_metrics(y_test_lbl, pred_test)\n",
    "\n",
    "            results.append({\n",
    "                'hidden_layers': str(params['hidden_layers']),\n",
    "                'optimizer': params['optimizer'],\n",
    "                'momentum': params['momentum'],\n",
    "                'learning_rate': params['learning_rate'],\n",
    "                'epochs': params['epochs'],\n",
    "                'repeat': r,\n",
    "                'acc_train': acc_train,\n",
    "                'precision_train': prec_train,\n",
    "                'recall_train': rec_train,\n",
    "                'f1_train': f1_train,\n",
    "                'acc_test': acc_test,\n",
    "                'precision_test': prec_test,\n",
    "                'recall_test': rec_test,\n",
    "                'f1_test': f1_test,\n",
    "                'final_loss': final_loss\n",
    "            })\n",
    "\n",
    "df_mlp = pd.DataFrame(results)\n",
    "df_mlp.to_excel('mlp_sekwencyjne_wspólne_dane.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zagregowane wyniki (średnia po powtórzeniach)\n",
    "summary_mlp = df_mlp.groupby(['hidden_layers', 'optimizer', 'momentum', 'learning_rate', 'epochs']) \\\n",
    "    .agg({\n",
    "        'acc_train': ['mean', 'min', 'max'],\n",
    "        'precision_train': ['mean', 'min', 'max'],\n",
    "        'recall_train': ['mean', 'min', 'max'],\n",
    "        'f1_train': ['mean', 'min', 'max'],\n",
    "        'acc_test': ['mean', 'min', 'max'],\n",
    "        'precision_test': ['mean', 'min', 'max'],\n",
    "        'recall_test': ['mean', 'min', 'max'],\n",
    "        'f1_test': ['mean', 'min', 'max'],\n",
    "        'final_loss': ['mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "\n",
    "summary_mlp.columns = ['_'.join(col).strip('_') for col in summary_mlp.columns.values]\n",
    "\n",
    "summary_mlp.to_excel('mlp_sekwencyjnie_wspólne_dane_zagregowane.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "base_params = {\n",
    "    'hidden_layer_sizes': [32, 16, 8],\n",
    "    'optimizer': 'sgd',\n",
    "    'momentum': 0.9,\n",
    "    'learning_rate': 0.1,\n",
    "    'epochs': 500\n",
    "}\n",
    "\n",
    "hidden_layer_sizes = [\n",
    "        [128, 64, 32, 16], [64, 32, 16, 8], [32, 16, 8, 4],\n",
    "        [64, 32, 16], [32, 16, 8], [16, 8, 4],\n",
    "        [64, 32], [32, 16], [16, 8], [8, 4],\n",
    "        [64], [32], [16], [8]\n",
    "]\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "epochs_list = [1500, 1200, 900, 500]\n",
    "optimizers = [\"sgd\", \"adam\"]\n",
    "momentum_values = [0.6, 0.7, 0.8, 0.9]\n",
    "repeat = 5\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "def train_and_evaluate(params, r):\n",
    "    solver = params['optimizer']  # 'sgd' lub 'adam'\n",
    "    mom = params['momentum'] if solver == 'sgd' else 0.0\n",
    "    \n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=tuple(params['hidden_layer_sizes']),\n",
    "        activation='relu',\n",
    "        solver=solver,\n",
    "        learning_rate_init=params['learning_rate'],\n",
    "        momentum=mom,\n",
    "        max_iter=params['epochs'],\n",
    "        random_state=r\n",
    "    )\n",
    "    \n",
    "    mlp.fit(X_train, y_train_lbl)\n",
    "    \n",
    "    y_pred_train = mlp.predict(X_train)\n",
    "    y_pred_test = mlp.predict(X_test)\n",
    "    \n",
    "    results.append({\n",
    "        'hidden_layers': str(params['hidden_layer_sizes']),\n",
    "        'optimizer': solver,\n",
    "        'momentum': mom,\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'epochs': params['epochs'],\n",
    "        'repeat': r,\n",
    "        'acc_train': accuracy_score(y_train_lbl, y_pred_train),\n",
    "        'precision_train': precision_score(y_train_lbl, y_pred_train, average='macro', zero_division=0),\n",
    "        'recall_train': recall_score(y_train_lbl, y_pred_train, average='macro', zero_division=0),\n",
    "        'f1_train': f1_score(y_train_lbl, y_pred_train, average='macro', zero_division=0),\n",
    "        'acc_test': accuracy_score(y_test_lbl, y_pred_test),\n",
    "        'precision_test': precision_score(y_test_lbl, y_pred_test, average='macro', zero_division=0),\n",
    "        'recall_test': recall_score(y_test_lbl, y_pred_test, average='macro', zero_division=0),\n",
    "        'f1_test': f1_score(y_test_lbl, y_pred_test, average='macro', zero_division=0)\n",
    "    })\n",
    "\n",
    "# Hidden layers\n",
    "for h in hidden_layer_configs:\n",
    "    params = base_params.copy()\n",
    "    params['hidden_layer_sizes'] = h\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "# Learning rate\n",
    "for lr in learning_rates:\n",
    "    params = base_params.copy()\n",
    "    params['learning_rate'] = lr\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "# Optimizer\n",
    "for opt in optimizers:\n",
    "    params = base_params.copy()\n",
    "    params['optimizer'] = opt\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "# Momentum (tylko dla SGD)\n",
    "for mom in momentum_values:\n",
    "    params = base_params.copy()\n",
    "    params['momentum'] = mom\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "# Epochs\n",
    "for ep in epochs_list:\n",
    "    params = base_params.copy()\n",
    "    params['epochs'] = ep\n",
    "    for r in range(1, repeat+1):\n",
    "        train_and_evaluate(params, r)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_excel('mlp_biblioteka_sekwencyjnie_wspólne_dane.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zagregowane wyniki (średnia po powtórzeniach)\n",
    "summary_mlp = df_mlp.groupby(['hidden_layers', 'optimizer', 'momentum', 'learning_rate', 'epochs']) \\\n",
    "    .agg({\n",
    "        'acc_train': ['mean', 'min', 'max'],\n",
    "        'precision_train': ['mean', 'min', 'max'],\n",
    "        'recall_train': ['mean', 'min', 'max'],\n",
    "        'f1_train': ['mean', 'min', 'max'],\n",
    "        'acc_test': ['mean', 'min', 'max'],\n",
    "        'precision_test': ['mean', 'min', 'max'],\n",
    "        'recall_test': ['mean', 'min', 'max'],\n",
    "        'f1_test': ['mean', 'min', 'max']\n",
    "    }).reset_index()\n",
    "\n",
    "summary_mlp.columns = ['_'.join(col).strip('_') for col in summary_mlp.columns.values]\n",
    "\n",
    "df_results.to_excel('mlp_biblioteka_sekwencyjnie_wspólne_dane_zagregowane.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
